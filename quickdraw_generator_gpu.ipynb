{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw_generator_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNn33zSPS+dqu/h+vP1woCt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bombaybrew/colab_experiments/blob/master/quickdraw_generator_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6Bqn6Mn5y5",
        "colab_type": "text"
      },
      "source": [
        "REFERENCE:\n",
        "\n",
        "https://colab.research.google.com/github/zaidalyafeai/zaidalyafeai.github.io/blob/master/sketcher/Sketcher.ipynb\n",
        "https://github.com/googlecreativelab/quickdraw-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTq5pvsZnGWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !gcloud auth login\n",
        "# !gcloud config set project 'upheld-quanta-284618'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GY-xacwnf6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d29989c7-3567-468a-eef9-ac8c65d043e5"
      },
      "source": [
        "!gsutil cp gs://ml_workspace/quick_draw/qd_20k_split.tar.gz ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml_workspace/quick_draw/qd_20k_split.tar.gz...\n",
            "| [1 files][921.5 MiB/921.5 MiB]   65.2 MiB/s                                   \n",
            "Operation completed over 1 objects/921.5 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68vCWD7tnPnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !tar -xzf qd_20k_split.tar.gz \n",
        "# !rm qd_20k_split.tar.gz\n",
        "# !ls -lah data_processed\n",
        "# !mkdir data\n",
        "# !unzip data.zip\n",
        "# !rm data.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebqxC45znzvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTS\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import gc\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# classes = ['airplane', 'apple', 'ant', 'axe']\n",
        "classes = ['airplane','apple','ant','axe','banana','baseball','bee','beach','bird','book','bucket','bus','butterfly','cactus','cake','cannon','car','carrot','castle',\n",
        "            'cat','chair','church','circle','clock','cloud','computer','cookie','cow','crab','crayon','cup','dolphin','donut','door','dragon','diamond','drums','duck',\n",
        "            'ear','elephant','envelope','eraser','eye','face','fan','finger','fish','flower','frog','garden','grapes','grass','guitar','hammer','hand','hat','helmet',\n",
        "            'horse','house','hospital','jail','kangaroo','knife','laptop','leg','lion','lollipop','map','microwave','monkey','moon','mountain','mouse','mug','nail',\n",
        "            'nose','onion','owl','panda','parachute','parrot','passport','peanut','pencil','piano','pig','pizza','potato','rabbit','radio','rain','rainbow','river',\n",
        "            'sandwich','saw','sun','shark','shoe','skull','snail','snowflake','spider','square','stairs','star','stove','strawberry','table','teapot','television','tent',\n",
        "            'toilet','toothbrush','toothpaste','train','triangle','truck','umbrella','whale','zebra']"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xbfJcbY65ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# files = glob.iglob(\"data/*.npy\")\n",
        "# file = next(files)\n",
        "\n",
        "# data = np.load(file)\n",
        "# np.random.shuffle(data)\n",
        "# data = data[:10000, :]\n",
        "# data.shape\n",
        "\n",
        "# np.full(data.shape[0], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1gE7Pjzmtrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "image_size = 28\n",
        "\n",
        "def data_gen():\n",
        "\n",
        "  for batch in range(14):\n",
        "    print('processing batch: ' + str(batch))\n",
        "    x = np.empty([0, 784])\n",
        "    y = np.empty([0]) \n",
        "\n",
        "    for index, class_label in enumerate(classes): \n",
        "      class_file = 'data_processed/'+str(batch)+'/'+class_label+'.npy'\n",
        "      print('processing file: ' + class_file + ' - ' + str(index))\n",
        "      data = np.load(class_file)\n",
        "      labels = np.full(data.shape[0], index)\n",
        "      x = np.concatenate((x, data), axis=0)\n",
        "      y = np.append(y, labels)\n",
        "\n",
        "    x, y = shuffle(x, y)\n",
        "\n",
        "    x = x/255.0\n",
        "    y = keras.utils.to_categorical(y, len(classes))\n",
        "\n",
        "    x = x.reshape(x.shape[0], image_size, image_size, 1) #.astype('float16')\n",
        "    \n",
        "    print('---------- Loading complete')\n",
        "    yield x, y\n",
        "\n",
        "def val_test_data():\n",
        "\n",
        "  print('processing val and test data')\n",
        "  x = np.empty([0, 784])\n",
        "  y = np.empty([0])\n",
        "\n",
        "  for index, class_label in enumerate(classes):\n",
        "    class_file = 'data_processed/14/'+class_label+'.npy'\n",
        "    data = np.load(class_file)\n",
        "    labels = np.full(data.shape[0], index)\n",
        "    x = np.concatenate((x, data), axis=0)\n",
        "    y = np.append(y, labels)\n",
        "  x = x/255.0\n",
        "  x = x.reshape(x.shape[0], image_size, image_size, 1) #.astype('float16')\n",
        "  y = keras.utils.to_categorical(y, len(classes))\n",
        "  print('Done processing')\n",
        "  return x, y\n",
        "\n",
        "train_gen = data_gen()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcsmnBlRowPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5617cc70-52de-4605-e546-969222210ac8"
      },
      "source": [
        "# x_train, y_train = next(train_gen)\n",
        "x_test, y_test = val_test_data()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing val and test data\n",
            "Done processing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrsYPVu1Ynlk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b6c1d9cc-f46e-4551-ef8c-1dba50797aac"
      },
      "source": [
        "from random import randint\n",
        "def visualise_random(x, y):\n",
        "\n",
        "  idx = randint(0, len(x))\n",
        "  plt.imshow(x[idx].reshape(28,28))\n",
        "  print(classes[y[idx].tolist().index(1)])\n",
        "  # print(classes[int(y_train[idx])])\n",
        "  # print(y_train[idx].tolist().index(1))\n",
        "  # print(x[idx].reshape(28,28))\n",
        "\n",
        "visualise_random(x_test, y_test)\n",
        "# x_train[0].reshape(28,28)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "airplane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCiQ2qx-KYAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8efa620-4dd2-4810-92ad-ef2e5ae370e0"
      },
      "source": [
        "print(x[1999])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUiSAcit3-V6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2cf79357-b4cf-44ba-fe7c-7d4cde8b9b16"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_train.nbytes/1024/1024)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000, 120)\n",
            "358.88671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcadqsGZwCBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b7c19810-1e68-4e95-dfe1-e50955344a0f"
      },
      "source": [
        ""
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing val and test data\n",
            "Done processing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X_40UWLo17a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d9bdbe2-106e-4a2e-d64a-f9f3e66b3af6"
      },
      "source": [
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 28, 28, 1)\n",
            "(8000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrzn6_cZ9log",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "  print(epoch)\n",
        "  if epoch > 2:\n",
        "    lr = 0.0001\n",
        "    return lr\n",
        "  return lr\n",
        "\n",
        "def print_metric(model):\n",
        "\n",
        "  score = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))\n",
        "  print('----------')\n",
        "\n",
        "  y_pred = model.predict(x_test, verbose=2)\n",
        "  # y_pred = (y_pred > 0.5 ? 1 : 0)\n",
        "  y_pred_argmax = np.argmax(y_pred, axis=1)\n",
        "  y_test_argmax = np.argmax(y_test, axis=1)\n",
        "  print(metrics.classification_report(y_test_argmax, y_pred_argmax))\n",
        "  print('----------')\n",
        "  print('HammingLoss: ', metrics.hamming_loss(y_test_argmax, y_pred_argmax))\n",
        "\n",
        "def run_exp(model, x_train, y_train, epoch=8, batch_size=256):\n",
        "  # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "  lr_callback = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "  model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size=256, epochs=10, callbacks=[lr_callback], verbose=1)\n",
        "  print_metric(model)\n",
        "  return model\n",
        "\n",
        "def run_exp_gen(model, epoch=14, batch_size=256):\n",
        "  model.fit(train_gen, validation_data=(x_val, y_val), steps_per_epoch=16, epochs=epoch, verbose=1)\n",
        "  print_metric(model)\n",
        "  return model\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIF1_okn0vJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "4c552605-b37a-480f-d6c9-04ba501c56dc"
      },
      "source": [
        "def get_simple_cnn(num_classes):\n",
        "  activation = 'elu'\n",
        "\n",
        "  model = keras.Sequential(name=\"vgg_bn_reg_seq\")\n",
        "  model.add(layers.Convolution2D(16, (3, 3), name='c2d_11', input_shape=(28, 28, 1), padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(16, (3, 3), name='c2d_12', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(16, (3, 3), name='c2d_13', padding='same', activation=activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Convolution2D(32, (3, 3), name='c2d_21', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(32, (3, 3), name='c2d_22', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(32, (3, 3), name='c2d_23', padding='same', activation=activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Convolution2D(64, (3, 3), name='c2d_31', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(64, (3, 3), name='c2d_32', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(64, (3, 3), name='c2d_33', padding='same', activation=activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(256, name='d1', activation=activation))\n",
        "  model.add(layers.Dense(256, name='d2', activation=activation))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Dense(num_classes, name='softmax', activation='softmax'))\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(), metrics=['accuracy'])\n",
        "\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "model = get_simple_cnn(120)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg_bn_reg_seq\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "c2d_11 (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "c2d_12 (Conv2D)              (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "c2d_13 (Conv2D)              (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "c2d_21 (Conv2D)              (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "c2d_22 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "_________________________________________________________________\n",
            "c2d_23 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "c2d_31 (Conv2D)              (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "c2d_32 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "c2d_33 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 576)               2304      \n",
            "_________________________________________________________________\n",
            "d1 (Dense)                   (None, 256)               147712    \n",
            "_________________________________________________________________\n",
            "d2 (Dense)                   (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 120)               30840     \n",
            "=================================================================\n",
            "Total params: 367,128\n",
            "Trainable params: 365,880\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmN01uTuJH5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd256fad-05d9-4677-b962-c115b488327f"
      },
      "source": [
        "# x_train, y_train = next(train_gen)\n",
        "model = get_simple_cnn(120)\n",
        "\n",
        "for (x_train, y_train) in train_gen:\n",
        "  model = run_exp(model, x_train, y_train)\n",
        "  gc.collect()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg_bn_reg_seq\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "c2d_11 (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "c2d_12 (Conv2D)              (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "c2d_13 (Conv2D)              (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "c2d_21 (Conv2D)              (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "c2d_22 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "_________________________________________________________________\n",
            "c2d_23 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_55 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "c2d_31 (Conv2D)              (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "c2d_32 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "c2d_33 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 576)               2304      \n",
            "_________________________________________________________________\n",
            "d1 (Dense)                   (None, 256)               147712    \n",
            "_________________________________________________________________\n",
            "d2 (Dense)                   (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 120)               30840     \n",
            "=================================================================\n",
            "Total params: 367,128\n",
            "Trainable params: 365,880\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n",
            "processing batch: 0\n",
            "processing file: data_processed/0/airplane.npy - 0\n",
            "processing file: data_processed/0/apple.npy - 1\n",
            "processing file: data_processed/0/ant.npy - 2\n",
            "processing file: data_processed/0/axe.npy - 3\n",
            "processing file: data_processed/0/banana.npy - 4\n",
            "processing file: data_processed/0/baseball.npy - 5\n",
            "processing file: data_processed/0/bee.npy - 6\n",
            "processing file: data_processed/0/beach.npy - 7\n",
            "processing file: data_processed/0/bird.npy - 8\n",
            "processing file: data_processed/0/book.npy - 9\n",
            "processing file: data_processed/0/bucket.npy - 10\n",
            "processing file: data_processed/0/bus.npy - 11\n",
            "processing file: data_processed/0/butterfly.npy - 12\n",
            "processing file: data_processed/0/cactus.npy - 13\n",
            "processing file: data_processed/0/cake.npy - 14\n",
            "processing file: data_processed/0/cannon.npy - 15\n",
            "processing file: data_processed/0/car.npy - 16\n",
            "processing file: data_processed/0/carrot.npy - 17\n",
            "processing file: data_processed/0/castle.npy - 18\n",
            "processing file: data_processed/0/cat.npy - 19\n",
            "processing file: data_processed/0/chair.npy - 20\n",
            "processing file: data_processed/0/church.npy - 21\n",
            "processing file: data_processed/0/circle.npy - 22\n",
            "processing file: data_processed/0/clock.npy - 23\n",
            "processing file: data_processed/0/cloud.npy - 24\n",
            "processing file: data_processed/0/computer.npy - 25\n",
            "processing file: data_processed/0/cookie.npy - 26\n",
            "processing file: data_processed/0/cow.npy - 27\n",
            "processing file: data_processed/0/crab.npy - 28\n",
            "processing file: data_processed/0/crayon.npy - 29\n",
            "processing file: data_processed/0/cup.npy - 30\n",
            "processing file: data_processed/0/dolphin.npy - 31\n",
            "processing file: data_processed/0/donut.npy - 32\n",
            "processing file: data_processed/0/door.npy - 33\n",
            "processing file: data_processed/0/dragon.npy - 34\n",
            "processing file: data_processed/0/diamond.npy - 35\n",
            "processing file: data_processed/0/drums.npy - 36\n",
            "processing file: data_processed/0/duck.npy - 37\n",
            "processing file: data_processed/0/ear.npy - 38\n",
            "processing file: data_processed/0/elephant.npy - 39\n",
            "processing file: data_processed/0/envelope.npy - 40\n",
            "processing file: data_processed/0/eraser.npy - 41\n",
            "processing file: data_processed/0/eye.npy - 42\n",
            "processing file: data_processed/0/face.npy - 43\n",
            "processing file: data_processed/0/fan.npy - 44\n",
            "processing file: data_processed/0/finger.npy - 45\n",
            "processing file: data_processed/0/fish.npy - 46\n",
            "processing file: data_processed/0/flower.npy - 47\n",
            "processing file: data_processed/0/frog.npy - 48\n",
            "processing file: data_processed/0/garden.npy - 49\n",
            "processing file: data_processed/0/grapes.npy - 50\n",
            "processing file: data_processed/0/grass.npy - 51\n",
            "processing file: data_processed/0/guitar.npy - 52\n",
            "processing file: data_processed/0/hammer.npy - 53\n",
            "processing file: data_processed/0/hand.npy - 54\n",
            "processing file: data_processed/0/hat.npy - 55\n",
            "processing file: data_processed/0/helmet.npy - 56\n",
            "processing file: data_processed/0/horse.npy - 57\n",
            "processing file: data_processed/0/house.npy - 58\n",
            "processing file: data_processed/0/hospital.npy - 59\n",
            "processing file: data_processed/0/jail.npy - 60\n",
            "processing file: data_processed/0/kangaroo.npy - 61\n",
            "processing file: data_processed/0/knife.npy - 62\n",
            "processing file: data_processed/0/laptop.npy - 63\n",
            "processing file: data_processed/0/leg.npy - 64\n",
            "processing file: data_processed/0/lion.npy - 65\n",
            "processing file: data_processed/0/lollipop.npy - 66\n",
            "processing file: data_processed/0/map.npy - 67\n",
            "processing file: data_processed/0/microwave.npy - 68\n",
            "processing file: data_processed/0/monkey.npy - 69\n",
            "processing file: data_processed/0/moon.npy - 70\n",
            "processing file: data_processed/0/mountain.npy - 71\n",
            "processing file: data_processed/0/mouse.npy - 72\n",
            "processing file: data_processed/0/mug.npy - 73\n",
            "processing file: data_processed/0/nail.npy - 74\n",
            "processing file: data_processed/0/nose.npy - 75\n",
            "processing file: data_processed/0/onion.npy - 76\n",
            "processing file: data_processed/0/owl.npy - 77\n",
            "processing file: data_processed/0/panda.npy - 78\n",
            "processing file: data_processed/0/parachute.npy - 79\n",
            "processing file: data_processed/0/parrot.npy - 80\n",
            "processing file: data_processed/0/passport.npy - 81\n",
            "processing file: data_processed/0/peanut.npy - 82\n",
            "processing file: data_processed/0/pencil.npy - 83\n",
            "processing file: data_processed/0/piano.npy - 84\n",
            "processing file: data_processed/0/pig.npy - 85\n",
            "processing file: data_processed/0/pizza.npy - 86\n",
            "processing file: data_processed/0/potato.npy - 87\n",
            "processing file: data_processed/0/rabbit.npy - 88\n",
            "processing file: data_processed/0/radio.npy - 89\n",
            "processing file: data_processed/0/rain.npy - 90\n",
            "processing file: data_processed/0/rainbow.npy - 91\n",
            "processing file: data_processed/0/river.npy - 92\n",
            "processing file: data_processed/0/sandwich.npy - 93\n",
            "processing file: data_processed/0/saw.npy - 94\n",
            "processing file: data_processed/0/sun.npy - 95\n",
            "processing file: data_processed/0/shark.npy - 96\n",
            "processing file: data_processed/0/shoe.npy - 97\n",
            "processing file: data_processed/0/skull.npy - 98\n",
            "processing file: data_processed/0/snail.npy - 99\n",
            "processing file: data_processed/0/snowflake.npy - 100\n",
            "processing file: data_processed/0/spider.npy - 101\n",
            "processing file: data_processed/0/square.npy - 102\n",
            "processing file: data_processed/0/stairs.npy - 103\n",
            "processing file: data_processed/0/star.npy - 104\n",
            "processing file: data_processed/0/stove.npy - 105\n",
            "processing file: data_processed/0/strawberry.npy - 106\n",
            "processing file: data_processed/0/table.npy - 107\n",
            "processing file: data_processed/0/teapot.npy - 108\n",
            "processing file: data_processed/0/television.npy - 109\n",
            "processing file: data_processed/0/tent.npy - 110\n",
            "processing file: data_processed/0/toilet.npy - 111\n",
            "processing file: data_processed/0/toothbrush.npy - 112\n",
            "processing file: data_processed/0/toothpaste.npy - 113\n",
            "processing file: data_processed/0/train.npy - 114\n",
            "processing file: data_processed/0/triangle.npy - 115\n",
            "processing file: data_processed/0/truck.npy - 116\n",
            "processing file: data_processed/0/umbrella.npy - 117\n",
            "processing file: data_processed/0/whale.npy - 118\n",
            "processing file: data_processed/0/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 2.2477 - accuracy: 0.4468 - val_loss: 1.4200 - val_accuracy: 0.6322\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 1.5252 - accuracy: 0.6084 - val_loss: 1.2177 - val_accuracy: 0.6823\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.3514 - accuracy: 0.6501 - val_loss: 1.0939 - val_accuracy: 0.7126\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.1684 - accuracy: 0.6955 - val_loss: 0.9644 - val_accuracy: 0.7467\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.1244 - accuracy: 0.7064 - val_loss: 0.9485 - val_accuracy: 0.7514\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.1081 - accuracy: 0.7105 - val_loss: 0.9337 - val_accuracy: 0.7545\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0921 - accuracy: 0.7134 - val_loss: 0.9232 - val_accuracy: 0.7573\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0784 - accuracy: 0.7179 - val_loss: 0.9209 - val_accuracy: 0.7555\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0669 - accuracy: 0.7193 - val_loss: 0.9092 - val_accuracy: 0.7583\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0580 - accuracy: 0.7225 - val_loss: 0.9072 - val_accuracy: 0.7605\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.9081 - accuracy: 0.7630\n",
            "Test accuarcy: 76.30%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79      2000\n",
            "           1       0.88      0.92      0.90      2000\n",
            "           2       0.65      0.77      0.70      2000\n",
            "           3       0.81      0.75      0.78      2000\n",
            "           4       0.86      0.79      0.82      2000\n",
            "           5       0.84      0.77      0.81      2000\n",
            "           6       0.77      0.74      0.75      2000\n",
            "           7       0.54      0.65      0.59      2000\n",
            "           8       0.52      0.36      0.42      2000\n",
            "           9       0.75      0.74      0.75      2000\n",
            "          10       0.85      0.84      0.84      2000\n",
            "          11       0.78      0.85      0.81      2000\n",
            "          12       0.87      0.91      0.89      2000\n",
            "          13       0.82      0.80      0.81      2000\n",
            "          14       0.78      0.87      0.82      2000\n",
            "          15       0.67      0.69      0.68      2000\n",
            "          16       0.79      0.83      0.81      2000\n",
            "          17       0.83      0.87      0.85      2000\n",
            "          18       0.81      0.81      0.81      2000\n",
            "          19       0.78      0.65      0.71      2000\n",
            "          20       0.89      0.89      0.89      2000\n",
            "          21       0.77      0.68      0.72      2000\n",
            "          22       0.76      0.85      0.80      2000\n",
            "          23       0.85      0.87      0.86      2000\n",
            "          24       0.80      0.88      0.84      2000\n",
            "          25       0.75      0.68      0.71      2000\n",
            "          26       0.71      0.81      0.75      2000\n",
            "          27       0.52      0.72      0.61      2000\n",
            "          28       0.72      0.68      0.70      2000\n",
            "          29       0.66      0.47      0.55      2000\n",
            "          30       0.65      0.46      0.54      2000\n",
            "          31       0.64      0.61      0.63      2000\n",
            "          32       0.84      0.92      0.87      2000\n",
            "          33       0.88      0.86      0.87      2000\n",
            "          34       0.44      0.44      0.44      2000\n",
            "          35       0.88      0.85      0.87      2000\n",
            "          36       0.67      0.72      0.69      2000\n",
            "          37       0.66      0.65      0.66      2000\n",
            "          38       0.84      0.79      0.81      2000\n",
            "          39       0.70      0.65      0.67      2000\n",
            "          40       0.98      0.94      0.96      2000\n",
            "          41       0.67      0.59      0.63      2000\n",
            "          42       0.89      0.84      0.87      2000\n",
            "          43       0.83      0.77      0.80      2000\n",
            "          44       0.74      0.59      0.65      2000\n",
            "          45       0.79      0.72      0.75      2000\n",
            "          46       0.83      0.83      0.83      2000\n",
            "          47       0.71      0.84      0.77      2000\n",
            "          48       0.56      0.39      0.46      2000\n",
            "          49       0.50      0.66      0.57      2000\n",
            "          50       0.54      0.85      0.66      2000\n",
            "          51       0.75      0.82      0.78      2000\n",
            "          52       0.83      0.89      0.86      2000\n",
            "          53       0.80      0.80      0.80      2000\n",
            "          54       0.87      0.86      0.86      2000\n",
            "          55       0.86      0.77      0.81      2000\n",
            "          56       0.79      0.70      0.75      2000\n",
            "          57       0.76      0.70      0.73      2000\n",
            "          58       0.86      0.91      0.88      2000\n",
            "          59       0.75      0.76      0.75      2000\n",
            "          60       0.84      0.89      0.86      2000\n",
            "          61       0.70      0.79      0.74      2000\n",
            "          62       0.82      0.74      0.78      2000\n",
            "          63       0.74      0.79      0.76      2000\n",
            "          64       0.77      0.84      0.80      2000\n",
            "          65       0.64      0.75      0.69      2000\n",
            "          66       0.93      0.92      0.93      2000\n",
            "          67       0.74      0.78      0.76      2000\n",
            "          68       0.86      0.88      0.87      2000\n",
            "          69       0.54      0.62      0.58      2000\n",
            "          70       0.76      0.54      0.63      2000\n",
            "          71       0.87      0.86      0.86      2000\n",
            "          72       0.56      0.46      0.50      2000\n",
            "          73       0.68      0.78      0.73      2000\n",
            "          74       0.85      0.58      0.69      2000\n",
            "          75       0.85      0.74      0.79      2000\n",
            "          76       0.81      0.74      0.78      2000\n",
            "          77       0.68      0.76      0.71      2000\n",
            "          78       0.73      0.68      0.70      2000\n",
            "          79       0.83      0.82      0.82      2000\n",
            "          80       0.71      0.58      0.64      2000\n",
            "          81       0.66      0.67      0.66      2000\n",
            "          82       0.83      0.77      0.80      2000\n",
            "          83       0.60      0.73      0.66      2000\n",
            "          84       0.74      0.59      0.66      2000\n",
            "          85       0.64      0.66      0.65      2000\n",
            "          86       0.70      0.81      0.75      2000\n",
            "          87       0.68      0.65      0.66      2000\n",
            "          88       0.75      0.74      0.75      2000\n",
            "          89       0.81      0.83      0.82      2000\n",
            "          90       0.82      0.89      0.85      2000\n",
            "          91       0.91      0.94      0.93      2000\n",
            "          92       0.67      0.77      0.72      2000\n",
            "          93       0.76      0.73      0.75      2000\n",
            "          94       0.78      0.78      0.78      2000\n",
            "          95       0.84      0.89      0.87      2000\n",
            "          96       0.72      0.69      0.71      2000\n",
            "          97       0.81      0.78      0.80      2000\n",
            "          98       0.87      0.90      0.89      2000\n",
            "          99       0.83      0.90      0.86      2000\n",
            "         100       0.84      0.85      0.85      2000\n",
            "         101       0.76      0.72      0.74      2000\n",
            "         102       0.90      0.90      0.90      2000\n",
            "         103       0.95      0.90      0.92      2000\n",
            "         104       0.90      0.89      0.89      2000\n",
            "         105       0.73      0.74      0.73      2000\n",
            "         106       0.89      0.87      0.88      2000\n",
            "         107       0.88      0.87      0.87      2000\n",
            "         108       0.83      0.83      0.83      2000\n",
            "         109       0.88      0.87      0.87      2000\n",
            "         110       0.88      0.80      0.84      2000\n",
            "         111       0.83      0.82      0.82      2000\n",
            "         112       0.73      0.82      0.78      2000\n",
            "         113       0.63      0.53      0.57      2000\n",
            "         114       0.64      0.74      0.69      2000\n",
            "         115       0.88      0.91      0.90      2000\n",
            "         116       0.79      0.69      0.73      2000\n",
            "         117       0.93      0.91      0.92      2000\n",
            "         118       0.70      0.70      0.70      2000\n",
            "         119       0.76      0.78      0.77      2000\n",
            "\n",
            "    accuracy                           0.76    240000\n",
            "   macro avg       0.77      0.76      0.76    240000\n",
            "weighted avg       0.77      0.76      0.76    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.23698333333333332\n",
            "processing batch: 1\n",
            "processing file: data_processed/1/airplane.npy - 0\n",
            "processing file: data_processed/1/apple.npy - 1\n",
            "processing file: data_processed/1/ant.npy - 2\n",
            "processing file: data_processed/1/axe.npy - 3\n",
            "processing file: data_processed/1/banana.npy - 4\n",
            "processing file: data_processed/1/baseball.npy - 5\n",
            "processing file: data_processed/1/bee.npy - 6\n",
            "processing file: data_processed/1/beach.npy - 7\n",
            "processing file: data_processed/1/bird.npy - 8\n",
            "processing file: data_processed/1/book.npy - 9\n",
            "processing file: data_processed/1/bucket.npy - 10\n",
            "processing file: data_processed/1/bus.npy - 11\n",
            "processing file: data_processed/1/butterfly.npy - 12\n",
            "processing file: data_processed/1/cactus.npy - 13\n",
            "processing file: data_processed/1/cake.npy - 14\n",
            "processing file: data_processed/1/cannon.npy - 15\n",
            "processing file: data_processed/1/car.npy - 16\n",
            "processing file: data_processed/1/carrot.npy - 17\n",
            "processing file: data_processed/1/castle.npy - 18\n",
            "processing file: data_processed/1/cat.npy - 19\n",
            "processing file: data_processed/1/chair.npy - 20\n",
            "processing file: data_processed/1/church.npy - 21\n",
            "processing file: data_processed/1/circle.npy - 22\n",
            "processing file: data_processed/1/clock.npy - 23\n",
            "processing file: data_processed/1/cloud.npy - 24\n",
            "processing file: data_processed/1/computer.npy - 25\n",
            "processing file: data_processed/1/cookie.npy - 26\n",
            "processing file: data_processed/1/cow.npy - 27\n",
            "processing file: data_processed/1/crab.npy - 28\n",
            "processing file: data_processed/1/crayon.npy - 29\n",
            "processing file: data_processed/1/cup.npy - 30\n",
            "processing file: data_processed/1/dolphin.npy - 31\n",
            "processing file: data_processed/1/donut.npy - 32\n",
            "processing file: data_processed/1/door.npy - 33\n",
            "processing file: data_processed/1/dragon.npy - 34\n",
            "processing file: data_processed/1/diamond.npy - 35\n",
            "processing file: data_processed/1/drums.npy - 36\n",
            "processing file: data_processed/1/duck.npy - 37\n",
            "processing file: data_processed/1/ear.npy - 38\n",
            "processing file: data_processed/1/elephant.npy - 39\n",
            "processing file: data_processed/1/envelope.npy - 40\n",
            "processing file: data_processed/1/eraser.npy - 41\n",
            "processing file: data_processed/1/eye.npy - 42\n",
            "processing file: data_processed/1/face.npy - 43\n",
            "processing file: data_processed/1/fan.npy - 44\n",
            "processing file: data_processed/1/finger.npy - 45\n",
            "processing file: data_processed/1/fish.npy - 46\n",
            "processing file: data_processed/1/flower.npy - 47\n",
            "processing file: data_processed/1/frog.npy - 48\n",
            "processing file: data_processed/1/garden.npy - 49\n",
            "processing file: data_processed/1/grapes.npy - 50\n",
            "processing file: data_processed/1/grass.npy - 51\n",
            "processing file: data_processed/1/guitar.npy - 52\n",
            "processing file: data_processed/1/hammer.npy - 53\n",
            "processing file: data_processed/1/hand.npy - 54\n",
            "processing file: data_processed/1/hat.npy - 55\n",
            "processing file: data_processed/1/helmet.npy - 56\n",
            "processing file: data_processed/1/horse.npy - 57\n",
            "processing file: data_processed/1/house.npy - 58\n",
            "processing file: data_processed/1/hospital.npy - 59\n",
            "processing file: data_processed/1/jail.npy - 60\n",
            "processing file: data_processed/1/kangaroo.npy - 61\n",
            "processing file: data_processed/1/knife.npy - 62\n",
            "processing file: data_processed/1/laptop.npy - 63\n",
            "processing file: data_processed/1/leg.npy - 64\n",
            "processing file: data_processed/1/lion.npy - 65\n",
            "processing file: data_processed/1/lollipop.npy - 66\n",
            "processing file: data_processed/1/map.npy - 67\n",
            "processing file: data_processed/1/microwave.npy - 68\n",
            "processing file: data_processed/1/monkey.npy - 69\n",
            "processing file: data_processed/1/moon.npy - 70\n",
            "processing file: data_processed/1/mountain.npy - 71\n",
            "processing file: data_processed/1/mouse.npy - 72\n",
            "processing file: data_processed/1/mug.npy - 73\n",
            "processing file: data_processed/1/nail.npy - 74\n",
            "processing file: data_processed/1/nose.npy - 75\n",
            "processing file: data_processed/1/onion.npy - 76\n",
            "processing file: data_processed/1/owl.npy - 77\n",
            "processing file: data_processed/1/panda.npy - 78\n",
            "processing file: data_processed/1/parachute.npy - 79\n",
            "processing file: data_processed/1/parrot.npy - 80\n",
            "processing file: data_processed/1/passport.npy - 81\n",
            "processing file: data_processed/1/peanut.npy - 82\n",
            "processing file: data_processed/1/pencil.npy - 83\n",
            "processing file: data_processed/1/piano.npy - 84\n",
            "processing file: data_processed/1/pig.npy - 85\n",
            "processing file: data_processed/1/pizza.npy - 86\n",
            "processing file: data_processed/1/potato.npy - 87\n",
            "processing file: data_processed/1/rabbit.npy - 88\n",
            "processing file: data_processed/1/radio.npy - 89\n",
            "processing file: data_processed/1/rain.npy - 90\n",
            "processing file: data_processed/1/rainbow.npy - 91\n",
            "processing file: data_processed/1/river.npy - 92\n",
            "processing file: data_processed/1/sandwich.npy - 93\n",
            "processing file: data_processed/1/saw.npy - 94\n",
            "processing file: data_processed/1/sun.npy - 95\n",
            "processing file: data_processed/1/shark.npy - 96\n",
            "processing file: data_processed/1/shoe.npy - 97\n",
            "processing file: data_processed/1/skull.npy - 98\n",
            "processing file: data_processed/1/snail.npy - 99\n",
            "processing file: data_processed/1/snowflake.npy - 100\n",
            "processing file: data_processed/1/spider.npy - 101\n",
            "processing file: data_processed/1/square.npy - 102\n",
            "processing file: data_processed/1/stairs.npy - 103\n",
            "processing file: data_processed/1/star.npy - 104\n",
            "processing file: data_processed/1/stove.npy - 105\n",
            "processing file: data_processed/1/strawberry.npy - 106\n",
            "processing file: data_processed/1/table.npy - 107\n",
            "processing file: data_processed/1/teapot.npy - 108\n",
            "processing file: data_processed/1/television.npy - 109\n",
            "processing file: data_processed/1/tent.npy - 110\n",
            "processing file: data_processed/1/toilet.npy - 111\n",
            "processing file: data_processed/1/toothbrush.npy - 112\n",
            "processing file: data_processed/1/toothpaste.npy - 113\n",
            "processing file: data_processed/1/train.npy - 114\n",
            "processing file: data_processed/1/triangle.npy - 115\n",
            "processing file: data_processed/1/truck.npy - 116\n",
            "processing file: data_processed/1/umbrella.npy - 117\n",
            "processing file: data_processed/1/whale.npy - 118\n",
            "processing file: data_processed/1/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.1364 - accuracy: 0.7063 - val_loss: 0.8952 - val_accuracy: 0.7662\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.1109 - accuracy: 0.7117 - val_loss: 0.8900 - val_accuracy: 0.7677\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0969 - accuracy: 0.7156 - val_loss: 0.8841 - val_accuracy: 0.7692\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0815 - accuracy: 0.7183 - val_loss: 0.8844 - val_accuracy: 0.7680\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0723 - accuracy: 0.7210 - val_loss: 0.8771 - val_accuracy: 0.7707\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0596 - accuracy: 0.7240 - val_loss: 0.8662 - val_accuracy: 0.7736\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0517 - accuracy: 0.7256 - val_loss: 0.8638 - val_accuracy: 0.7753\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0391 - accuracy: 0.7281 - val_loss: 0.8571 - val_accuracy: 0.7779\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0328 - accuracy: 0.7304 - val_loss: 0.8523 - val_accuracy: 0.7777\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0222 - accuracy: 0.7325 - val_loss: 0.8529 - val_accuracy: 0.7775\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.8525 - accuracy: 0.7758\n",
            "Test accuarcy: 77.58%\n",
            "----------\n",
            "7500/7500 - 14s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.79      2000\n",
            "           1       0.90      0.92      0.91      2000\n",
            "           2       0.67      0.79      0.72      2000\n",
            "           3       0.80      0.77      0.79      2000\n",
            "           4       0.85      0.82      0.83      2000\n",
            "           5       0.82      0.82      0.82      2000\n",
            "           6       0.74      0.79      0.76      2000\n",
            "           7       0.63      0.65      0.64      2000\n",
            "           8       0.54      0.33      0.41      2000\n",
            "           9       0.81      0.73      0.76      2000\n",
            "          10       0.88      0.82      0.85      2000\n",
            "          11       0.75      0.87      0.81      2000\n",
            "          12       0.90      0.91      0.91      2000\n",
            "          13       0.84      0.82      0.83      2000\n",
            "          14       0.83      0.86      0.85      2000\n",
            "          15       0.68      0.71      0.70      2000\n",
            "          16       0.83      0.81      0.82      2000\n",
            "          17       0.88      0.86      0.87      2000\n",
            "          18       0.77      0.85      0.81      2000\n",
            "          19       0.80      0.67      0.73      2000\n",
            "          20       0.88      0.91      0.89      2000\n",
            "          21       0.76      0.71      0.73      2000\n",
            "          22       0.75      0.87      0.80      2000\n",
            "          23       0.89      0.86      0.88      2000\n",
            "          24       0.78      0.90      0.84      2000\n",
            "          25       0.76      0.70      0.73      2000\n",
            "          26       0.70      0.84      0.76      2000\n",
            "          27       0.56      0.73      0.64      2000\n",
            "          28       0.73      0.70      0.72      2000\n",
            "          29       0.63      0.50      0.56      2000\n",
            "          30       0.65      0.51      0.57      2000\n",
            "          31       0.72      0.57      0.64      2000\n",
            "          32       0.86      0.92      0.89      2000\n",
            "          33       0.89      0.86      0.88      2000\n",
            "          34       0.45      0.47      0.46      2000\n",
            "          35       0.88      0.87      0.87      2000\n",
            "          36       0.68      0.74      0.71      2000\n",
            "          37       0.63      0.71      0.67      2000\n",
            "          38       0.86      0.80      0.83      2000\n",
            "          39       0.67      0.73      0.70      2000\n",
            "          40       0.98      0.94      0.96      2000\n",
            "          41       0.67      0.61      0.64      2000\n",
            "          42       0.88      0.88      0.88      2000\n",
            "          43       0.87      0.77      0.81      2000\n",
            "          44       0.78      0.61      0.68      2000\n",
            "          45       0.77      0.75      0.76      2000\n",
            "          46       0.84      0.85      0.85      2000\n",
            "          47       0.76      0.86      0.81      2000\n",
            "          48       0.55      0.43      0.48      2000\n",
            "          49       0.52      0.68      0.59      2000\n",
            "          50       0.60      0.85      0.71      2000\n",
            "          51       0.75      0.85      0.80      2000\n",
            "          52       0.86      0.90      0.88      2000\n",
            "          53       0.82      0.80      0.81      2000\n",
            "          54       0.85      0.86      0.86      2000\n",
            "          55       0.84      0.80      0.82      2000\n",
            "          56       0.80      0.72      0.76      2000\n",
            "          57       0.76      0.71      0.74      2000\n",
            "          58       0.87      0.90      0.89      2000\n",
            "          59       0.70      0.81      0.75      2000\n",
            "          60       0.81      0.91      0.86      2000\n",
            "          61       0.74      0.80      0.77      2000\n",
            "          62       0.80      0.78      0.79      2000\n",
            "          63       0.75      0.80      0.77      2000\n",
            "          64       0.76      0.86      0.81      2000\n",
            "          65       0.68      0.76      0.72      2000\n",
            "          66       0.92      0.93      0.92      2000\n",
            "          67       0.76      0.80      0.78      2000\n",
            "          68       0.84      0.90      0.87      2000\n",
            "          69       0.56      0.66      0.61      2000\n",
            "          70       0.79      0.53      0.64      2000\n",
            "          71       0.86      0.87      0.86      2000\n",
            "          72       0.58      0.48      0.53      2000\n",
            "          73       0.70      0.75      0.72      2000\n",
            "          74       0.85      0.58      0.69      2000\n",
            "          75       0.85      0.75      0.80      2000\n",
            "          76       0.80      0.79      0.79      2000\n",
            "          77       0.74      0.73      0.74      2000\n",
            "          78       0.73      0.71      0.72      2000\n",
            "          79       0.86      0.82      0.84      2000\n",
            "          80       0.70      0.61      0.65      2000\n",
            "          81       0.69      0.67      0.68      2000\n",
            "          82       0.84      0.78      0.81      2000\n",
            "          83       0.61      0.74      0.67      2000\n",
            "          84       0.75      0.61      0.68      2000\n",
            "          85       0.65      0.68      0.67      2000\n",
            "          86       0.77      0.81      0.79      2000\n",
            "          87       0.70      0.61      0.66      2000\n",
            "          88       0.76      0.76      0.76      2000\n",
            "          89       0.83      0.85      0.84      2000\n",
            "          90       0.80      0.91      0.85      2000\n",
            "          91       0.91      0.94      0.93      2000\n",
            "          92       0.78      0.73      0.76      2000\n",
            "          93       0.77      0.74      0.75      2000\n",
            "          94       0.80      0.80      0.80      2000\n",
            "          95       0.85      0.90      0.88      2000\n",
            "          96       0.72      0.72      0.72      2000\n",
            "          97       0.81      0.81      0.81      2000\n",
            "          98       0.87      0.91      0.89      2000\n",
            "          99       0.87      0.90      0.88      2000\n",
            "         100       0.84      0.87      0.85      2000\n",
            "         101       0.81      0.73      0.77      2000\n",
            "         102       0.89      0.92      0.90      2000\n",
            "         103       0.94      0.91      0.92      2000\n",
            "         104       0.92      0.89      0.90      2000\n",
            "         105       0.75      0.74      0.75      2000\n",
            "         106       0.90      0.86      0.88      2000\n",
            "         107       0.84      0.89      0.87      2000\n",
            "         108       0.83      0.84      0.83      2000\n",
            "         109       0.91      0.86      0.88      2000\n",
            "         110       0.89      0.80      0.84      2000\n",
            "         111       0.85      0.82      0.83      2000\n",
            "         112       0.72      0.86      0.78      2000\n",
            "         113       0.71      0.53      0.61      2000\n",
            "         114       0.63      0.75      0.69      2000\n",
            "         115       0.88      0.92      0.90      2000\n",
            "         116       0.76      0.73      0.74      2000\n",
            "         117       0.92      0.92      0.92      2000\n",
            "         118       0.75      0.70      0.72      2000\n",
            "         119       0.79      0.80      0.80      2000\n",
            "\n",
            "    accuracy                           0.78    240000\n",
            "   macro avg       0.78      0.78      0.77    240000\n",
            "weighted avg       0.78      0.78      0.77    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.2242\n",
            "processing batch: 2\n",
            "processing file: data_processed/2/airplane.npy - 0\n",
            "processing file: data_processed/2/apple.npy - 1\n",
            "processing file: data_processed/2/ant.npy - 2\n",
            "processing file: data_processed/2/axe.npy - 3\n",
            "processing file: data_processed/2/banana.npy - 4\n",
            "processing file: data_processed/2/baseball.npy - 5\n",
            "processing file: data_processed/2/bee.npy - 6\n",
            "processing file: data_processed/2/beach.npy - 7\n",
            "processing file: data_processed/2/bird.npy - 8\n",
            "processing file: data_processed/2/book.npy - 9\n",
            "processing file: data_processed/2/bucket.npy - 10\n",
            "processing file: data_processed/2/bus.npy - 11\n",
            "processing file: data_processed/2/butterfly.npy - 12\n",
            "processing file: data_processed/2/cactus.npy - 13\n",
            "processing file: data_processed/2/cake.npy - 14\n",
            "processing file: data_processed/2/cannon.npy - 15\n",
            "processing file: data_processed/2/car.npy - 16\n",
            "processing file: data_processed/2/carrot.npy - 17\n",
            "processing file: data_processed/2/castle.npy - 18\n",
            "processing file: data_processed/2/cat.npy - 19\n",
            "processing file: data_processed/2/chair.npy - 20\n",
            "processing file: data_processed/2/church.npy - 21\n",
            "processing file: data_processed/2/circle.npy - 22\n",
            "processing file: data_processed/2/clock.npy - 23\n",
            "processing file: data_processed/2/cloud.npy - 24\n",
            "processing file: data_processed/2/computer.npy - 25\n",
            "processing file: data_processed/2/cookie.npy - 26\n",
            "processing file: data_processed/2/cow.npy - 27\n",
            "processing file: data_processed/2/crab.npy - 28\n",
            "processing file: data_processed/2/crayon.npy - 29\n",
            "processing file: data_processed/2/cup.npy - 30\n",
            "processing file: data_processed/2/dolphin.npy - 31\n",
            "processing file: data_processed/2/donut.npy - 32\n",
            "processing file: data_processed/2/door.npy - 33\n",
            "processing file: data_processed/2/dragon.npy - 34\n",
            "processing file: data_processed/2/diamond.npy - 35\n",
            "processing file: data_processed/2/drums.npy - 36\n",
            "processing file: data_processed/2/duck.npy - 37\n",
            "processing file: data_processed/2/ear.npy - 38\n",
            "processing file: data_processed/2/elephant.npy - 39\n",
            "processing file: data_processed/2/envelope.npy - 40\n",
            "processing file: data_processed/2/eraser.npy - 41\n",
            "processing file: data_processed/2/eye.npy - 42\n",
            "processing file: data_processed/2/face.npy - 43\n",
            "processing file: data_processed/2/fan.npy - 44\n",
            "processing file: data_processed/2/finger.npy - 45\n",
            "processing file: data_processed/2/fish.npy - 46\n",
            "processing file: data_processed/2/flower.npy - 47\n",
            "processing file: data_processed/2/frog.npy - 48\n",
            "processing file: data_processed/2/garden.npy - 49\n",
            "processing file: data_processed/2/grapes.npy - 50\n",
            "processing file: data_processed/2/grass.npy - 51\n",
            "processing file: data_processed/2/guitar.npy - 52\n",
            "processing file: data_processed/2/hammer.npy - 53\n",
            "processing file: data_processed/2/hand.npy - 54\n",
            "processing file: data_processed/2/hat.npy - 55\n",
            "processing file: data_processed/2/helmet.npy - 56\n",
            "processing file: data_processed/2/horse.npy - 57\n",
            "processing file: data_processed/2/house.npy - 58\n",
            "processing file: data_processed/2/hospital.npy - 59\n",
            "processing file: data_processed/2/jail.npy - 60\n",
            "processing file: data_processed/2/kangaroo.npy - 61\n",
            "processing file: data_processed/2/knife.npy - 62\n",
            "processing file: data_processed/2/laptop.npy - 63\n",
            "processing file: data_processed/2/leg.npy - 64\n",
            "processing file: data_processed/2/lion.npy - 65\n",
            "processing file: data_processed/2/lollipop.npy - 66\n",
            "processing file: data_processed/2/map.npy - 67\n",
            "processing file: data_processed/2/microwave.npy - 68\n",
            "processing file: data_processed/2/monkey.npy - 69\n",
            "processing file: data_processed/2/moon.npy - 70\n",
            "processing file: data_processed/2/mountain.npy - 71\n",
            "processing file: data_processed/2/mouse.npy - 72\n",
            "processing file: data_processed/2/mug.npy - 73\n",
            "processing file: data_processed/2/nail.npy - 74\n",
            "processing file: data_processed/2/nose.npy - 75\n",
            "processing file: data_processed/2/onion.npy - 76\n",
            "processing file: data_processed/2/owl.npy - 77\n",
            "processing file: data_processed/2/panda.npy - 78\n",
            "processing file: data_processed/2/parachute.npy - 79\n",
            "processing file: data_processed/2/parrot.npy - 80\n",
            "processing file: data_processed/2/passport.npy - 81\n",
            "processing file: data_processed/2/peanut.npy - 82\n",
            "processing file: data_processed/2/pencil.npy - 83\n",
            "processing file: data_processed/2/piano.npy - 84\n",
            "processing file: data_processed/2/pig.npy - 85\n",
            "processing file: data_processed/2/pizza.npy - 86\n",
            "processing file: data_processed/2/potato.npy - 87\n",
            "processing file: data_processed/2/rabbit.npy - 88\n",
            "processing file: data_processed/2/radio.npy - 89\n",
            "processing file: data_processed/2/rain.npy - 90\n",
            "processing file: data_processed/2/rainbow.npy - 91\n",
            "processing file: data_processed/2/river.npy - 92\n",
            "processing file: data_processed/2/sandwich.npy - 93\n",
            "processing file: data_processed/2/saw.npy - 94\n",
            "processing file: data_processed/2/sun.npy - 95\n",
            "processing file: data_processed/2/shark.npy - 96\n",
            "processing file: data_processed/2/shoe.npy - 97\n",
            "processing file: data_processed/2/skull.npy - 98\n",
            "processing file: data_processed/2/snail.npy - 99\n",
            "processing file: data_processed/2/snowflake.npy - 100\n",
            "processing file: data_processed/2/spider.npy - 101\n",
            "processing file: data_processed/2/square.npy - 102\n",
            "processing file: data_processed/2/stairs.npy - 103\n",
            "processing file: data_processed/2/star.npy - 104\n",
            "processing file: data_processed/2/stove.npy - 105\n",
            "processing file: data_processed/2/strawberry.npy - 106\n",
            "processing file: data_processed/2/table.npy - 107\n",
            "processing file: data_processed/2/teapot.npy - 108\n",
            "processing file: data_processed/2/television.npy - 109\n",
            "processing file: data_processed/2/tent.npy - 110\n",
            "processing file: data_processed/2/toilet.npy - 111\n",
            "processing file: data_processed/2/toothbrush.npy - 112\n",
            "processing file: data_processed/2/toothpaste.npy - 113\n",
            "processing file: data_processed/2/train.npy - 114\n",
            "processing file: data_processed/2/triangle.npy - 115\n",
            "processing file: data_processed/2/truck.npy - 116\n",
            "processing file: data_processed/2/umbrella.npy - 117\n",
            "processing file: data_processed/2/whale.npy - 118\n",
            "processing file: data_processed/2/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0617 - accuracy: 0.7254 - val_loss: 0.8630 - val_accuracy: 0.7742\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 1.0458 - accuracy: 0.7270 - val_loss: 0.8535 - val_accuracy: 0.7763\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 1.0316 - accuracy: 0.7310 - val_loss: 0.8513 - val_accuracy: 0.7770\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 1.0197 - accuracy: 0.7346 - val_loss: 0.8521 - val_accuracy: 0.7773\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 1.0123 - accuracy: 0.7349 - val_loss: 0.8550 - val_accuracy: 0.7772\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9997 - accuracy: 0.7386 - val_loss: 0.8427 - val_accuracy: 0.7798\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9949 - accuracy: 0.7395 - val_loss: 0.8459 - val_accuracy: 0.7783\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9839 - accuracy: 0.7425 - val_loss: 0.8482 - val_accuracy: 0.7792\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9773 - accuracy: 0.7437 - val_loss: 0.8357 - val_accuracy: 0.7830\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9700 - accuracy: 0.7450 - val_loss: 0.8328 - val_accuracy: 0.7830\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.8144 - accuracy: 0.7864\n",
            "Test accuarcy: 78.64%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.82      0.80      2000\n",
            "           1       0.91      0.92      0.92      2000\n",
            "           2       0.75      0.78      0.77      2000\n",
            "           3       0.79      0.79      0.79      2000\n",
            "           4       0.85      0.82      0.84      2000\n",
            "           5       0.86      0.81      0.83      2000\n",
            "           6       0.77      0.81      0.79      2000\n",
            "           7       0.69      0.62      0.65      2000\n",
            "           8       0.56      0.36      0.44      2000\n",
            "           9       0.79      0.77      0.78      2000\n",
            "          10       0.86      0.85      0.85      2000\n",
            "          11       0.81      0.86      0.83      2000\n",
            "          12       0.90      0.92      0.91      2000\n",
            "          13       0.85      0.82      0.84      2000\n",
            "          14       0.85      0.87      0.86      2000\n",
            "          15       0.71      0.74      0.72      2000\n",
            "          16       0.79      0.84      0.82      2000\n",
            "          17       0.86      0.88      0.87      2000\n",
            "          18       0.84      0.83      0.83      2000\n",
            "          19       0.79      0.69      0.74      2000\n",
            "          20       0.88      0.91      0.89      2000\n",
            "          21       0.78      0.71      0.74      2000\n",
            "          22       0.76      0.86      0.81      2000\n",
            "          23       0.88      0.88      0.88      2000\n",
            "          24       0.88      0.87      0.87      2000\n",
            "          25       0.79      0.66      0.72      2000\n",
            "          26       0.75      0.80      0.77      2000\n",
            "          27       0.62      0.71      0.66      2000\n",
            "          28       0.75      0.72      0.74      2000\n",
            "          29       0.63      0.50      0.56      2000\n",
            "          30       0.66      0.48      0.55      2000\n",
            "          31       0.74      0.60      0.66      2000\n",
            "          32       0.89      0.91      0.90      2000\n",
            "          33       0.90      0.86      0.88      2000\n",
            "          34       0.47      0.50      0.48      2000\n",
            "          35       0.87      0.89      0.88      2000\n",
            "          36       0.68      0.78      0.73      2000\n",
            "          37       0.64      0.73      0.68      2000\n",
            "          38       0.90      0.80      0.84      2000\n",
            "          39       0.74      0.71      0.72      2000\n",
            "          40       0.98      0.95      0.96      2000\n",
            "          41       0.60      0.67      0.63      2000\n",
            "          42       0.89      0.89      0.89      2000\n",
            "          43       0.85      0.81      0.83      2000\n",
            "          44       0.74      0.67      0.70      2000\n",
            "          45       0.80      0.75      0.77      2000\n",
            "          46       0.81      0.86      0.84      2000\n",
            "          47       0.79      0.86      0.82      2000\n",
            "          48       0.56      0.45      0.50      2000\n",
            "          49       0.55      0.68      0.61      2000\n",
            "          50       0.65      0.85      0.74      2000\n",
            "          51       0.80      0.82      0.81      2000\n",
            "          52       0.86      0.92      0.89      2000\n",
            "          53       0.83      0.79      0.81      2000\n",
            "          54       0.88      0.86      0.87      2000\n",
            "          55       0.84      0.82      0.83      2000\n",
            "          56       0.80      0.73      0.77      2000\n",
            "          57       0.75      0.77      0.76      2000\n",
            "          58       0.88      0.91      0.89      2000\n",
            "          59       0.75      0.80      0.77      2000\n",
            "          60       0.85      0.90      0.87      2000\n",
            "          61       0.78      0.78      0.78      2000\n",
            "          62       0.81      0.77      0.79      2000\n",
            "          63       0.71      0.86      0.78      2000\n",
            "          64       0.81      0.83      0.82      2000\n",
            "          65       0.67      0.79      0.73      2000\n",
            "          66       0.91      0.93      0.92      2000\n",
            "          67       0.77      0.81      0.79      2000\n",
            "          68       0.86      0.90      0.88      2000\n",
            "          69       0.58      0.67      0.62      2000\n",
            "          70       0.78      0.56      0.65      2000\n",
            "          71       0.86      0.89      0.87      2000\n",
            "          72       0.57      0.52      0.54      2000\n",
            "          73       0.68      0.78      0.72      2000\n",
            "          74       0.84      0.61      0.71      2000\n",
            "          75       0.86      0.78      0.81      2000\n",
            "          76       0.83      0.78      0.81      2000\n",
            "          77       0.70      0.79      0.74      2000\n",
            "          78       0.73      0.74      0.73      2000\n",
            "          79       0.89      0.80      0.84      2000\n",
            "          80       0.68      0.63      0.65      2000\n",
            "          81       0.70      0.69      0.70      2000\n",
            "          82       0.82      0.81      0.81      2000\n",
            "          83       0.60      0.77      0.67      2000\n",
            "          84       0.77      0.64      0.70      2000\n",
            "          85       0.67      0.71      0.69      2000\n",
            "          86       0.74      0.84      0.79      2000\n",
            "          87       0.68      0.67      0.67      2000\n",
            "          88       0.77      0.78      0.78      2000\n",
            "          89       0.84      0.85      0.84      2000\n",
            "          90       0.87      0.89      0.88      2000\n",
            "          91       0.91      0.95      0.93      2000\n",
            "          92       0.74      0.80      0.76      2000\n",
            "          93       0.71      0.79      0.75      2000\n",
            "          94       0.86      0.80      0.83      2000\n",
            "          95       0.89      0.88      0.89      2000\n",
            "          96       0.74      0.71      0.72      2000\n",
            "          97       0.82      0.82      0.82      2000\n",
            "          98       0.90      0.91      0.90      2000\n",
            "          99       0.88      0.89      0.89      2000\n",
            "         100       0.90      0.86      0.88      2000\n",
            "         101       0.78      0.78      0.78      2000\n",
            "         102       0.89      0.92      0.90      2000\n",
            "         103       0.95      0.91      0.93      2000\n",
            "         104       0.91      0.90      0.90      2000\n",
            "         105       0.76      0.77      0.76      2000\n",
            "         106       0.91      0.88      0.89      2000\n",
            "         107       0.86      0.89      0.87      2000\n",
            "         108       0.85      0.85      0.85      2000\n",
            "         109       0.91      0.87      0.89      2000\n",
            "         110       0.89      0.81      0.85      2000\n",
            "         111       0.80      0.86      0.83      2000\n",
            "         112       0.77      0.85      0.80      2000\n",
            "         113       0.67      0.58      0.62      2000\n",
            "         114       0.63      0.76      0.69      2000\n",
            "         115       0.89      0.92      0.90      2000\n",
            "         116       0.77      0.76      0.77      2000\n",
            "         117       0.93      0.93      0.93      2000\n",
            "         118       0.78      0.69      0.73      2000\n",
            "         119       0.80      0.80      0.80      2000\n",
            "\n",
            "    accuracy                           0.79    240000\n",
            "   macro avg       0.79      0.79      0.79    240000\n",
            "weighted avg       0.79      0.79      0.79    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.21360416666666668\n",
            "processing batch: 3\n",
            "processing file: data_processed/3/airplane.npy - 0\n",
            "processing file: data_processed/3/apple.npy - 1\n",
            "processing file: data_processed/3/ant.npy - 2\n",
            "processing file: data_processed/3/axe.npy - 3\n",
            "processing file: data_processed/3/banana.npy - 4\n",
            "processing file: data_processed/3/baseball.npy - 5\n",
            "processing file: data_processed/3/bee.npy - 6\n",
            "processing file: data_processed/3/beach.npy - 7\n",
            "processing file: data_processed/3/bird.npy - 8\n",
            "processing file: data_processed/3/book.npy - 9\n",
            "processing file: data_processed/3/bucket.npy - 10\n",
            "processing file: data_processed/3/bus.npy - 11\n",
            "processing file: data_processed/3/butterfly.npy - 12\n",
            "processing file: data_processed/3/cactus.npy - 13\n",
            "processing file: data_processed/3/cake.npy - 14\n",
            "processing file: data_processed/3/cannon.npy - 15\n",
            "processing file: data_processed/3/car.npy - 16\n",
            "processing file: data_processed/3/carrot.npy - 17\n",
            "processing file: data_processed/3/castle.npy - 18\n",
            "processing file: data_processed/3/cat.npy - 19\n",
            "processing file: data_processed/3/chair.npy - 20\n",
            "processing file: data_processed/3/church.npy - 21\n",
            "processing file: data_processed/3/circle.npy - 22\n",
            "processing file: data_processed/3/clock.npy - 23\n",
            "processing file: data_processed/3/cloud.npy - 24\n",
            "processing file: data_processed/3/computer.npy - 25\n",
            "processing file: data_processed/3/cookie.npy - 26\n",
            "processing file: data_processed/3/cow.npy - 27\n",
            "processing file: data_processed/3/crab.npy - 28\n",
            "processing file: data_processed/3/crayon.npy - 29\n",
            "processing file: data_processed/3/cup.npy - 30\n",
            "processing file: data_processed/3/dolphin.npy - 31\n",
            "processing file: data_processed/3/donut.npy - 32\n",
            "processing file: data_processed/3/door.npy - 33\n",
            "processing file: data_processed/3/dragon.npy - 34\n",
            "processing file: data_processed/3/diamond.npy - 35\n",
            "processing file: data_processed/3/drums.npy - 36\n",
            "processing file: data_processed/3/duck.npy - 37\n",
            "processing file: data_processed/3/ear.npy - 38\n",
            "processing file: data_processed/3/elephant.npy - 39\n",
            "processing file: data_processed/3/envelope.npy - 40\n",
            "processing file: data_processed/3/eraser.npy - 41\n",
            "processing file: data_processed/3/eye.npy - 42\n",
            "processing file: data_processed/3/face.npy - 43\n",
            "processing file: data_processed/3/fan.npy - 44\n",
            "processing file: data_processed/3/finger.npy - 45\n",
            "processing file: data_processed/3/fish.npy - 46\n",
            "processing file: data_processed/3/flower.npy - 47\n",
            "processing file: data_processed/3/frog.npy - 48\n",
            "processing file: data_processed/3/garden.npy - 49\n",
            "processing file: data_processed/3/grapes.npy - 50\n",
            "processing file: data_processed/3/grass.npy - 51\n",
            "processing file: data_processed/3/guitar.npy - 52\n",
            "processing file: data_processed/3/hammer.npy - 53\n",
            "processing file: data_processed/3/hand.npy - 54\n",
            "processing file: data_processed/3/hat.npy - 55\n",
            "processing file: data_processed/3/helmet.npy - 56\n",
            "processing file: data_processed/3/horse.npy - 57\n",
            "processing file: data_processed/3/house.npy - 58\n",
            "processing file: data_processed/3/hospital.npy - 59\n",
            "processing file: data_processed/3/jail.npy - 60\n",
            "processing file: data_processed/3/kangaroo.npy - 61\n",
            "processing file: data_processed/3/knife.npy - 62\n",
            "processing file: data_processed/3/laptop.npy - 63\n",
            "processing file: data_processed/3/leg.npy - 64\n",
            "processing file: data_processed/3/lion.npy - 65\n",
            "processing file: data_processed/3/lollipop.npy - 66\n",
            "processing file: data_processed/3/map.npy - 67\n",
            "processing file: data_processed/3/microwave.npy - 68\n",
            "processing file: data_processed/3/monkey.npy - 69\n",
            "processing file: data_processed/3/moon.npy - 70\n",
            "processing file: data_processed/3/mountain.npy - 71\n",
            "processing file: data_processed/3/mouse.npy - 72\n",
            "processing file: data_processed/3/mug.npy - 73\n",
            "processing file: data_processed/3/nail.npy - 74\n",
            "processing file: data_processed/3/nose.npy - 75\n",
            "processing file: data_processed/3/onion.npy - 76\n",
            "processing file: data_processed/3/owl.npy - 77\n",
            "processing file: data_processed/3/panda.npy - 78\n",
            "processing file: data_processed/3/parachute.npy - 79\n",
            "processing file: data_processed/3/parrot.npy - 80\n",
            "processing file: data_processed/3/passport.npy - 81\n",
            "processing file: data_processed/3/peanut.npy - 82\n",
            "processing file: data_processed/3/pencil.npy - 83\n",
            "processing file: data_processed/3/piano.npy - 84\n",
            "processing file: data_processed/3/pig.npy - 85\n",
            "processing file: data_processed/3/pizza.npy - 86\n",
            "processing file: data_processed/3/potato.npy - 87\n",
            "processing file: data_processed/3/rabbit.npy - 88\n",
            "processing file: data_processed/3/radio.npy - 89\n",
            "processing file: data_processed/3/rain.npy - 90\n",
            "processing file: data_processed/3/rainbow.npy - 91\n",
            "processing file: data_processed/3/river.npy - 92\n",
            "processing file: data_processed/3/sandwich.npy - 93\n",
            "processing file: data_processed/3/saw.npy - 94\n",
            "processing file: data_processed/3/sun.npy - 95\n",
            "processing file: data_processed/3/shark.npy - 96\n",
            "processing file: data_processed/3/shoe.npy - 97\n",
            "processing file: data_processed/3/skull.npy - 98\n",
            "processing file: data_processed/3/snail.npy - 99\n",
            "processing file: data_processed/3/snowflake.npy - 100\n",
            "processing file: data_processed/3/spider.npy - 101\n",
            "processing file: data_processed/3/square.npy - 102\n",
            "processing file: data_processed/3/stairs.npy - 103\n",
            "processing file: data_processed/3/star.npy - 104\n",
            "processing file: data_processed/3/stove.npy - 105\n",
            "processing file: data_processed/3/strawberry.npy - 106\n",
            "processing file: data_processed/3/table.npy - 107\n",
            "processing file: data_processed/3/teapot.npy - 108\n",
            "processing file: data_processed/3/television.npy - 109\n",
            "processing file: data_processed/3/tent.npy - 110\n",
            "processing file: data_processed/3/toilet.npy - 111\n",
            "processing file: data_processed/3/toothbrush.npy - 112\n",
            "processing file: data_processed/3/toothpaste.npy - 113\n",
            "processing file: data_processed/3/train.npy - 114\n",
            "processing file: data_processed/3/triangle.npy - 115\n",
            "processing file: data_processed/3/truck.npy - 116\n",
            "processing file: data_processed/3/umbrella.npy - 117\n",
            "processing file: data_processed/3/whale.npy - 118\n",
            "processing file: data_processed/3/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 1.0270 - accuracy: 0.7338 - val_loss: 0.8281 - val_accuracy: 0.7822\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 1.0080 - accuracy: 0.7380 - val_loss: 0.8271 - val_accuracy: 0.7820\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9970 - accuracy: 0.7398 - val_loss: 0.8238 - val_accuracy: 0.7852\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9874 - accuracy: 0.7421 - val_loss: 0.8224 - val_accuracy: 0.7840\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9818 - accuracy: 0.7439 - val_loss: 0.8161 - val_accuracy: 0.7854\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9741 - accuracy: 0.7449 - val_loss: 0.8175 - val_accuracy: 0.7858\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9659 - accuracy: 0.7478 - val_loss: 0.8107 - val_accuracy: 0.7875\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9601 - accuracy: 0.7491 - val_loss: 0.8121 - val_accuracy: 0.7881\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9545 - accuracy: 0.7489 - val_loss: 0.8126 - val_accuracy: 0.7880\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9485 - accuracy: 0.7506 - val_loss: 0.8086 - val_accuracy: 0.7883\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7935 - accuracy: 0.7917\n",
            "Test accuarcy: 79.17%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80      2000\n",
            "           1       0.89      0.93      0.91      2000\n",
            "           2       0.70      0.82      0.75      2000\n",
            "           3       0.82      0.77      0.80      2000\n",
            "           4       0.89      0.79      0.83      2000\n",
            "           5       0.85      0.81      0.83      2000\n",
            "           6       0.82      0.77      0.80      2000\n",
            "           7       0.62      0.71      0.66      2000\n",
            "           8       0.58      0.37      0.45      2000\n",
            "           9       0.81      0.77      0.79      2000\n",
            "          10       0.86      0.86      0.86      2000\n",
            "          11       0.82      0.86      0.84      2000\n",
            "          12       0.91      0.93      0.92      2000\n",
            "          13       0.84      0.84      0.84      2000\n",
            "          14       0.84      0.88      0.86      2000\n",
            "          15       0.72      0.76      0.74      2000\n",
            "          16       0.84      0.83      0.84      2000\n",
            "          17       0.86      0.89      0.88      2000\n",
            "          18       0.80      0.86      0.83      2000\n",
            "          19       0.78      0.72      0.75      2000\n",
            "          20       0.87      0.92      0.89      2000\n",
            "          21       0.77      0.73      0.75      2000\n",
            "          22       0.77      0.85      0.81      2000\n",
            "          23       0.89      0.87      0.88      2000\n",
            "          24       0.81      0.91      0.85      2000\n",
            "          25       0.79      0.68      0.73      2000\n",
            "          26       0.70      0.85      0.77      2000\n",
            "          27       0.67      0.70      0.68      2000\n",
            "          28       0.78      0.71      0.74      2000\n",
            "          29       0.63      0.56      0.60      2000\n",
            "          30       0.68      0.45      0.55      2000\n",
            "          31       0.69      0.64      0.67      2000\n",
            "          32       0.87      0.93      0.90      2000\n",
            "          33       0.88      0.87      0.88      2000\n",
            "          34       0.48      0.51      0.50      2000\n",
            "          35       0.90      0.88      0.89      2000\n",
            "          36       0.69      0.77      0.73      2000\n",
            "          37       0.67      0.71      0.69      2000\n",
            "          38       0.85      0.82      0.84      2000\n",
            "          39       0.70      0.77      0.73      2000\n",
            "          40       0.98      0.95      0.96      2000\n",
            "          41       0.72      0.62      0.67      2000\n",
            "          42       0.91      0.87      0.89      2000\n",
            "          43       0.84      0.82      0.83      2000\n",
            "          44       0.76      0.67      0.71      2000\n",
            "          45       0.79      0.77      0.78      2000\n",
            "          46       0.87      0.84      0.85      2000\n",
            "          47       0.76      0.88      0.82      2000\n",
            "          48       0.55      0.48      0.51      2000\n",
            "          49       0.58      0.70      0.64      2000\n",
            "          50       0.69      0.85      0.76      2000\n",
            "          51       0.80      0.82      0.81      2000\n",
            "          52       0.88      0.92      0.90      2000\n",
            "          53       0.81      0.81      0.81      2000\n",
            "          54       0.88      0.87      0.88      2000\n",
            "          55       0.83      0.83      0.83      2000\n",
            "          56       0.81      0.74      0.77      2000\n",
            "          57       0.75      0.79      0.77      2000\n",
            "          58       0.89      0.91      0.90      2000\n",
            "          59       0.71      0.83      0.76      2000\n",
            "          60       0.83      0.91      0.87      2000\n",
            "          61       0.78      0.79      0.79      2000\n",
            "          62       0.84      0.77      0.80      2000\n",
            "          63       0.74      0.85      0.79      2000\n",
            "          64       0.79      0.86      0.82      2000\n",
            "          65       0.71      0.78      0.74      2000\n",
            "          66       0.93      0.93      0.93      2000\n",
            "          67       0.77      0.82      0.79      2000\n",
            "          68       0.87      0.89      0.88      2000\n",
            "          69       0.57      0.71      0.64      2000\n",
            "          70       0.76      0.59      0.66      2000\n",
            "          71       0.87      0.88      0.88      2000\n",
            "          72       0.56      0.52      0.54      2000\n",
            "          73       0.69      0.80      0.74      2000\n",
            "          74       0.82      0.63      0.71      2000\n",
            "          75       0.83      0.77      0.80      2000\n",
            "          76       0.81      0.76      0.78      2000\n",
            "          77       0.79      0.72      0.75      2000\n",
            "          78       0.76      0.75      0.75      2000\n",
            "          79       0.89      0.82      0.85      2000\n",
            "          80       0.70      0.64      0.67      2000\n",
            "          81       0.72      0.68      0.70      2000\n",
            "          82       0.81      0.81      0.81      2000\n",
            "          83       0.68      0.70      0.69      2000\n",
            "          84       0.74      0.67      0.70      2000\n",
            "          85       0.66      0.73      0.69      2000\n",
            "          86       0.80      0.81      0.81      2000\n",
            "          87       0.69      0.65      0.67      2000\n",
            "          88       0.80      0.78      0.79      2000\n",
            "          89       0.86      0.85      0.86      2000\n",
            "          90       0.82      0.92      0.87      2000\n",
            "          91       0.92      0.95      0.93      2000\n",
            "          92       0.78      0.79      0.79      2000\n",
            "          93       0.74      0.77      0.76      2000\n",
            "          94       0.87      0.81      0.83      2000\n",
            "          95       0.85      0.91      0.88      2000\n",
            "          96       0.73      0.73      0.73      2000\n",
            "          97       0.81      0.83      0.82      2000\n",
            "          98       0.90      0.91      0.90      2000\n",
            "          99       0.88      0.90      0.89      2000\n",
            "         100       0.84      0.88      0.86      2000\n",
            "         101       0.82      0.75      0.78      2000\n",
            "         102       0.89      0.92      0.90      2000\n",
            "         103       0.93      0.91      0.92      2000\n",
            "         104       0.92      0.90      0.91      2000\n",
            "         105       0.79      0.76      0.77      2000\n",
            "         106       0.89      0.89      0.89      2000\n",
            "         107       0.86      0.90      0.88      2000\n",
            "         108       0.87      0.85      0.86      2000\n",
            "         109       0.91      0.85      0.88      2000\n",
            "         110       0.91      0.81      0.86      2000\n",
            "         111       0.84      0.84      0.84      2000\n",
            "         112       0.78      0.85      0.81      2000\n",
            "         113       0.66      0.61      0.63      2000\n",
            "         114       0.68      0.75      0.71      2000\n",
            "         115       0.89      0.92      0.90      2000\n",
            "         116       0.77      0.77      0.77      2000\n",
            "         117       0.93      0.93      0.93      2000\n",
            "         118       0.76      0.72      0.74      2000\n",
            "         119       0.85      0.77      0.81      2000\n",
            "\n",
            "    accuracy                           0.79    240000\n",
            "   macro avg       0.79      0.79      0.79    240000\n",
            "weighted avg       0.79      0.79      0.79    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.2082875\n",
            "processing batch: 4\n",
            "processing file: data_processed/4/airplane.npy - 0\n",
            "processing file: data_processed/4/apple.npy - 1\n",
            "processing file: data_processed/4/ant.npy - 2\n",
            "processing file: data_processed/4/axe.npy - 3\n",
            "processing file: data_processed/4/banana.npy - 4\n",
            "processing file: data_processed/4/baseball.npy - 5\n",
            "processing file: data_processed/4/bee.npy - 6\n",
            "processing file: data_processed/4/beach.npy - 7\n",
            "processing file: data_processed/4/bird.npy - 8\n",
            "processing file: data_processed/4/book.npy - 9\n",
            "processing file: data_processed/4/bucket.npy - 10\n",
            "processing file: data_processed/4/bus.npy - 11\n",
            "processing file: data_processed/4/butterfly.npy - 12\n",
            "processing file: data_processed/4/cactus.npy - 13\n",
            "processing file: data_processed/4/cake.npy - 14\n",
            "processing file: data_processed/4/cannon.npy - 15\n",
            "processing file: data_processed/4/car.npy - 16\n",
            "processing file: data_processed/4/carrot.npy - 17\n",
            "processing file: data_processed/4/castle.npy - 18\n",
            "processing file: data_processed/4/cat.npy - 19\n",
            "processing file: data_processed/4/chair.npy - 20\n",
            "processing file: data_processed/4/church.npy - 21\n",
            "processing file: data_processed/4/circle.npy - 22\n",
            "processing file: data_processed/4/clock.npy - 23\n",
            "processing file: data_processed/4/cloud.npy - 24\n",
            "processing file: data_processed/4/computer.npy - 25\n",
            "processing file: data_processed/4/cookie.npy - 26\n",
            "processing file: data_processed/4/cow.npy - 27\n",
            "processing file: data_processed/4/crab.npy - 28\n",
            "processing file: data_processed/4/crayon.npy - 29\n",
            "processing file: data_processed/4/cup.npy - 30\n",
            "processing file: data_processed/4/dolphin.npy - 31\n",
            "processing file: data_processed/4/donut.npy - 32\n",
            "processing file: data_processed/4/door.npy - 33\n",
            "processing file: data_processed/4/dragon.npy - 34\n",
            "processing file: data_processed/4/diamond.npy - 35\n",
            "processing file: data_processed/4/drums.npy - 36\n",
            "processing file: data_processed/4/duck.npy - 37\n",
            "processing file: data_processed/4/ear.npy - 38\n",
            "processing file: data_processed/4/elephant.npy - 39\n",
            "processing file: data_processed/4/envelope.npy - 40\n",
            "processing file: data_processed/4/eraser.npy - 41\n",
            "processing file: data_processed/4/eye.npy - 42\n",
            "processing file: data_processed/4/face.npy - 43\n",
            "processing file: data_processed/4/fan.npy - 44\n",
            "processing file: data_processed/4/finger.npy - 45\n",
            "processing file: data_processed/4/fish.npy - 46\n",
            "processing file: data_processed/4/flower.npy - 47\n",
            "processing file: data_processed/4/frog.npy - 48\n",
            "processing file: data_processed/4/garden.npy - 49\n",
            "processing file: data_processed/4/grapes.npy - 50\n",
            "processing file: data_processed/4/grass.npy - 51\n",
            "processing file: data_processed/4/guitar.npy - 52\n",
            "processing file: data_processed/4/hammer.npy - 53\n",
            "processing file: data_processed/4/hand.npy - 54\n",
            "processing file: data_processed/4/hat.npy - 55\n",
            "processing file: data_processed/4/helmet.npy - 56\n",
            "processing file: data_processed/4/horse.npy - 57\n",
            "processing file: data_processed/4/house.npy - 58\n",
            "processing file: data_processed/4/hospital.npy - 59\n",
            "processing file: data_processed/4/jail.npy - 60\n",
            "processing file: data_processed/4/kangaroo.npy - 61\n",
            "processing file: data_processed/4/knife.npy - 62\n",
            "processing file: data_processed/4/laptop.npy - 63\n",
            "processing file: data_processed/4/leg.npy - 64\n",
            "processing file: data_processed/4/lion.npy - 65\n",
            "processing file: data_processed/4/lollipop.npy - 66\n",
            "processing file: data_processed/4/map.npy - 67\n",
            "processing file: data_processed/4/microwave.npy - 68\n",
            "processing file: data_processed/4/monkey.npy - 69\n",
            "processing file: data_processed/4/moon.npy - 70\n",
            "processing file: data_processed/4/mountain.npy - 71\n",
            "processing file: data_processed/4/mouse.npy - 72\n",
            "processing file: data_processed/4/mug.npy - 73\n",
            "processing file: data_processed/4/nail.npy - 74\n",
            "processing file: data_processed/4/nose.npy - 75\n",
            "processing file: data_processed/4/onion.npy - 76\n",
            "processing file: data_processed/4/owl.npy - 77\n",
            "processing file: data_processed/4/panda.npy - 78\n",
            "processing file: data_processed/4/parachute.npy - 79\n",
            "processing file: data_processed/4/parrot.npy - 80\n",
            "processing file: data_processed/4/passport.npy - 81\n",
            "processing file: data_processed/4/peanut.npy - 82\n",
            "processing file: data_processed/4/pencil.npy - 83\n",
            "processing file: data_processed/4/piano.npy - 84\n",
            "processing file: data_processed/4/pig.npy - 85\n",
            "processing file: data_processed/4/pizza.npy - 86\n",
            "processing file: data_processed/4/potato.npy - 87\n",
            "processing file: data_processed/4/rabbit.npy - 88\n",
            "processing file: data_processed/4/radio.npy - 89\n",
            "processing file: data_processed/4/rain.npy - 90\n",
            "processing file: data_processed/4/rainbow.npy - 91\n",
            "processing file: data_processed/4/river.npy - 92\n",
            "processing file: data_processed/4/sandwich.npy - 93\n",
            "processing file: data_processed/4/saw.npy - 94\n",
            "processing file: data_processed/4/sun.npy - 95\n",
            "processing file: data_processed/4/shark.npy - 96\n",
            "processing file: data_processed/4/shoe.npy - 97\n",
            "processing file: data_processed/4/skull.npy - 98\n",
            "processing file: data_processed/4/snail.npy - 99\n",
            "processing file: data_processed/4/snowflake.npy - 100\n",
            "processing file: data_processed/4/spider.npy - 101\n",
            "processing file: data_processed/4/square.npy - 102\n",
            "processing file: data_processed/4/stairs.npy - 103\n",
            "processing file: data_processed/4/star.npy - 104\n",
            "processing file: data_processed/4/stove.npy - 105\n",
            "processing file: data_processed/4/strawberry.npy - 106\n",
            "processing file: data_processed/4/table.npy - 107\n",
            "processing file: data_processed/4/teapot.npy - 108\n",
            "processing file: data_processed/4/television.npy - 109\n",
            "processing file: data_processed/4/tent.npy - 110\n",
            "processing file: data_processed/4/toilet.npy - 111\n",
            "processing file: data_processed/4/toothbrush.npy - 112\n",
            "processing file: data_processed/4/toothpaste.npy - 113\n",
            "processing file: data_processed/4/train.npy - 114\n",
            "processing file: data_processed/4/triangle.npy - 115\n",
            "processing file: data_processed/4/truck.npy - 116\n",
            "processing file: data_processed/4/umbrella.npy - 117\n",
            "processing file: data_processed/4/whale.npy - 118\n",
            "processing file: data_processed/4/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9893 - accuracy: 0.7425 - val_loss: 0.7953 - val_accuracy: 0.7915\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9750 - accuracy: 0.7455 - val_loss: 0.7999 - val_accuracy: 0.7927\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9623 - accuracy: 0.7484 - val_loss: 0.7976 - val_accuracy: 0.7925\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9528 - accuracy: 0.7513 - val_loss: 0.7962 - val_accuracy: 0.7924\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9470 - accuracy: 0.7518 - val_loss: 0.7921 - val_accuracy: 0.7938\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9413 - accuracy: 0.7530 - val_loss: 0.7899 - val_accuracy: 0.7933\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9336 - accuracy: 0.7552 - val_loss: 0.7933 - val_accuracy: 0.7932\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9274 - accuracy: 0.7557 - val_loss: 0.7872 - val_accuracy: 0.7951\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9194 - accuracy: 0.7583 - val_loss: 0.7853 - val_accuracy: 0.7954\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9143 - accuracy: 0.7595 - val_loss: 0.7927 - val_accuracy: 0.7926\n",
            "7500/7500 [==============================] - 24s 3ms/step - loss: 0.7796 - accuracy: 0.7951\n",
            "Test accuarcy: 79.51%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.83      0.81      2000\n",
            "           1       0.91      0.92      0.92      2000\n",
            "           2       0.80      0.78      0.79      2000\n",
            "           3       0.81      0.80      0.80      2000\n",
            "           4       0.84      0.84      0.84      2000\n",
            "           5       0.89      0.79      0.84      2000\n",
            "           6       0.74      0.83      0.78      2000\n",
            "           7       0.71      0.65      0.68      2000\n",
            "           8       0.57      0.40      0.47      2000\n",
            "           9       0.75      0.84      0.79      2000\n",
            "          10       0.82      0.89      0.85      2000\n",
            "          11       0.78      0.88      0.83      2000\n",
            "          12       0.92      0.92      0.92      2000\n",
            "          13       0.88      0.82      0.85      2000\n",
            "          14       0.84      0.89      0.86      2000\n",
            "          15       0.72      0.75      0.73      2000\n",
            "          16       0.83      0.83      0.83      2000\n",
            "          17       0.88      0.89      0.88      2000\n",
            "          18       0.83      0.86      0.84      2000\n",
            "          19       0.82      0.70      0.75      2000\n",
            "          20       0.89      0.91      0.90      2000\n",
            "          21       0.80      0.72      0.76      2000\n",
            "          22       0.74      0.89      0.81      2000\n",
            "          23       0.88      0.89      0.89      2000\n",
            "          24       0.90      0.88      0.89      2000\n",
            "          25       0.80      0.67      0.73      2000\n",
            "          26       0.73      0.83      0.78      2000\n",
            "          27       0.62      0.75      0.68      2000\n",
            "          28       0.75      0.74      0.75      2000\n",
            "          29       0.63      0.52      0.57      2000\n",
            "          30       0.68      0.48      0.56      2000\n",
            "          31       0.73      0.62      0.67      2000\n",
            "          32       0.87      0.92      0.90      2000\n",
            "          33       0.92      0.86      0.89      2000\n",
            "          34       0.49      0.52      0.50      2000\n",
            "          35       0.90      0.88      0.89      2000\n",
            "          36       0.64      0.80      0.71      2000\n",
            "          37       0.67      0.71      0.69      2000\n",
            "          38       0.91      0.78      0.84      2000\n",
            "          39       0.73      0.75      0.74      2000\n",
            "          40       0.98      0.95      0.97      2000\n",
            "          41       0.69      0.63      0.66      2000\n",
            "          42       0.91      0.88      0.89      2000\n",
            "          43       0.86      0.81      0.84      2000\n",
            "          44       0.74      0.69      0.71      2000\n",
            "          45       0.84      0.72      0.78      2000\n",
            "          46       0.88      0.84      0.86      2000\n",
            "          47       0.79      0.88      0.83      2000\n",
            "          48       0.58      0.49      0.53      2000\n",
            "          49       0.60      0.69      0.64      2000\n",
            "          50       0.64      0.87      0.74      2000\n",
            "          51       0.77      0.85      0.81      2000\n",
            "          52       0.85      0.93      0.89      2000\n",
            "          53       0.81      0.82      0.82      2000\n",
            "          54       0.89      0.87      0.88      2000\n",
            "          55       0.85      0.81      0.83      2000\n",
            "          56       0.79      0.76      0.77      2000\n",
            "          57       0.78      0.77      0.77      2000\n",
            "          58       0.89      0.91      0.90      2000\n",
            "          59       0.78      0.80      0.79      2000\n",
            "          60       0.85      0.90      0.87      2000\n",
            "          61       0.76      0.82      0.79      2000\n",
            "          62       0.81      0.77      0.79      2000\n",
            "          63       0.72      0.87      0.79      2000\n",
            "          64       0.80      0.85      0.83      2000\n",
            "          65       0.71      0.78      0.75      2000\n",
            "          66       0.92      0.93      0.93      2000\n",
            "          67       0.77      0.81      0.79      2000\n",
            "          68       0.86      0.90      0.88      2000\n",
            "          69       0.56      0.68      0.61      2000\n",
            "          70       0.77      0.58      0.66      2000\n",
            "          71       0.89      0.88      0.89      2000\n",
            "          72       0.67      0.49      0.56      2000\n",
            "          73       0.69      0.79      0.74      2000\n",
            "          74       0.83      0.63      0.72      2000\n",
            "          75       0.89      0.74      0.81      2000\n",
            "          76       0.77      0.83      0.80      2000\n",
            "          77       0.75      0.79      0.77      2000\n",
            "          78       0.68      0.79      0.73      2000\n",
            "          79       0.88      0.84      0.86      2000\n",
            "          80       0.72      0.61      0.66      2000\n",
            "          81       0.76      0.64      0.69      2000\n",
            "          82       0.83      0.81      0.82      2000\n",
            "          83       0.62      0.77      0.69      2000\n",
            "          84       0.75      0.68      0.71      2000\n",
            "          85       0.68      0.72      0.70      2000\n",
            "          86       0.75      0.87      0.81      2000\n",
            "          87       0.73      0.58      0.65      2000\n",
            "          88       0.78      0.80      0.79      2000\n",
            "          89       0.88      0.83      0.85      2000\n",
            "          90       0.86      0.90      0.88      2000\n",
            "          91       0.93      0.95      0.94      2000\n",
            "          92       0.81      0.79      0.80      2000\n",
            "          93       0.74      0.79      0.76      2000\n",
            "          94       0.91      0.80      0.85      2000\n",
            "          95       0.91      0.89      0.90      2000\n",
            "          96       0.74      0.73      0.74      2000\n",
            "          97       0.84      0.82      0.83      2000\n",
            "          98       0.91      0.90      0.91      2000\n",
            "          99       0.85      0.92      0.88      2000\n",
            "         100       0.85      0.87      0.86      2000\n",
            "         101       0.83      0.77      0.80      2000\n",
            "         102       0.91      0.91      0.91      2000\n",
            "         103       0.97      0.91      0.94      2000\n",
            "         104       0.90      0.91      0.91      2000\n",
            "         105       0.76      0.79      0.77      2000\n",
            "         106       0.91      0.88      0.90      2000\n",
            "         107       0.89      0.89      0.89      2000\n",
            "         108       0.83      0.87      0.85      2000\n",
            "         109       0.88      0.89      0.88      2000\n",
            "         110       0.90      0.82      0.86      2000\n",
            "         111       0.84      0.85      0.85      2000\n",
            "         112       0.76      0.86      0.81      2000\n",
            "         113       0.72      0.58      0.64      2000\n",
            "         114       0.66      0.79      0.72      2000\n",
            "         115       0.89      0.93      0.91      2000\n",
            "         116       0.79      0.74      0.77      2000\n",
            "         117       0.95      0.92      0.93      2000\n",
            "         118       0.74      0.75      0.75      2000\n",
            "         119       0.81      0.82      0.81      2000\n",
            "\n",
            "    accuracy                           0.80    240000\n",
            "   macro avg       0.80      0.80      0.79    240000\n",
            "weighted avg       0.80      0.80      0.79    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.2048625\n",
            "processing batch: 5\n",
            "processing file: data_processed/5/airplane.npy - 0\n",
            "processing file: data_processed/5/apple.npy - 1\n",
            "processing file: data_processed/5/ant.npy - 2\n",
            "processing file: data_processed/5/axe.npy - 3\n",
            "processing file: data_processed/5/banana.npy - 4\n",
            "processing file: data_processed/5/baseball.npy - 5\n",
            "processing file: data_processed/5/bee.npy - 6\n",
            "processing file: data_processed/5/beach.npy - 7\n",
            "processing file: data_processed/5/bird.npy - 8\n",
            "processing file: data_processed/5/book.npy - 9\n",
            "processing file: data_processed/5/bucket.npy - 10\n",
            "processing file: data_processed/5/bus.npy - 11\n",
            "processing file: data_processed/5/butterfly.npy - 12\n",
            "processing file: data_processed/5/cactus.npy - 13\n",
            "processing file: data_processed/5/cake.npy - 14\n",
            "processing file: data_processed/5/cannon.npy - 15\n",
            "processing file: data_processed/5/car.npy - 16\n",
            "processing file: data_processed/5/carrot.npy - 17\n",
            "processing file: data_processed/5/castle.npy - 18\n",
            "processing file: data_processed/5/cat.npy - 19\n",
            "processing file: data_processed/5/chair.npy - 20\n",
            "processing file: data_processed/5/church.npy - 21\n",
            "processing file: data_processed/5/circle.npy - 22\n",
            "processing file: data_processed/5/clock.npy - 23\n",
            "processing file: data_processed/5/cloud.npy - 24\n",
            "processing file: data_processed/5/computer.npy - 25\n",
            "processing file: data_processed/5/cookie.npy - 26\n",
            "processing file: data_processed/5/cow.npy - 27\n",
            "processing file: data_processed/5/crab.npy - 28\n",
            "processing file: data_processed/5/crayon.npy - 29\n",
            "processing file: data_processed/5/cup.npy - 30\n",
            "processing file: data_processed/5/dolphin.npy - 31\n",
            "processing file: data_processed/5/donut.npy - 32\n",
            "processing file: data_processed/5/door.npy - 33\n",
            "processing file: data_processed/5/dragon.npy - 34\n",
            "processing file: data_processed/5/diamond.npy - 35\n",
            "processing file: data_processed/5/drums.npy - 36\n",
            "processing file: data_processed/5/duck.npy - 37\n",
            "processing file: data_processed/5/ear.npy - 38\n",
            "processing file: data_processed/5/elephant.npy - 39\n",
            "processing file: data_processed/5/envelope.npy - 40\n",
            "processing file: data_processed/5/eraser.npy - 41\n",
            "processing file: data_processed/5/eye.npy - 42\n",
            "processing file: data_processed/5/face.npy - 43\n",
            "processing file: data_processed/5/fan.npy - 44\n",
            "processing file: data_processed/5/finger.npy - 45\n",
            "processing file: data_processed/5/fish.npy - 46\n",
            "processing file: data_processed/5/flower.npy - 47\n",
            "processing file: data_processed/5/frog.npy - 48\n",
            "processing file: data_processed/5/garden.npy - 49\n",
            "processing file: data_processed/5/grapes.npy - 50\n",
            "processing file: data_processed/5/grass.npy - 51\n",
            "processing file: data_processed/5/guitar.npy - 52\n",
            "processing file: data_processed/5/hammer.npy - 53\n",
            "processing file: data_processed/5/hand.npy - 54\n",
            "processing file: data_processed/5/hat.npy - 55\n",
            "processing file: data_processed/5/helmet.npy - 56\n",
            "processing file: data_processed/5/horse.npy - 57\n",
            "processing file: data_processed/5/house.npy - 58\n",
            "processing file: data_processed/5/hospital.npy - 59\n",
            "processing file: data_processed/5/jail.npy - 60\n",
            "processing file: data_processed/5/kangaroo.npy - 61\n",
            "processing file: data_processed/5/knife.npy - 62\n",
            "processing file: data_processed/5/laptop.npy - 63\n",
            "processing file: data_processed/5/leg.npy - 64\n",
            "processing file: data_processed/5/lion.npy - 65\n",
            "processing file: data_processed/5/lollipop.npy - 66\n",
            "processing file: data_processed/5/map.npy - 67\n",
            "processing file: data_processed/5/microwave.npy - 68\n",
            "processing file: data_processed/5/monkey.npy - 69\n",
            "processing file: data_processed/5/moon.npy - 70\n",
            "processing file: data_processed/5/mountain.npy - 71\n",
            "processing file: data_processed/5/mouse.npy - 72\n",
            "processing file: data_processed/5/mug.npy - 73\n",
            "processing file: data_processed/5/nail.npy - 74\n",
            "processing file: data_processed/5/nose.npy - 75\n",
            "processing file: data_processed/5/onion.npy - 76\n",
            "processing file: data_processed/5/owl.npy - 77\n",
            "processing file: data_processed/5/panda.npy - 78\n",
            "processing file: data_processed/5/parachute.npy - 79\n",
            "processing file: data_processed/5/parrot.npy - 80\n",
            "processing file: data_processed/5/passport.npy - 81\n",
            "processing file: data_processed/5/peanut.npy - 82\n",
            "processing file: data_processed/5/pencil.npy - 83\n",
            "processing file: data_processed/5/piano.npy - 84\n",
            "processing file: data_processed/5/pig.npy - 85\n",
            "processing file: data_processed/5/pizza.npy - 86\n",
            "processing file: data_processed/5/potato.npy - 87\n",
            "processing file: data_processed/5/rabbit.npy - 88\n",
            "processing file: data_processed/5/radio.npy - 89\n",
            "processing file: data_processed/5/rain.npy - 90\n",
            "processing file: data_processed/5/rainbow.npy - 91\n",
            "processing file: data_processed/5/river.npy - 92\n",
            "processing file: data_processed/5/sandwich.npy - 93\n",
            "processing file: data_processed/5/saw.npy - 94\n",
            "processing file: data_processed/5/sun.npy - 95\n",
            "processing file: data_processed/5/shark.npy - 96\n",
            "processing file: data_processed/5/shoe.npy - 97\n",
            "processing file: data_processed/5/skull.npy - 98\n",
            "processing file: data_processed/5/snail.npy - 99\n",
            "processing file: data_processed/5/snowflake.npy - 100\n",
            "processing file: data_processed/5/spider.npy - 101\n",
            "processing file: data_processed/5/square.npy - 102\n",
            "processing file: data_processed/5/stairs.npy - 103\n",
            "processing file: data_processed/5/star.npy - 104\n",
            "processing file: data_processed/5/stove.npy - 105\n",
            "processing file: data_processed/5/strawberry.npy - 106\n",
            "processing file: data_processed/5/table.npy - 107\n",
            "processing file: data_processed/5/teapot.npy - 108\n",
            "processing file: data_processed/5/television.npy - 109\n",
            "processing file: data_processed/5/tent.npy - 110\n",
            "processing file: data_processed/5/toilet.npy - 111\n",
            "processing file: data_processed/5/toothbrush.npy - 112\n",
            "processing file: data_processed/5/toothpaste.npy - 113\n",
            "processing file: data_processed/5/train.npy - 114\n",
            "processing file: data_processed/5/triangle.npy - 115\n",
            "processing file: data_processed/5/truck.npy - 116\n",
            "processing file: data_processed/5/umbrella.npy - 117\n",
            "processing file: data_processed/5/whale.npy - 118\n",
            "processing file: data_processed/5/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9684 - accuracy: 0.7485 - val_loss: 0.7613 - val_accuracy: 0.8009\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9526 - accuracy: 0.7516 - val_loss: 0.7640 - val_accuracy: 0.8018\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9412 - accuracy: 0.7552 - val_loss: 0.7743 - val_accuracy: 0.7968\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9344 - accuracy: 0.7560 - val_loss: 0.7567 - val_accuracy: 0.8028\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9274 - accuracy: 0.7582 - val_loss: 0.7556 - val_accuracy: 0.8034\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9206 - accuracy: 0.7592 - val_loss: 0.7554 - val_accuracy: 0.8035\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9123 - accuracy: 0.7613 - val_loss: 0.7579 - val_accuracy: 0.8037\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9083 - accuracy: 0.7616 - val_loss: 0.7561 - val_accuracy: 0.8035\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9036 - accuracy: 0.7639 - val_loss: 0.7535 - val_accuracy: 0.8056\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8956 - accuracy: 0.7647 - val_loss: 0.7649 - val_accuracy: 0.8006\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7722 - accuracy: 0.7969\n",
            "Test accuarcy: 79.69%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.82      2000\n",
            "           1       0.91      0.92      0.92      2000\n",
            "           2       0.59      0.86      0.70      2000\n",
            "           3       0.87      0.77      0.82      2000\n",
            "           4       0.89      0.80      0.84      2000\n",
            "           5       0.87      0.82      0.85      2000\n",
            "           6       0.84      0.79      0.81      2000\n",
            "           7       0.56      0.77      0.65      2000\n",
            "           8       0.57      0.43      0.49      2000\n",
            "           9       0.80      0.80      0.80      2000\n",
            "          10       0.90      0.84      0.87      2000\n",
            "          11       0.80      0.88      0.84      2000\n",
            "          12       0.90      0.93      0.91      2000\n",
            "          13       0.81      0.86      0.84      2000\n",
            "          14       0.80      0.90      0.85      2000\n",
            "          15       0.78      0.72      0.75      2000\n",
            "          16       0.85      0.81      0.83      2000\n",
            "          17       0.91      0.88      0.89      2000\n",
            "          18       0.83      0.86      0.85      2000\n",
            "          19       0.83      0.70      0.76      2000\n",
            "          20       0.91      0.91      0.91      2000\n",
            "          21       0.80      0.71      0.75      2000\n",
            "          22       0.77      0.87      0.81      2000\n",
            "          23       0.90      0.87      0.89      2000\n",
            "          24       0.75      0.92      0.83      2000\n",
            "          25       0.73      0.72      0.73      2000\n",
            "          26       0.70      0.86      0.77      2000\n",
            "          27       0.69      0.72      0.70      2000\n",
            "          28       0.76      0.72      0.74      2000\n",
            "          29       0.64      0.57      0.61      2000\n",
            "          30       0.69      0.47      0.56      2000\n",
            "          31       0.73      0.61      0.66      2000\n",
            "          32       0.86      0.94      0.89      2000\n",
            "          33       0.88      0.88      0.88      2000\n",
            "          34       0.49      0.50      0.50      2000\n",
            "          35       0.89      0.88      0.89      2000\n",
            "          36       0.74      0.77      0.76      2000\n",
            "          37       0.71      0.67      0.69      2000\n",
            "          38       0.81      0.85      0.83      2000\n",
            "          39       0.78      0.74      0.76      2000\n",
            "          40       0.98      0.95      0.96      2000\n",
            "          41       0.67      0.64      0.66      2000\n",
            "          42       0.89      0.88      0.89      2000\n",
            "          43       0.82      0.84      0.83      2000\n",
            "          44       0.81      0.68      0.74      2000\n",
            "          45       0.77      0.79      0.78      2000\n",
            "          46       0.86      0.86      0.86      2000\n",
            "          47       0.79      0.89      0.84      2000\n",
            "          48       0.57      0.51      0.54      2000\n",
            "          49       0.59      0.67      0.63      2000\n",
            "          50       0.70      0.86      0.77      2000\n",
            "          51       0.79      0.85      0.82      2000\n",
            "          52       0.88      0.92      0.90      2000\n",
            "          53       0.82      0.83      0.83      2000\n",
            "          54       0.86      0.88      0.87      2000\n",
            "          55       0.88      0.81      0.84      2000\n",
            "          56       0.81      0.75      0.78      2000\n",
            "          57       0.78      0.78      0.78      2000\n",
            "          58       0.89      0.92      0.90      2000\n",
            "          59       0.72      0.83      0.77      2000\n",
            "          60       0.81      0.92      0.86      2000\n",
            "          61       0.78      0.81      0.80      2000\n",
            "          62       0.89      0.75      0.82      2000\n",
            "          63       0.76      0.81      0.79      2000\n",
            "          64       0.78      0.87      0.83      2000\n",
            "          65       0.67      0.81      0.73      2000\n",
            "          66       0.92      0.94      0.93      2000\n",
            "          67       0.77      0.82      0.79      2000\n",
            "          68       0.87      0.90      0.89      2000\n",
            "          69       0.60      0.71      0.65      2000\n",
            "          70       0.74      0.59      0.66      2000\n",
            "          71       0.88      0.88      0.88      2000\n",
            "          72       0.58      0.57      0.57      2000\n",
            "          73       0.69      0.79      0.74      2000\n",
            "          74       0.87      0.61      0.71      2000\n",
            "          75       0.80      0.79      0.80      2000\n",
            "          76       0.81      0.80      0.80      2000\n",
            "          77       0.81      0.74      0.77      2000\n",
            "          78       0.80      0.73      0.76      2000\n",
            "          79       0.87      0.84      0.85      2000\n",
            "          80       0.74      0.62      0.67      2000\n",
            "          81       0.72      0.69      0.70      2000\n",
            "          82       0.86      0.79      0.82      2000\n",
            "          83       0.69      0.70      0.69      2000\n",
            "          84       0.77      0.67      0.72      2000\n",
            "          85       0.71      0.70      0.71      2000\n",
            "          86       0.85      0.81      0.83      2000\n",
            "          87       0.68      0.66      0.67      2000\n",
            "          88       0.80      0.79      0.79      2000\n",
            "          89       0.88      0.84      0.86      2000\n",
            "          90       0.77      0.94      0.84      2000\n",
            "          91       0.93      0.95      0.94      2000\n",
            "          92       0.72      0.84      0.77      2000\n",
            "          93       0.79      0.77      0.78      2000\n",
            "          94       0.84      0.83      0.83      2000\n",
            "          95       0.85      0.92      0.88      2000\n",
            "          96       0.74      0.75      0.75      2000\n",
            "          97       0.84      0.81      0.82      2000\n",
            "          98       0.89      0.91      0.90      2000\n",
            "          99       0.88      0.92      0.90      2000\n",
            "         100       0.84      0.88      0.86      2000\n",
            "         101       0.74      0.81      0.77      2000\n",
            "         102       0.90      0.92      0.91      2000\n",
            "         103       0.95      0.91      0.93      2000\n",
            "         104       0.94      0.89      0.91      2000\n",
            "         105       0.82      0.75      0.78      2000\n",
            "         106       0.87      0.91      0.89      2000\n",
            "         107       0.87      0.88      0.88      2000\n",
            "         108       0.87      0.85      0.86      2000\n",
            "         109       0.91      0.86      0.89      2000\n",
            "         110       0.91      0.82      0.86      2000\n",
            "         111       0.88      0.84      0.86      2000\n",
            "         112       0.83      0.82      0.82      2000\n",
            "         113       0.71      0.58      0.64      2000\n",
            "         114       0.69      0.77      0.73      2000\n",
            "         115       0.90      0.91      0.90      2000\n",
            "         116       0.79      0.76      0.77      2000\n",
            "         117       0.93      0.93      0.93      2000\n",
            "         118       0.78      0.72      0.75      2000\n",
            "         119       0.85      0.79      0.82      2000\n",
            "\n",
            "    accuracy                           0.80    240000\n",
            "   macro avg       0.80      0.80      0.80    240000\n",
            "weighted avg       0.80      0.80      0.80    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.20314583333333333\n",
            "processing batch: 6\n",
            "processing file: data_processed/6/airplane.npy - 0\n",
            "processing file: data_processed/6/apple.npy - 1\n",
            "processing file: data_processed/6/ant.npy - 2\n",
            "processing file: data_processed/6/axe.npy - 3\n",
            "processing file: data_processed/6/banana.npy - 4\n",
            "processing file: data_processed/6/baseball.npy - 5\n",
            "processing file: data_processed/6/bee.npy - 6\n",
            "processing file: data_processed/6/beach.npy - 7\n",
            "processing file: data_processed/6/bird.npy - 8\n",
            "processing file: data_processed/6/book.npy - 9\n",
            "processing file: data_processed/6/bucket.npy - 10\n",
            "processing file: data_processed/6/bus.npy - 11\n",
            "processing file: data_processed/6/butterfly.npy - 12\n",
            "processing file: data_processed/6/cactus.npy - 13\n",
            "processing file: data_processed/6/cake.npy - 14\n",
            "processing file: data_processed/6/cannon.npy - 15\n",
            "processing file: data_processed/6/car.npy - 16\n",
            "processing file: data_processed/6/carrot.npy - 17\n",
            "processing file: data_processed/6/castle.npy - 18\n",
            "processing file: data_processed/6/cat.npy - 19\n",
            "processing file: data_processed/6/chair.npy - 20\n",
            "processing file: data_processed/6/church.npy - 21\n",
            "processing file: data_processed/6/circle.npy - 22\n",
            "processing file: data_processed/6/clock.npy - 23\n",
            "processing file: data_processed/6/cloud.npy - 24\n",
            "processing file: data_processed/6/computer.npy - 25\n",
            "processing file: data_processed/6/cookie.npy - 26\n",
            "processing file: data_processed/6/cow.npy - 27\n",
            "processing file: data_processed/6/crab.npy - 28\n",
            "processing file: data_processed/6/crayon.npy - 29\n",
            "processing file: data_processed/6/cup.npy - 30\n",
            "processing file: data_processed/6/dolphin.npy - 31\n",
            "processing file: data_processed/6/donut.npy - 32\n",
            "processing file: data_processed/6/door.npy - 33\n",
            "processing file: data_processed/6/dragon.npy - 34\n",
            "processing file: data_processed/6/diamond.npy - 35\n",
            "processing file: data_processed/6/drums.npy - 36\n",
            "processing file: data_processed/6/duck.npy - 37\n",
            "processing file: data_processed/6/ear.npy - 38\n",
            "processing file: data_processed/6/elephant.npy - 39\n",
            "processing file: data_processed/6/envelope.npy - 40\n",
            "processing file: data_processed/6/eraser.npy - 41\n",
            "processing file: data_processed/6/eye.npy - 42\n",
            "processing file: data_processed/6/face.npy - 43\n",
            "processing file: data_processed/6/fan.npy - 44\n",
            "processing file: data_processed/6/finger.npy - 45\n",
            "processing file: data_processed/6/fish.npy - 46\n",
            "processing file: data_processed/6/flower.npy - 47\n",
            "processing file: data_processed/6/frog.npy - 48\n",
            "processing file: data_processed/6/garden.npy - 49\n",
            "processing file: data_processed/6/grapes.npy - 50\n",
            "processing file: data_processed/6/grass.npy - 51\n",
            "processing file: data_processed/6/guitar.npy - 52\n",
            "processing file: data_processed/6/hammer.npy - 53\n",
            "processing file: data_processed/6/hand.npy - 54\n",
            "processing file: data_processed/6/hat.npy - 55\n",
            "processing file: data_processed/6/helmet.npy - 56\n",
            "processing file: data_processed/6/horse.npy - 57\n",
            "processing file: data_processed/6/house.npy - 58\n",
            "processing file: data_processed/6/hospital.npy - 59\n",
            "processing file: data_processed/6/jail.npy - 60\n",
            "processing file: data_processed/6/kangaroo.npy - 61\n",
            "processing file: data_processed/6/knife.npy - 62\n",
            "processing file: data_processed/6/laptop.npy - 63\n",
            "processing file: data_processed/6/leg.npy - 64\n",
            "processing file: data_processed/6/lion.npy - 65\n",
            "processing file: data_processed/6/lollipop.npy - 66\n",
            "processing file: data_processed/6/map.npy - 67\n",
            "processing file: data_processed/6/microwave.npy - 68\n",
            "processing file: data_processed/6/monkey.npy - 69\n",
            "processing file: data_processed/6/moon.npy - 70\n",
            "processing file: data_processed/6/mountain.npy - 71\n",
            "processing file: data_processed/6/mouse.npy - 72\n",
            "processing file: data_processed/6/mug.npy - 73\n",
            "processing file: data_processed/6/nail.npy - 74\n",
            "processing file: data_processed/6/nose.npy - 75\n",
            "processing file: data_processed/6/onion.npy - 76\n",
            "processing file: data_processed/6/owl.npy - 77\n",
            "processing file: data_processed/6/panda.npy - 78\n",
            "processing file: data_processed/6/parachute.npy - 79\n",
            "processing file: data_processed/6/parrot.npy - 80\n",
            "processing file: data_processed/6/passport.npy - 81\n",
            "processing file: data_processed/6/peanut.npy - 82\n",
            "processing file: data_processed/6/pencil.npy - 83\n",
            "processing file: data_processed/6/piano.npy - 84\n",
            "processing file: data_processed/6/pig.npy - 85\n",
            "processing file: data_processed/6/pizza.npy - 86\n",
            "processing file: data_processed/6/potato.npy - 87\n",
            "processing file: data_processed/6/rabbit.npy - 88\n",
            "processing file: data_processed/6/radio.npy - 89\n",
            "processing file: data_processed/6/rain.npy - 90\n",
            "processing file: data_processed/6/rainbow.npy - 91\n",
            "processing file: data_processed/6/river.npy - 92\n",
            "processing file: data_processed/6/sandwich.npy - 93\n",
            "processing file: data_processed/6/saw.npy - 94\n",
            "processing file: data_processed/6/sun.npy - 95\n",
            "processing file: data_processed/6/shark.npy - 96\n",
            "processing file: data_processed/6/shoe.npy - 97\n",
            "processing file: data_processed/6/skull.npy - 98\n",
            "processing file: data_processed/6/snail.npy - 99\n",
            "processing file: data_processed/6/snowflake.npy - 100\n",
            "processing file: data_processed/6/spider.npy - 101\n",
            "processing file: data_processed/6/square.npy - 102\n",
            "processing file: data_processed/6/stairs.npy - 103\n",
            "processing file: data_processed/6/star.npy - 104\n",
            "processing file: data_processed/6/stove.npy - 105\n",
            "processing file: data_processed/6/strawberry.npy - 106\n",
            "processing file: data_processed/6/table.npy - 107\n",
            "processing file: data_processed/6/teapot.npy - 108\n",
            "processing file: data_processed/6/television.npy - 109\n",
            "processing file: data_processed/6/tent.npy - 110\n",
            "processing file: data_processed/6/toilet.npy - 111\n",
            "processing file: data_processed/6/toothbrush.npy - 112\n",
            "processing file: data_processed/6/toothpaste.npy - 113\n",
            "processing file: data_processed/6/train.npy - 114\n",
            "processing file: data_processed/6/triangle.npy - 115\n",
            "processing file: data_processed/6/truck.npy - 116\n",
            "processing file: data_processed/6/umbrella.npy - 117\n",
            "processing file: data_processed/6/whale.npy - 118\n",
            "processing file: data_processed/6/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9438 - accuracy: 0.7543 - val_loss: 0.7650 - val_accuracy: 0.7999\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9308 - accuracy: 0.7580 - val_loss: 0.7640 - val_accuracy: 0.7987\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9191 - accuracy: 0.7603 - val_loss: 0.7585 - val_accuracy: 0.8004\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9088 - accuracy: 0.7621 - val_loss: 0.7567 - val_accuracy: 0.8011\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9045 - accuracy: 0.7630 - val_loss: 0.7700 - val_accuracy: 0.7978\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9002 - accuracy: 0.7643 - val_loss: 0.7658 - val_accuracy: 0.7998\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8941 - accuracy: 0.7652 - val_loss: 0.7723 - val_accuracy: 0.7944\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8888 - accuracy: 0.7665 - val_loss: 0.7561 - val_accuracy: 0.8002\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8828 - accuracy: 0.7680 - val_loss: 0.7575 - val_accuracy: 0.8007\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8774 - accuracy: 0.7694 - val_loss: 0.7541 - val_accuracy: 0.8017\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7462 - accuracy: 0.8042\n",
            "Test accuarcy: 80.42%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82      2000\n",
            "           1       0.90      0.94      0.92      2000\n",
            "           2       0.75      0.82      0.79      2000\n",
            "           3       0.81      0.80      0.81      2000\n",
            "           4       0.85      0.84      0.84      2000\n",
            "           5       0.88      0.82      0.85      2000\n",
            "           6       0.75      0.84      0.79      2000\n",
            "           7       0.65      0.72      0.68      2000\n",
            "           8       0.56      0.47      0.51      2000\n",
            "           9       0.79      0.82      0.81      2000\n",
            "          10       0.87      0.87      0.87      2000\n",
            "          11       0.81      0.88      0.85      2000\n",
            "          12       0.90      0.93      0.92      2000\n",
            "          13       0.88      0.84      0.86      2000\n",
            "          14       0.82      0.90      0.86      2000\n",
            "          15       0.78      0.75      0.76      2000\n",
            "          16       0.82      0.85      0.83      2000\n",
            "          17       0.89      0.89      0.89      2000\n",
            "          18       0.86      0.86      0.86      2000\n",
            "          19       0.83      0.72      0.77      2000\n",
            "          20       0.89      0.92      0.91      2000\n",
            "          21       0.83      0.71      0.77      2000\n",
            "          22       0.77      0.86      0.82      2000\n",
            "          23       0.90      0.88      0.89      2000\n",
            "          24       0.84      0.91      0.87      2000\n",
            "          25       0.83      0.66      0.74      2000\n",
            "          26       0.74      0.83      0.78      2000\n",
            "          27       0.71      0.72      0.71      2000\n",
            "          28       0.77      0.75      0.76      2000\n",
            "          29       0.68      0.53      0.60      2000\n",
            "          30       0.70      0.46      0.55      2000\n",
            "          31       0.76      0.62      0.68      2000\n",
            "          32       0.87      0.93      0.90      2000\n",
            "          33       0.91      0.86      0.88      2000\n",
            "          34       0.51      0.55      0.53      2000\n",
            "          35       0.88      0.90      0.89      2000\n",
            "          36       0.75      0.78      0.76      2000\n",
            "          37       0.74      0.66      0.70      2000\n",
            "          38       0.89      0.81      0.85      2000\n",
            "          39       0.74      0.78      0.76      2000\n",
            "          40       0.97      0.95      0.96      2000\n",
            "          41       0.68      0.65      0.67      2000\n",
            "          42       0.89      0.90      0.89      2000\n",
            "          43       0.86      0.82      0.84      2000\n",
            "          44       0.77      0.72      0.74      2000\n",
            "          45       0.82      0.76      0.79      2000\n",
            "          46       0.84      0.86      0.85      2000\n",
            "          47       0.83      0.87      0.85      2000\n",
            "          48       0.57      0.51      0.54      2000\n",
            "          49       0.60      0.69      0.64      2000\n",
            "          50       0.77      0.84      0.80      2000\n",
            "          51       0.76      0.88      0.81      2000\n",
            "          52       0.88      0.93      0.90      2000\n",
            "          53       0.84      0.82      0.83      2000\n",
            "          54       0.88      0.88      0.88      2000\n",
            "          55       0.86      0.83      0.84      2000\n",
            "          56       0.80      0.76      0.78      2000\n",
            "          57       0.77      0.79      0.78      2000\n",
            "          58       0.88      0.92      0.90      2000\n",
            "          59       0.80      0.79      0.80      2000\n",
            "          60       0.85      0.91      0.88      2000\n",
            "          61       0.79      0.81      0.80      2000\n",
            "          62       0.84      0.79      0.81      2000\n",
            "          63       0.72      0.88      0.79      2000\n",
            "          64       0.80      0.87      0.83      2000\n",
            "          65       0.70      0.80      0.75      2000\n",
            "          66       0.93      0.93      0.93      2000\n",
            "          67       0.77      0.82      0.80      2000\n",
            "          68       0.85      0.92      0.88      2000\n",
            "          69       0.61      0.71      0.66      2000\n",
            "          70       0.75      0.59      0.66      2000\n",
            "          71       0.89      0.88      0.89      2000\n",
            "          72       0.63      0.52      0.57      2000\n",
            "          73       0.68      0.82      0.74      2000\n",
            "          74       0.86      0.63      0.73      2000\n",
            "          75       0.86      0.77      0.81      2000\n",
            "          76       0.82      0.81      0.82      2000\n",
            "          77       0.80      0.77      0.78      2000\n",
            "          78       0.76      0.77      0.77      2000\n",
            "          79       0.87      0.85      0.86      2000\n",
            "          80       0.72      0.66      0.69      2000\n",
            "          81       0.75      0.69      0.71      2000\n",
            "          82       0.82      0.82      0.82      2000\n",
            "          83       0.64      0.76      0.69      2000\n",
            "          84       0.76      0.68      0.72      2000\n",
            "          85       0.68      0.74      0.71      2000\n",
            "          86       0.79      0.85      0.82      2000\n",
            "          87       0.69      0.67      0.68      2000\n",
            "          88       0.80      0.79      0.80      2000\n",
            "          89       0.87      0.85      0.86      2000\n",
            "          90       0.85      0.92      0.89      2000\n",
            "          91       0.92      0.96      0.94      2000\n",
            "          92       0.78      0.81      0.80      2000\n",
            "          93       0.74      0.81      0.77      2000\n",
            "          94       0.85      0.84      0.84      2000\n",
            "          95       0.85      0.93      0.89      2000\n",
            "          96       0.77      0.73      0.75      2000\n",
            "          97       0.84      0.82      0.83      2000\n",
            "          98       0.91      0.91      0.91      2000\n",
            "          99       0.87      0.92      0.90      2000\n",
            "         100       0.84      0.88      0.86      2000\n",
            "         101       0.85      0.78      0.81      2000\n",
            "         102       0.89      0.93      0.91      2000\n",
            "         103       0.97      0.91      0.94      2000\n",
            "         104       0.92      0.91      0.91      2000\n",
            "         105       0.82      0.76      0.79      2000\n",
            "         106       0.91      0.89      0.90      2000\n",
            "         107       0.88      0.88      0.88      2000\n",
            "         108       0.85      0.87      0.86      2000\n",
            "         109       0.86      0.90      0.88      2000\n",
            "         110       0.92      0.82      0.86      2000\n",
            "         111       0.85      0.86      0.85      2000\n",
            "         112       0.80      0.85      0.82      2000\n",
            "         113       0.73      0.59      0.66      2000\n",
            "         114       0.65      0.81      0.72      2000\n",
            "         115       0.88      0.93      0.91      2000\n",
            "         116       0.80      0.74      0.77      2000\n",
            "         117       0.93      0.94      0.94      2000\n",
            "         118       0.76      0.74      0.75      2000\n",
            "         119       0.81      0.83      0.82      2000\n",
            "\n",
            "    accuracy                           0.80    240000\n",
            "   macro avg       0.80      0.80      0.80    240000\n",
            "weighted avg       0.80      0.80      0.80    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.19579583333333334\n",
            "processing batch: 7\n",
            "processing file: data_processed/7/airplane.npy - 0\n",
            "processing file: data_processed/7/apple.npy - 1\n",
            "processing file: data_processed/7/ant.npy - 2\n",
            "processing file: data_processed/7/axe.npy - 3\n",
            "processing file: data_processed/7/banana.npy - 4\n",
            "processing file: data_processed/7/baseball.npy - 5\n",
            "processing file: data_processed/7/bee.npy - 6\n",
            "processing file: data_processed/7/beach.npy - 7\n",
            "processing file: data_processed/7/bird.npy - 8\n",
            "processing file: data_processed/7/book.npy - 9\n",
            "processing file: data_processed/7/bucket.npy - 10\n",
            "processing file: data_processed/7/bus.npy - 11\n",
            "processing file: data_processed/7/butterfly.npy - 12\n",
            "processing file: data_processed/7/cactus.npy - 13\n",
            "processing file: data_processed/7/cake.npy - 14\n",
            "processing file: data_processed/7/cannon.npy - 15\n",
            "processing file: data_processed/7/car.npy - 16\n",
            "processing file: data_processed/7/carrot.npy - 17\n",
            "processing file: data_processed/7/castle.npy - 18\n",
            "processing file: data_processed/7/cat.npy - 19\n",
            "processing file: data_processed/7/chair.npy - 20\n",
            "processing file: data_processed/7/church.npy - 21\n",
            "processing file: data_processed/7/circle.npy - 22\n",
            "processing file: data_processed/7/clock.npy - 23\n",
            "processing file: data_processed/7/cloud.npy - 24\n",
            "processing file: data_processed/7/computer.npy - 25\n",
            "processing file: data_processed/7/cookie.npy - 26\n",
            "processing file: data_processed/7/cow.npy - 27\n",
            "processing file: data_processed/7/crab.npy - 28\n",
            "processing file: data_processed/7/crayon.npy - 29\n",
            "processing file: data_processed/7/cup.npy - 30\n",
            "processing file: data_processed/7/dolphin.npy - 31\n",
            "processing file: data_processed/7/donut.npy - 32\n",
            "processing file: data_processed/7/door.npy - 33\n",
            "processing file: data_processed/7/dragon.npy - 34\n",
            "processing file: data_processed/7/diamond.npy - 35\n",
            "processing file: data_processed/7/drums.npy - 36\n",
            "processing file: data_processed/7/duck.npy - 37\n",
            "processing file: data_processed/7/ear.npy - 38\n",
            "processing file: data_processed/7/elephant.npy - 39\n",
            "processing file: data_processed/7/envelope.npy - 40\n",
            "processing file: data_processed/7/eraser.npy - 41\n",
            "processing file: data_processed/7/eye.npy - 42\n",
            "processing file: data_processed/7/face.npy - 43\n",
            "processing file: data_processed/7/fan.npy - 44\n",
            "processing file: data_processed/7/finger.npy - 45\n",
            "processing file: data_processed/7/fish.npy - 46\n",
            "processing file: data_processed/7/flower.npy - 47\n",
            "processing file: data_processed/7/frog.npy - 48\n",
            "processing file: data_processed/7/garden.npy - 49\n",
            "processing file: data_processed/7/grapes.npy - 50\n",
            "processing file: data_processed/7/grass.npy - 51\n",
            "processing file: data_processed/7/guitar.npy - 52\n",
            "processing file: data_processed/7/hammer.npy - 53\n",
            "processing file: data_processed/7/hand.npy - 54\n",
            "processing file: data_processed/7/hat.npy - 55\n",
            "processing file: data_processed/7/helmet.npy - 56\n",
            "processing file: data_processed/7/horse.npy - 57\n",
            "processing file: data_processed/7/house.npy - 58\n",
            "processing file: data_processed/7/hospital.npy - 59\n",
            "processing file: data_processed/7/jail.npy - 60\n",
            "processing file: data_processed/7/kangaroo.npy - 61\n",
            "processing file: data_processed/7/knife.npy - 62\n",
            "processing file: data_processed/7/laptop.npy - 63\n",
            "processing file: data_processed/7/leg.npy - 64\n",
            "processing file: data_processed/7/lion.npy - 65\n",
            "processing file: data_processed/7/lollipop.npy - 66\n",
            "processing file: data_processed/7/map.npy - 67\n",
            "processing file: data_processed/7/microwave.npy - 68\n",
            "processing file: data_processed/7/monkey.npy - 69\n",
            "processing file: data_processed/7/moon.npy - 70\n",
            "processing file: data_processed/7/mountain.npy - 71\n",
            "processing file: data_processed/7/mouse.npy - 72\n",
            "processing file: data_processed/7/mug.npy - 73\n",
            "processing file: data_processed/7/nail.npy - 74\n",
            "processing file: data_processed/7/nose.npy - 75\n",
            "processing file: data_processed/7/onion.npy - 76\n",
            "processing file: data_processed/7/owl.npy - 77\n",
            "processing file: data_processed/7/panda.npy - 78\n",
            "processing file: data_processed/7/parachute.npy - 79\n",
            "processing file: data_processed/7/parrot.npy - 80\n",
            "processing file: data_processed/7/passport.npy - 81\n",
            "processing file: data_processed/7/peanut.npy - 82\n",
            "processing file: data_processed/7/pencil.npy - 83\n",
            "processing file: data_processed/7/piano.npy - 84\n",
            "processing file: data_processed/7/pig.npy - 85\n",
            "processing file: data_processed/7/pizza.npy - 86\n",
            "processing file: data_processed/7/potato.npy - 87\n",
            "processing file: data_processed/7/rabbit.npy - 88\n",
            "processing file: data_processed/7/radio.npy - 89\n",
            "processing file: data_processed/7/rain.npy - 90\n",
            "processing file: data_processed/7/rainbow.npy - 91\n",
            "processing file: data_processed/7/river.npy - 92\n",
            "processing file: data_processed/7/sandwich.npy - 93\n",
            "processing file: data_processed/7/saw.npy - 94\n",
            "processing file: data_processed/7/sun.npy - 95\n",
            "processing file: data_processed/7/shark.npy - 96\n",
            "processing file: data_processed/7/shoe.npy - 97\n",
            "processing file: data_processed/7/skull.npy - 98\n",
            "processing file: data_processed/7/snail.npy - 99\n",
            "processing file: data_processed/7/snowflake.npy - 100\n",
            "processing file: data_processed/7/spider.npy - 101\n",
            "processing file: data_processed/7/square.npy - 102\n",
            "processing file: data_processed/7/stairs.npy - 103\n",
            "processing file: data_processed/7/star.npy - 104\n",
            "processing file: data_processed/7/stove.npy - 105\n",
            "processing file: data_processed/7/strawberry.npy - 106\n",
            "processing file: data_processed/7/table.npy - 107\n",
            "processing file: data_processed/7/teapot.npy - 108\n",
            "processing file: data_processed/7/television.npy - 109\n",
            "processing file: data_processed/7/tent.npy - 110\n",
            "processing file: data_processed/7/toilet.npy - 111\n",
            "processing file: data_processed/7/toothbrush.npy - 112\n",
            "processing file: data_processed/7/toothpaste.npy - 113\n",
            "processing file: data_processed/7/train.npy - 114\n",
            "processing file: data_processed/7/triangle.npy - 115\n",
            "processing file: data_processed/7/truck.npy - 116\n",
            "processing file: data_processed/7/umbrella.npy - 117\n",
            "processing file: data_processed/7/whale.npy - 118\n",
            "processing file: data_processed/7/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9335 - accuracy: 0.7579 - val_loss: 0.7498 - val_accuracy: 0.8026\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9188 - accuracy: 0.7606 - val_loss: 0.7434 - val_accuracy: 0.8058\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9072 - accuracy: 0.7641 - val_loss: 0.7489 - val_accuracy: 0.8029\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8990 - accuracy: 0.7656 - val_loss: 0.7423 - val_accuracy: 0.8050\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8930 - accuracy: 0.7667 - val_loss: 0.7471 - val_accuracy: 0.8043\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8865 - accuracy: 0.7673 - val_loss: 0.7525 - val_accuracy: 0.8024\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8818 - accuracy: 0.7686 - val_loss: 0.7516 - val_accuracy: 0.8029\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8784 - accuracy: 0.7686 - val_loss: 0.7396 - val_accuracy: 0.8061\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8740 - accuracy: 0.7704 - val_loss: 0.7418 - val_accuracy: 0.8067\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8682 - accuracy: 0.7720 - val_loss: 0.7363 - val_accuracy: 0.8068\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7319 - accuracy: 0.8075\n",
            "Test accuarcy: 80.75%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83      2000\n",
            "           1       0.89      0.94      0.91      2000\n",
            "           2       0.77      0.82      0.80      2000\n",
            "           3       0.84      0.80      0.82      2000\n",
            "           4       0.88      0.80      0.84      2000\n",
            "           5       0.87      0.83      0.85      2000\n",
            "           6       0.80      0.82      0.81      2000\n",
            "           7       0.70      0.69      0.70      2000\n",
            "           8       0.59      0.44      0.50      2000\n",
            "           9       0.79      0.83      0.81      2000\n",
            "          10       0.84      0.88      0.86      2000\n",
            "          11       0.83      0.87      0.85      2000\n",
            "          12       0.91      0.93      0.92      2000\n",
            "          13       0.87      0.86      0.86      2000\n",
            "          14       0.87      0.89      0.88      2000\n",
            "          15       0.77      0.77      0.77      2000\n",
            "          16       0.85      0.84      0.85      2000\n",
            "          17       0.89      0.89      0.89      2000\n",
            "          18       0.84      0.87      0.85      2000\n",
            "          19       0.78      0.74      0.76      2000\n",
            "          20       0.90      0.91      0.91      2000\n",
            "          21       0.81      0.71      0.76      2000\n",
            "          22       0.78      0.85      0.81      2000\n",
            "          23       0.91      0.88      0.89      2000\n",
            "          24       0.85      0.91      0.88      2000\n",
            "          25       0.80      0.69      0.74      2000\n",
            "          26       0.73      0.85      0.79      2000\n",
            "          27       0.75      0.68      0.71      2000\n",
            "          28       0.78      0.74      0.76      2000\n",
            "          29       0.68      0.54      0.60      2000\n",
            "          30       0.71      0.46      0.56      2000\n",
            "          31       0.73      0.64      0.68      2000\n",
            "          32       0.89      0.93      0.91      2000\n",
            "          33       0.94      0.85      0.89      2000\n",
            "          34       0.51      0.57      0.54      2000\n",
            "          35       0.88      0.90      0.89      2000\n",
            "          36       0.73      0.81      0.77      2000\n",
            "          37       0.69      0.72      0.70      2000\n",
            "          38       0.87      0.83      0.85      2000\n",
            "          39       0.74      0.78      0.76      2000\n",
            "          40       0.98      0.95      0.97      2000\n",
            "          41       0.72      0.64      0.68      2000\n",
            "          42       0.91      0.89      0.90      2000\n",
            "          43       0.87      0.82      0.84      2000\n",
            "          44       0.74      0.74      0.74      2000\n",
            "          45       0.81      0.77      0.79      2000\n",
            "          46       0.86      0.87      0.87      2000\n",
            "          47       0.79      0.90      0.84      2000\n",
            "          48       0.61      0.49      0.54      2000\n",
            "          49       0.58      0.71      0.64      2000\n",
            "          50       0.67      0.87      0.76      2000\n",
            "          51       0.79      0.87      0.83      2000\n",
            "          52       0.89      0.93      0.91      2000\n",
            "          53       0.86      0.81      0.83      2000\n",
            "          54       0.88      0.88      0.88      2000\n",
            "          55       0.84      0.83      0.84      2000\n",
            "          56       0.83      0.74      0.78      2000\n",
            "          57       0.73      0.83      0.77      2000\n",
            "          58       0.89      0.92      0.90      2000\n",
            "          59       0.74      0.83      0.78      2000\n",
            "          60       0.86      0.90      0.88      2000\n",
            "          61       0.79      0.81      0.80      2000\n",
            "          62       0.83      0.79      0.81      2000\n",
            "          63       0.75      0.85      0.80      2000\n",
            "          64       0.80      0.87      0.83      2000\n",
            "          65       0.73      0.80      0.76      2000\n",
            "          66       0.93      0.93      0.93      2000\n",
            "          67       0.77      0.82      0.80      2000\n",
            "          68       0.86      0.91      0.88      2000\n",
            "          69       0.61      0.73      0.67      2000\n",
            "          70       0.73      0.62      0.67      2000\n",
            "          71       0.88      0.89      0.88      2000\n",
            "          72       0.61      0.56      0.59      2000\n",
            "          73       0.68      0.83      0.75      2000\n",
            "          74       0.84      0.65      0.73      2000\n",
            "          75       0.85      0.77      0.81      2000\n",
            "          76       0.82      0.80      0.81      2000\n",
            "          77       0.78      0.79      0.78      2000\n",
            "          78       0.80      0.75      0.77      2000\n",
            "          79       0.88      0.85      0.86      2000\n",
            "          80       0.74      0.62      0.68      2000\n",
            "          81       0.76      0.68      0.72      2000\n",
            "          82       0.82      0.83      0.82      2000\n",
            "          83       0.66      0.76      0.70      2000\n",
            "          84       0.77      0.71      0.74      2000\n",
            "          85       0.71      0.75      0.73      2000\n",
            "          86       0.81      0.84      0.83      2000\n",
            "          87       0.69      0.68      0.68      2000\n",
            "          88       0.79      0.81      0.80      2000\n",
            "          89       0.89      0.85      0.87      2000\n",
            "          90       0.86      0.92      0.89      2000\n",
            "          91       0.94      0.95      0.94      2000\n",
            "          92       0.75      0.85      0.79      2000\n",
            "          93       0.77      0.80      0.78      2000\n",
            "          94       0.90      0.83      0.87      2000\n",
            "          95       0.88      0.92      0.90      2000\n",
            "          96       0.74      0.76      0.75      2000\n",
            "          97       0.84      0.83      0.83      2000\n",
            "          98       0.91      0.91      0.91      2000\n",
            "          99       0.87      0.92      0.90      2000\n",
            "         100       0.89      0.87      0.88      2000\n",
            "         101       0.80      0.81      0.81      2000\n",
            "         102       0.87      0.93      0.90      2000\n",
            "         103       0.96      0.92      0.94      2000\n",
            "         104       0.91      0.91      0.91      2000\n",
            "         105       0.79      0.78      0.79      2000\n",
            "         106       0.90      0.91      0.90      2000\n",
            "         107       0.88      0.89      0.89      2000\n",
            "         108       0.86      0.87      0.86      2000\n",
            "         109       0.90      0.88      0.89      2000\n",
            "         110       0.91      0.82      0.86      2000\n",
            "         111       0.86      0.87      0.86      2000\n",
            "         112       0.80      0.85      0.83      2000\n",
            "         113       0.73      0.60      0.66      2000\n",
            "         114       0.76      0.75      0.76      2000\n",
            "         115       0.89      0.93      0.91      2000\n",
            "         116       0.79      0.77      0.78      2000\n",
            "         117       0.95      0.93      0.94      2000\n",
            "         118       0.80      0.73      0.76      2000\n",
            "         119       0.81      0.83      0.82      2000\n",
            "\n",
            "    accuracy                           0.81    240000\n",
            "   macro avg       0.81      0.81      0.81    240000\n",
            "weighted avg       0.81      0.81      0.81    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.19247083333333334\n",
            "processing batch: 8\n",
            "processing file: data_processed/8/airplane.npy - 0\n",
            "processing file: data_processed/8/apple.npy - 1\n",
            "processing file: data_processed/8/ant.npy - 2\n",
            "processing file: data_processed/8/axe.npy - 3\n",
            "processing file: data_processed/8/banana.npy - 4\n",
            "processing file: data_processed/8/baseball.npy - 5\n",
            "processing file: data_processed/8/bee.npy - 6\n",
            "processing file: data_processed/8/beach.npy - 7\n",
            "processing file: data_processed/8/bird.npy - 8\n",
            "processing file: data_processed/8/book.npy - 9\n",
            "processing file: data_processed/8/bucket.npy - 10\n",
            "processing file: data_processed/8/bus.npy - 11\n",
            "processing file: data_processed/8/butterfly.npy - 12\n",
            "processing file: data_processed/8/cactus.npy - 13\n",
            "processing file: data_processed/8/cake.npy - 14\n",
            "processing file: data_processed/8/cannon.npy - 15\n",
            "processing file: data_processed/8/car.npy - 16\n",
            "processing file: data_processed/8/carrot.npy - 17\n",
            "processing file: data_processed/8/castle.npy - 18\n",
            "processing file: data_processed/8/cat.npy - 19\n",
            "processing file: data_processed/8/chair.npy - 20\n",
            "processing file: data_processed/8/church.npy - 21\n",
            "processing file: data_processed/8/circle.npy - 22\n",
            "processing file: data_processed/8/clock.npy - 23\n",
            "processing file: data_processed/8/cloud.npy - 24\n",
            "processing file: data_processed/8/computer.npy - 25\n",
            "processing file: data_processed/8/cookie.npy - 26\n",
            "processing file: data_processed/8/cow.npy - 27\n",
            "processing file: data_processed/8/crab.npy - 28\n",
            "processing file: data_processed/8/crayon.npy - 29\n",
            "processing file: data_processed/8/cup.npy - 30\n",
            "processing file: data_processed/8/dolphin.npy - 31\n",
            "processing file: data_processed/8/donut.npy - 32\n",
            "processing file: data_processed/8/door.npy - 33\n",
            "processing file: data_processed/8/dragon.npy - 34\n",
            "processing file: data_processed/8/diamond.npy - 35\n",
            "processing file: data_processed/8/drums.npy - 36\n",
            "processing file: data_processed/8/duck.npy - 37\n",
            "processing file: data_processed/8/ear.npy - 38\n",
            "processing file: data_processed/8/elephant.npy - 39\n",
            "processing file: data_processed/8/envelope.npy - 40\n",
            "processing file: data_processed/8/eraser.npy - 41\n",
            "processing file: data_processed/8/eye.npy - 42\n",
            "processing file: data_processed/8/face.npy - 43\n",
            "processing file: data_processed/8/fan.npy - 44\n",
            "processing file: data_processed/8/finger.npy - 45\n",
            "processing file: data_processed/8/fish.npy - 46\n",
            "processing file: data_processed/8/flower.npy - 47\n",
            "processing file: data_processed/8/frog.npy - 48\n",
            "processing file: data_processed/8/garden.npy - 49\n",
            "processing file: data_processed/8/grapes.npy - 50\n",
            "processing file: data_processed/8/grass.npy - 51\n",
            "processing file: data_processed/8/guitar.npy - 52\n",
            "processing file: data_processed/8/hammer.npy - 53\n",
            "processing file: data_processed/8/hand.npy - 54\n",
            "processing file: data_processed/8/hat.npy - 55\n",
            "processing file: data_processed/8/helmet.npy - 56\n",
            "processing file: data_processed/8/horse.npy - 57\n",
            "processing file: data_processed/8/house.npy - 58\n",
            "processing file: data_processed/8/hospital.npy - 59\n",
            "processing file: data_processed/8/jail.npy - 60\n",
            "processing file: data_processed/8/kangaroo.npy - 61\n",
            "processing file: data_processed/8/knife.npy - 62\n",
            "processing file: data_processed/8/laptop.npy - 63\n",
            "processing file: data_processed/8/leg.npy - 64\n",
            "processing file: data_processed/8/lion.npy - 65\n",
            "processing file: data_processed/8/lollipop.npy - 66\n",
            "processing file: data_processed/8/map.npy - 67\n",
            "processing file: data_processed/8/microwave.npy - 68\n",
            "processing file: data_processed/8/monkey.npy - 69\n",
            "processing file: data_processed/8/moon.npy - 70\n",
            "processing file: data_processed/8/mountain.npy - 71\n",
            "processing file: data_processed/8/mouse.npy - 72\n",
            "processing file: data_processed/8/mug.npy - 73\n",
            "processing file: data_processed/8/nail.npy - 74\n",
            "processing file: data_processed/8/nose.npy - 75\n",
            "processing file: data_processed/8/onion.npy - 76\n",
            "processing file: data_processed/8/owl.npy - 77\n",
            "processing file: data_processed/8/panda.npy - 78\n",
            "processing file: data_processed/8/parachute.npy - 79\n",
            "processing file: data_processed/8/parrot.npy - 80\n",
            "processing file: data_processed/8/passport.npy - 81\n",
            "processing file: data_processed/8/peanut.npy - 82\n",
            "processing file: data_processed/8/pencil.npy - 83\n",
            "processing file: data_processed/8/piano.npy - 84\n",
            "processing file: data_processed/8/pig.npy - 85\n",
            "processing file: data_processed/8/pizza.npy - 86\n",
            "processing file: data_processed/8/potato.npy - 87\n",
            "processing file: data_processed/8/rabbit.npy - 88\n",
            "processing file: data_processed/8/radio.npy - 89\n",
            "processing file: data_processed/8/rain.npy - 90\n",
            "processing file: data_processed/8/rainbow.npy - 91\n",
            "processing file: data_processed/8/river.npy - 92\n",
            "processing file: data_processed/8/sandwich.npy - 93\n",
            "processing file: data_processed/8/saw.npy - 94\n",
            "processing file: data_processed/8/sun.npy - 95\n",
            "processing file: data_processed/8/shark.npy - 96\n",
            "processing file: data_processed/8/shoe.npy - 97\n",
            "processing file: data_processed/8/skull.npy - 98\n",
            "processing file: data_processed/8/snail.npy - 99\n",
            "processing file: data_processed/8/snowflake.npy - 100\n",
            "processing file: data_processed/8/spider.npy - 101\n",
            "processing file: data_processed/8/square.npy - 102\n",
            "processing file: data_processed/8/stairs.npy - 103\n",
            "processing file: data_processed/8/star.npy - 104\n",
            "processing file: data_processed/8/stove.npy - 105\n",
            "processing file: data_processed/8/strawberry.npy - 106\n",
            "processing file: data_processed/8/table.npy - 107\n",
            "processing file: data_processed/8/teapot.npy - 108\n",
            "processing file: data_processed/8/television.npy - 109\n",
            "processing file: data_processed/8/tent.npy - 110\n",
            "processing file: data_processed/8/toilet.npy - 111\n",
            "processing file: data_processed/8/toothbrush.npy - 112\n",
            "processing file: data_processed/8/toothpaste.npy - 113\n",
            "processing file: data_processed/8/train.npy - 114\n",
            "processing file: data_processed/8/triangle.npy - 115\n",
            "processing file: data_processed/8/truck.npy - 116\n",
            "processing file: data_processed/8/umbrella.npy - 117\n",
            "processing file: data_processed/8/whale.npy - 118\n",
            "processing file: data_processed/8/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9149 - accuracy: 0.7617 - val_loss: 0.7391 - val_accuracy: 0.8068\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.9007 - accuracy: 0.7649 - val_loss: 0.7459 - val_accuracy: 0.8055\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8903 - accuracy: 0.7663 - val_loss: 0.7356 - val_accuracy: 0.8063\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8841 - accuracy: 0.7678 - val_loss: 0.7361 - val_accuracy: 0.8067\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8752 - accuracy: 0.7706 - val_loss: 0.7380 - val_accuracy: 0.8061\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8723 - accuracy: 0.7706 - val_loss: 0.7389 - val_accuracy: 0.8063\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8636 - accuracy: 0.7728 - val_loss: 0.7400 - val_accuracy: 0.8060\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8605 - accuracy: 0.7736 - val_loss: 0.7525 - val_accuracy: 0.8027\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8545 - accuracy: 0.7739 - val_loss: 0.7370 - val_accuracy: 0.8077\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8499 - accuracy: 0.7756 - val_loss: 0.7390 - val_accuracy: 0.8050\n",
            "7500/7500 [==============================] - 24s 3ms/step - loss: 0.7281 - accuracy: 0.8087\n",
            "Test accuarcy: 80.87%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84      2000\n",
            "           1       0.92      0.92      0.92      2000\n",
            "           2       0.71      0.84      0.77      2000\n",
            "           3       0.86      0.79      0.82      2000\n",
            "           4       0.87      0.83      0.85      2000\n",
            "           5       0.87      0.85      0.86      2000\n",
            "           6       0.80      0.81      0.80      2000\n",
            "           7       0.65      0.74      0.69      2000\n",
            "           8       0.59      0.42      0.50      2000\n",
            "           9       0.80      0.82      0.81      2000\n",
            "          10       0.87      0.87      0.87      2000\n",
            "          11       0.79      0.90      0.84      2000\n",
            "          12       0.91      0.93      0.92      2000\n",
            "          13       0.87      0.86      0.87      2000\n",
            "          14       0.82      0.90      0.86      2000\n",
            "          15       0.76      0.77      0.77      2000\n",
            "          16       0.81      0.86      0.83      2000\n",
            "          17       0.91      0.89      0.90      2000\n",
            "          18       0.87      0.86      0.86      2000\n",
            "          19       0.82      0.72      0.77      2000\n",
            "          20       0.91      0.91      0.91      2000\n",
            "          21       0.80      0.73      0.76      2000\n",
            "          22       0.78      0.85      0.81      2000\n",
            "          23       0.92      0.87      0.90      2000\n",
            "          24       0.85      0.91      0.88      2000\n",
            "          25       0.81      0.69      0.75      2000\n",
            "          26       0.73      0.85      0.79      2000\n",
            "          27       0.66      0.77      0.71      2000\n",
            "          28       0.79      0.74      0.76      2000\n",
            "          29       0.65      0.61      0.63      2000\n",
            "          30       0.69      0.51      0.59      2000\n",
            "          31       0.76      0.64      0.69      2000\n",
            "          32       0.88      0.93      0.90      2000\n",
            "          33       0.92      0.88      0.90      2000\n",
            "          34       0.54      0.54      0.54      2000\n",
            "          35       0.89      0.90      0.89      2000\n",
            "          36       0.72      0.81      0.76      2000\n",
            "          37       0.70      0.72      0.71      2000\n",
            "          38       0.85      0.85      0.85      2000\n",
            "          39       0.75      0.79      0.77      2000\n",
            "          40       0.98      0.95      0.97      2000\n",
            "          41       0.72      0.67      0.69      2000\n",
            "          42       0.90      0.90      0.90      2000\n",
            "          43       0.89      0.81      0.85      2000\n",
            "          44       0.75      0.73      0.74      2000\n",
            "          45       0.83      0.76      0.79      2000\n",
            "          46       0.85      0.87      0.86      2000\n",
            "          47       0.80      0.89      0.84      2000\n",
            "          48       0.59      0.51      0.55      2000\n",
            "          49       0.56      0.72      0.63      2000\n",
            "          50       0.67      0.87      0.76      2000\n",
            "          51       0.81      0.85      0.83      2000\n",
            "          52       0.91      0.91      0.91      2000\n",
            "          53       0.84      0.83      0.84      2000\n",
            "          54       0.88      0.88      0.88      2000\n",
            "          55       0.86      0.81      0.84      2000\n",
            "          56       0.82      0.74      0.78      2000\n",
            "          57       0.80      0.77      0.78      2000\n",
            "          58       0.91      0.91      0.91      2000\n",
            "          59       0.76      0.82      0.79      2000\n",
            "          60       0.84      0.92      0.88      2000\n",
            "          61       0.78      0.83      0.81      2000\n",
            "          62       0.85      0.80      0.82      2000\n",
            "          63       0.74      0.87      0.80      2000\n",
            "          64       0.82      0.85      0.84      2000\n",
            "          65       0.68      0.83      0.75      2000\n",
            "          66       0.94      0.92      0.93      2000\n",
            "          67       0.75      0.84      0.79      2000\n",
            "          68       0.87      0.91      0.89      2000\n",
            "          69       0.64      0.72      0.68      2000\n",
            "          70       0.79      0.57      0.66      2000\n",
            "          71       0.90      0.88      0.89      2000\n",
            "          72       0.61      0.57      0.59      2000\n",
            "          73       0.71      0.79      0.75      2000\n",
            "          74       0.87      0.63      0.73      2000\n",
            "          75       0.85      0.79      0.82      2000\n",
            "          76       0.83      0.81      0.82      2000\n",
            "          77       0.77      0.79      0.78      2000\n",
            "          78       0.76      0.78      0.77      2000\n",
            "          79       0.88      0.86      0.87      2000\n",
            "          80       0.73      0.66      0.69      2000\n",
            "          81       0.79      0.66      0.72      2000\n",
            "          82       0.82      0.83      0.83      2000\n",
            "          83       0.71      0.69      0.70      2000\n",
            "          84       0.80      0.68      0.74      2000\n",
            "          85       0.68      0.75      0.71      2000\n",
            "          86       0.82      0.85      0.83      2000\n",
            "          87       0.69      0.66      0.67      2000\n",
            "          88       0.80      0.80      0.80      2000\n",
            "          89       0.88      0.86      0.87      2000\n",
            "          90       0.85      0.92      0.88      2000\n",
            "          91       0.93      0.95      0.94      2000\n",
            "          92       0.77      0.83      0.80      2000\n",
            "          93       0.81      0.80      0.80      2000\n",
            "          94       0.86      0.84      0.85      2000\n",
            "          95       0.87      0.92      0.90      2000\n",
            "          96       0.76      0.75      0.76      2000\n",
            "          97       0.87      0.80      0.83      2000\n",
            "          98       0.92      0.91      0.91      2000\n",
            "          99       0.91      0.91      0.91      2000\n",
            "         100       0.86      0.89      0.87      2000\n",
            "         101       0.81      0.82      0.81      2000\n",
            "         102       0.91      0.92      0.91      2000\n",
            "         103       0.96      0.92      0.94      2000\n",
            "         104       0.92      0.92      0.92      2000\n",
            "         105       0.80      0.80      0.80      2000\n",
            "         106       0.88      0.92      0.90      2000\n",
            "         107       0.89      0.89      0.89      2000\n",
            "         108       0.87      0.87      0.87      2000\n",
            "         109       0.91      0.88      0.89      2000\n",
            "         110       0.93      0.82      0.87      2000\n",
            "         111       0.86      0.86      0.86      2000\n",
            "         112       0.83      0.83      0.83      2000\n",
            "         113       0.71      0.62      0.66      2000\n",
            "         114       0.68      0.80      0.73      2000\n",
            "         115       0.90      0.92      0.91      2000\n",
            "         116       0.82      0.73      0.77      2000\n",
            "         117       0.94      0.94      0.94      2000\n",
            "         118       0.76      0.75      0.76      2000\n",
            "         119       0.84      0.82      0.83      2000\n",
            "\n",
            "    accuracy                           0.81    240000\n",
            "   macro avg       0.81      0.81      0.81    240000\n",
            "weighted avg       0.81      0.81      0.81    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.19129166666666667\n",
            "processing batch: 9\n",
            "processing file: data_processed/9/airplane.npy - 0\n",
            "processing file: data_processed/9/apple.npy - 1\n",
            "processing file: data_processed/9/ant.npy - 2\n",
            "processing file: data_processed/9/axe.npy - 3\n",
            "processing file: data_processed/9/banana.npy - 4\n",
            "processing file: data_processed/9/baseball.npy - 5\n",
            "processing file: data_processed/9/bee.npy - 6\n",
            "processing file: data_processed/9/beach.npy - 7\n",
            "processing file: data_processed/9/bird.npy - 8\n",
            "processing file: data_processed/9/book.npy - 9\n",
            "processing file: data_processed/9/bucket.npy - 10\n",
            "processing file: data_processed/9/bus.npy - 11\n",
            "processing file: data_processed/9/butterfly.npy - 12\n",
            "processing file: data_processed/9/cactus.npy - 13\n",
            "processing file: data_processed/9/cake.npy - 14\n",
            "processing file: data_processed/9/cannon.npy - 15\n",
            "processing file: data_processed/9/car.npy - 16\n",
            "processing file: data_processed/9/carrot.npy - 17\n",
            "processing file: data_processed/9/castle.npy - 18\n",
            "processing file: data_processed/9/cat.npy - 19\n",
            "processing file: data_processed/9/chair.npy - 20\n",
            "processing file: data_processed/9/church.npy - 21\n",
            "processing file: data_processed/9/circle.npy - 22\n",
            "processing file: data_processed/9/clock.npy - 23\n",
            "processing file: data_processed/9/cloud.npy - 24\n",
            "processing file: data_processed/9/computer.npy - 25\n",
            "processing file: data_processed/9/cookie.npy - 26\n",
            "processing file: data_processed/9/cow.npy - 27\n",
            "processing file: data_processed/9/crab.npy - 28\n",
            "processing file: data_processed/9/crayon.npy - 29\n",
            "processing file: data_processed/9/cup.npy - 30\n",
            "processing file: data_processed/9/dolphin.npy - 31\n",
            "processing file: data_processed/9/donut.npy - 32\n",
            "processing file: data_processed/9/door.npy - 33\n",
            "processing file: data_processed/9/dragon.npy - 34\n",
            "processing file: data_processed/9/diamond.npy - 35\n",
            "processing file: data_processed/9/drums.npy - 36\n",
            "processing file: data_processed/9/duck.npy - 37\n",
            "processing file: data_processed/9/ear.npy - 38\n",
            "processing file: data_processed/9/elephant.npy - 39\n",
            "processing file: data_processed/9/envelope.npy - 40\n",
            "processing file: data_processed/9/eraser.npy - 41\n",
            "processing file: data_processed/9/eye.npy - 42\n",
            "processing file: data_processed/9/face.npy - 43\n",
            "processing file: data_processed/9/fan.npy - 44\n",
            "processing file: data_processed/9/finger.npy - 45\n",
            "processing file: data_processed/9/fish.npy - 46\n",
            "processing file: data_processed/9/flower.npy - 47\n",
            "processing file: data_processed/9/frog.npy - 48\n",
            "processing file: data_processed/9/garden.npy - 49\n",
            "processing file: data_processed/9/grapes.npy - 50\n",
            "processing file: data_processed/9/grass.npy - 51\n",
            "processing file: data_processed/9/guitar.npy - 52\n",
            "processing file: data_processed/9/hammer.npy - 53\n",
            "processing file: data_processed/9/hand.npy - 54\n",
            "processing file: data_processed/9/hat.npy - 55\n",
            "processing file: data_processed/9/helmet.npy - 56\n",
            "processing file: data_processed/9/horse.npy - 57\n",
            "processing file: data_processed/9/house.npy - 58\n",
            "processing file: data_processed/9/hospital.npy - 59\n",
            "processing file: data_processed/9/jail.npy - 60\n",
            "processing file: data_processed/9/kangaroo.npy - 61\n",
            "processing file: data_processed/9/knife.npy - 62\n",
            "processing file: data_processed/9/laptop.npy - 63\n",
            "processing file: data_processed/9/leg.npy - 64\n",
            "processing file: data_processed/9/lion.npy - 65\n",
            "processing file: data_processed/9/lollipop.npy - 66\n",
            "processing file: data_processed/9/map.npy - 67\n",
            "processing file: data_processed/9/microwave.npy - 68\n",
            "processing file: data_processed/9/monkey.npy - 69\n",
            "processing file: data_processed/9/moon.npy - 70\n",
            "processing file: data_processed/9/mountain.npy - 71\n",
            "processing file: data_processed/9/mouse.npy - 72\n",
            "processing file: data_processed/9/mug.npy - 73\n",
            "processing file: data_processed/9/nail.npy - 74\n",
            "processing file: data_processed/9/nose.npy - 75\n",
            "processing file: data_processed/9/onion.npy - 76\n",
            "processing file: data_processed/9/owl.npy - 77\n",
            "processing file: data_processed/9/panda.npy - 78\n",
            "processing file: data_processed/9/parachute.npy - 79\n",
            "processing file: data_processed/9/parrot.npy - 80\n",
            "processing file: data_processed/9/passport.npy - 81\n",
            "processing file: data_processed/9/peanut.npy - 82\n",
            "processing file: data_processed/9/pencil.npy - 83\n",
            "processing file: data_processed/9/piano.npy - 84\n",
            "processing file: data_processed/9/pig.npy - 85\n",
            "processing file: data_processed/9/pizza.npy - 86\n",
            "processing file: data_processed/9/potato.npy - 87\n",
            "processing file: data_processed/9/rabbit.npy - 88\n",
            "processing file: data_processed/9/radio.npy - 89\n",
            "processing file: data_processed/9/rain.npy - 90\n",
            "processing file: data_processed/9/rainbow.npy - 91\n",
            "processing file: data_processed/9/river.npy - 92\n",
            "processing file: data_processed/9/sandwich.npy - 93\n",
            "processing file: data_processed/9/saw.npy - 94\n",
            "processing file: data_processed/9/sun.npy - 95\n",
            "processing file: data_processed/9/shark.npy - 96\n",
            "processing file: data_processed/9/shoe.npy - 97\n",
            "processing file: data_processed/9/skull.npy - 98\n",
            "processing file: data_processed/9/snail.npy - 99\n",
            "processing file: data_processed/9/snowflake.npy - 100\n",
            "processing file: data_processed/9/spider.npy - 101\n",
            "processing file: data_processed/9/square.npy - 102\n",
            "processing file: data_processed/9/stairs.npy - 103\n",
            "processing file: data_processed/9/star.npy - 104\n",
            "processing file: data_processed/9/stove.npy - 105\n",
            "processing file: data_processed/9/strawberry.npy - 106\n",
            "processing file: data_processed/9/table.npy - 107\n",
            "processing file: data_processed/9/teapot.npy - 108\n",
            "processing file: data_processed/9/television.npy - 109\n",
            "processing file: data_processed/9/tent.npy - 110\n",
            "processing file: data_processed/9/toilet.npy - 111\n",
            "processing file: data_processed/9/toothbrush.npy - 112\n",
            "processing file: data_processed/9/toothpaste.npy - 113\n",
            "processing file: data_processed/9/train.npy - 114\n",
            "processing file: data_processed/9/triangle.npy - 115\n",
            "processing file: data_processed/9/truck.npy - 116\n",
            "processing file: data_processed/9/umbrella.npy - 117\n",
            "processing file: data_processed/9/whale.npy - 118\n",
            "processing file: data_processed/9/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.9062 - accuracy: 0.7637 - val_loss: 0.7242 - val_accuracy: 0.8099\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8889 - accuracy: 0.7680 - val_loss: 0.7193 - val_accuracy: 0.8107\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8779 - accuracy: 0.7693 - val_loss: 0.7206 - val_accuracy: 0.8098\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8723 - accuracy: 0.7713 - val_loss: 0.7213 - val_accuracy: 0.8080\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8672 - accuracy: 0.7728 - val_loss: 0.7251 - val_accuracy: 0.8090\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8580 - accuracy: 0.7739 - val_loss: 0.7245 - val_accuracy: 0.8090\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8552 - accuracy: 0.7746 - val_loss: 0.7219 - val_accuracy: 0.8096\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8488 - accuracy: 0.7771 - val_loss: 0.7192 - val_accuracy: 0.8085\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8434 - accuracy: 0.7783 - val_loss: 0.7168 - val_accuracy: 0.8098\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8410 - accuracy: 0.7780 - val_loss: 0.7243 - val_accuracy: 0.8089\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7222 - accuracy: 0.8104\n",
            "Test accuarcy: 81.04%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      2000\n",
            "           1       0.90      0.94      0.92      2000\n",
            "           2       0.78      0.82      0.80      2000\n",
            "           3       0.84      0.81      0.82      2000\n",
            "           4       0.88      0.82      0.85      2000\n",
            "           5       0.85      0.85      0.85      2000\n",
            "           6       0.84      0.80      0.82      2000\n",
            "           7       0.72      0.65      0.68      2000\n",
            "           8       0.61      0.41      0.49      2000\n",
            "           9       0.83      0.79      0.81      2000\n",
            "          10       0.86      0.88      0.87      2000\n",
            "          11       0.80      0.89      0.84      2000\n",
            "          12       0.93      0.93      0.93      2000\n",
            "          13       0.87      0.86      0.86      2000\n",
            "          14       0.84      0.90      0.87      2000\n",
            "          15       0.79      0.75      0.77      2000\n",
            "          16       0.84      0.84      0.84      2000\n",
            "          17       0.88      0.90      0.89      2000\n",
            "          18       0.88      0.84      0.86      2000\n",
            "          19       0.78      0.76      0.77      2000\n",
            "          20       0.90      0.91      0.91      2000\n",
            "          21       0.80      0.73      0.77      2000\n",
            "          22       0.75      0.88      0.81      2000\n",
            "          23       0.93      0.88      0.90      2000\n",
            "          24       0.88      0.89      0.89      2000\n",
            "          25       0.82      0.68      0.74      2000\n",
            "          26       0.75      0.85      0.79      2000\n",
            "          27       0.66      0.77      0.71      2000\n",
            "          28       0.79      0.74      0.76      2000\n",
            "          29       0.60      0.66      0.63      2000\n",
            "          30       0.69      0.48      0.57      2000\n",
            "          31       0.73      0.66      0.70      2000\n",
            "          32       0.90      0.93      0.91      2000\n",
            "          33       0.90      0.87      0.89      2000\n",
            "          34       0.53      0.57      0.55      2000\n",
            "          35       0.89      0.90      0.90      2000\n",
            "          36       0.77      0.78      0.78      2000\n",
            "          37       0.69      0.73      0.71      2000\n",
            "          38       0.90      0.82      0.86      2000\n",
            "          39       0.79      0.77      0.78      2000\n",
            "          40       0.98      0.95      0.97      2000\n",
            "          41       0.70      0.67      0.69      2000\n",
            "          42       0.91      0.90      0.90      2000\n",
            "          43       0.85      0.84      0.85      2000\n",
            "          44       0.75      0.76      0.75      2000\n",
            "          45       0.83      0.76      0.79      2000\n",
            "          46       0.86      0.87      0.87      2000\n",
            "          47       0.85      0.87      0.86      2000\n",
            "          48       0.59      0.54      0.56      2000\n",
            "          49       0.55      0.75      0.64      2000\n",
            "          50       0.68      0.89      0.77      2000\n",
            "          51       0.78      0.85      0.81      2000\n",
            "          52       0.88      0.93      0.91      2000\n",
            "          53       0.85      0.83      0.84      2000\n",
            "          54       0.89      0.88      0.89      2000\n",
            "          55       0.82      0.84      0.83      2000\n",
            "          56       0.83      0.73      0.78      2000\n",
            "          57       0.80      0.78      0.79      2000\n",
            "          58       0.90      0.92      0.91      2000\n",
            "          59       0.77      0.82      0.79      2000\n",
            "          60       0.83      0.93      0.88      2000\n",
            "          61       0.82      0.81      0.82      2000\n",
            "          62       0.83      0.80      0.81      2000\n",
            "          63       0.73      0.88      0.80      2000\n",
            "          64       0.80      0.87      0.83      2000\n",
            "          65       0.72      0.82      0.76      2000\n",
            "          66       0.93      0.94      0.93      2000\n",
            "          67       0.79      0.82      0.81      2000\n",
            "          68       0.88      0.90      0.89      2000\n",
            "          69       0.62      0.74      0.68      2000\n",
            "          70       0.77      0.61      0.68      2000\n",
            "          71       0.87      0.90      0.88      2000\n",
            "          72       0.64      0.55      0.59      2000\n",
            "          73       0.68      0.81      0.74      2000\n",
            "          74       0.85      0.64      0.73      2000\n",
            "          75       0.86      0.78      0.82      2000\n",
            "          76       0.83      0.80      0.82      2000\n",
            "          77       0.75      0.81      0.78      2000\n",
            "          78       0.77      0.77      0.77      2000\n",
            "          79       0.91      0.84      0.87      2000\n",
            "          80       0.73      0.64      0.68      2000\n",
            "          81       0.68      0.77      0.72      2000\n",
            "          82       0.83      0.83      0.83      2000\n",
            "          83       0.72      0.67      0.69      2000\n",
            "          84       0.76      0.69      0.73      2000\n",
            "          85       0.72      0.73      0.73      2000\n",
            "          86       0.83      0.84      0.84      2000\n",
            "          87       0.72      0.65      0.68      2000\n",
            "          88       0.81      0.80      0.81      2000\n",
            "          89       0.88      0.86      0.87      2000\n",
            "          90       0.86      0.91      0.89      2000\n",
            "          91       0.93      0.96      0.94      2000\n",
            "          92       0.80      0.82      0.81      2000\n",
            "          93       0.77      0.81      0.79      2000\n",
            "          94       0.86      0.85      0.85      2000\n",
            "          95       0.91      0.91      0.91      2000\n",
            "          96       0.76      0.75      0.75      2000\n",
            "          97       0.85      0.83      0.84      2000\n",
            "          98       0.92      0.91      0.91      2000\n",
            "          99       0.90      0.92      0.91      2000\n",
            "         100       0.87      0.88      0.88      2000\n",
            "         101       0.84      0.80      0.82      2000\n",
            "         102       0.89      0.93      0.91      2000\n",
            "         103       0.96      0.92      0.94      2000\n",
            "         104       0.90      0.92      0.91      2000\n",
            "         105       0.81      0.80      0.80      2000\n",
            "         106       0.89      0.90      0.90      2000\n",
            "         107       0.87      0.90      0.89      2000\n",
            "         108       0.85      0.88      0.87      2000\n",
            "         109       0.92      0.87      0.89      2000\n",
            "         110       0.92      0.81      0.86      2000\n",
            "         111       0.84      0.88      0.86      2000\n",
            "         112       0.75      0.87      0.81      2000\n",
            "         113       0.71      0.64      0.67      2000\n",
            "         114       0.76      0.76      0.76      2000\n",
            "         115       0.89      0.93      0.91      2000\n",
            "         116       0.80      0.76      0.78      2000\n",
            "         117       0.94      0.93      0.94      2000\n",
            "         118       0.79      0.74      0.77      2000\n",
            "         119       0.85      0.82      0.83      2000\n",
            "\n",
            "    accuracy                           0.81    240000\n",
            "   macro avg       0.81      0.81      0.81    240000\n",
            "weighted avg       0.81      0.81      0.81    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.18956666666666666\n",
            "processing batch: 10\n",
            "processing file: data_processed/10/airplane.npy - 0\n",
            "processing file: data_processed/10/apple.npy - 1\n",
            "processing file: data_processed/10/ant.npy - 2\n",
            "processing file: data_processed/10/axe.npy - 3\n",
            "processing file: data_processed/10/banana.npy - 4\n",
            "processing file: data_processed/10/baseball.npy - 5\n",
            "processing file: data_processed/10/bee.npy - 6\n",
            "processing file: data_processed/10/beach.npy - 7\n",
            "processing file: data_processed/10/bird.npy - 8\n",
            "processing file: data_processed/10/book.npy - 9\n",
            "processing file: data_processed/10/bucket.npy - 10\n",
            "processing file: data_processed/10/bus.npy - 11\n",
            "processing file: data_processed/10/butterfly.npy - 12\n",
            "processing file: data_processed/10/cactus.npy - 13\n",
            "processing file: data_processed/10/cake.npy - 14\n",
            "processing file: data_processed/10/cannon.npy - 15\n",
            "processing file: data_processed/10/car.npy - 16\n",
            "processing file: data_processed/10/carrot.npy - 17\n",
            "processing file: data_processed/10/castle.npy - 18\n",
            "processing file: data_processed/10/cat.npy - 19\n",
            "processing file: data_processed/10/chair.npy - 20\n",
            "processing file: data_processed/10/church.npy - 21\n",
            "processing file: data_processed/10/circle.npy - 22\n",
            "processing file: data_processed/10/clock.npy - 23\n",
            "processing file: data_processed/10/cloud.npy - 24\n",
            "processing file: data_processed/10/computer.npy - 25\n",
            "processing file: data_processed/10/cookie.npy - 26\n",
            "processing file: data_processed/10/cow.npy - 27\n",
            "processing file: data_processed/10/crab.npy - 28\n",
            "processing file: data_processed/10/crayon.npy - 29\n",
            "processing file: data_processed/10/cup.npy - 30\n",
            "processing file: data_processed/10/dolphin.npy - 31\n",
            "processing file: data_processed/10/donut.npy - 32\n",
            "processing file: data_processed/10/door.npy - 33\n",
            "processing file: data_processed/10/dragon.npy - 34\n",
            "processing file: data_processed/10/diamond.npy - 35\n",
            "processing file: data_processed/10/drums.npy - 36\n",
            "processing file: data_processed/10/duck.npy - 37\n",
            "processing file: data_processed/10/ear.npy - 38\n",
            "processing file: data_processed/10/elephant.npy - 39\n",
            "processing file: data_processed/10/envelope.npy - 40\n",
            "processing file: data_processed/10/eraser.npy - 41\n",
            "processing file: data_processed/10/eye.npy - 42\n",
            "processing file: data_processed/10/face.npy - 43\n",
            "processing file: data_processed/10/fan.npy - 44\n",
            "processing file: data_processed/10/finger.npy - 45\n",
            "processing file: data_processed/10/fish.npy - 46\n",
            "processing file: data_processed/10/flower.npy - 47\n",
            "processing file: data_processed/10/frog.npy - 48\n",
            "processing file: data_processed/10/garden.npy - 49\n",
            "processing file: data_processed/10/grapes.npy - 50\n",
            "processing file: data_processed/10/grass.npy - 51\n",
            "processing file: data_processed/10/guitar.npy - 52\n",
            "processing file: data_processed/10/hammer.npy - 53\n",
            "processing file: data_processed/10/hand.npy - 54\n",
            "processing file: data_processed/10/hat.npy - 55\n",
            "processing file: data_processed/10/helmet.npy - 56\n",
            "processing file: data_processed/10/horse.npy - 57\n",
            "processing file: data_processed/10/house.npy - 58\n",
            "processing file: data_processed/10/hospital.npy - 59\n",
            "processing file: data_processed/10/jail.npy - 60\n",
            "processing file: data_processed/10/kangaroo.npy - 61\n",
            "processing file: data_processed/10/knife.npy - 62\n",
            "processing file: data_processed/10/laptop.npy - 63\n",
            "processing file: data_processed/10/leg.npy - 64\n",
            "processing file: data_processed/10/lion.npy - 65\n",
            "processing file: data_processed/10/lollipop.npy - 66\n",
            "processing file: data_processed/10/map.npy - 67\n",
            "processing file: data_processed/10/microwave.npy - 68\n",
            "processing file: data_processed/10/monkey.npy - 69\n",
            "processing file: data_processed/10/moon.npy - 70\n",
            "processing file: data_processed/10/mountain.npy - 71\n",
            "processing file: data_processed/10/mouse.npy - 72\n",
            "processing file: data_processed/10/mug.npy - 73\n",
            "processing file: data_processed/10/nail.npy - 74\n",
            "processing file: data_processed/10/nose.npy - 75\n",
            "processing file: data_processed/10/onion.npy - 76\n",
            "processing file: data_processed/10/owl.npy - 77\n",
            "processing file: data_processed/10/panda.npy - 78\n",
            "processing file: data_processed/10/parachute.npy - 79\n",
            "processing file: data_processed/10/parrot.npy - 80\n",
            "processing file: data_processed/10/passport.npy - 81\n",
            "processing file: data_processed/10/peanut.npy - 82\n",
            "processing file: data_processed/10/pencil.npy - 83\n",
            "processing file: data_processed/10/piano.npy - 84\n",
            "processing file: data_processed/10/pig.npy - 85\n",
            "processing file: data_processed/10/pizza.npy - 86\n",
            "processing file: data_processed/10/potato.npy - 87\n",
            "processing file: data_processed/10/rabbit.npy - 88\n",
            "processing file: data_processed/10/radio.npy - 89\n",
            "processing file: data_processed/10/rain.npy - 90\n",
            "processing file: data_processed/10/rainbow.npy - 91\n",
            "processing file: data_processed/10/river.npy - 92\n",
            "processing file: data_processed/10/sandwich.npy - 93\n",
            "processing file: data_processed/10/saw.npy - 94\n",
            "processing file: data_processed/10/sun.npy - 95\n",
            "processing file: data_processed/10/shark.npy - 96\n",
            "processing file: data_processed/10/shoe.npy - 97\n",
            "processing file: data_processed/10/skull.npy - 98\n",
            "processing file: data_processed/10/snail.npy - 99\n",
            "processing file: data_processed/10/snowflake.npy - 100\n",
            "processing file: data_processed/10/spider.npy - 101\n",
            "processing file: data_processed/10/square.npy - 102\n",
            "processing file: data_processed/10/stairs.npy - 103\n",
            "processing file: data_processed/10/star.npy - 104\n",
            "processing file: data_processed/10/stove.npy - 105\n",
            "processing file: data_processed/10/strawberry.npy - 106\n",
            "processing file: data_processed/10/table.npy - 107\n",
            "processing file: data_processed/10/teapot.npy - 108\n",
            "processing file: data_processed/10/television.npy - 109\n",
            "processing file: data_processed/10/tent.npy - 110\n",
            "processing file: data_processed/10/toilet.npy - 111\n",
            "processing file: data_processed/10/toothbrush.npy - 112\n",
            "processing file: data_processed/10/toothpaste.npy - 113\n",
            "processing file: data_processed/10/train.npy - 114\n",
            "processing file: data_processed/10/triangle.npy - 115\n",
            "processing file: data_processed/10/truck.npy - 116\n",
            "processing file: data_processed/10/umbrella.npy - 117\n",
            "processing file: data_processed/10/whale.npy - 118\n",
            "processing file: data_processed/10/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8985 - accuracy: 0.7676 - val_loss: 0.7208 - val_accuracy: 0.8113\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8823 - accuracy: 0.7709 - val_loss: 0.7166 - val_accuracy: 0.8120\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8730 - accuracy: 0.7719 - val_loss: 0.7216 - val_accuracy: 0.8120\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8663 - accuracy: 0.7741 - val_loss: 0.7170 - val_accuracy: 0.8121\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8606 - accuracy: 0.7747 - val_loss: 0.7172 - val_accuracy: 0.8123\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8554 - accuracy: 0.7754 - val_loss: 0.7228 - val_accuracy: 0.8131\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8491 - accuracy: 0.7764 - val_loss: 0.7233 - val_accuracy: 0.8102\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8440 - accuracy: 0.7785 - val_loss: 0.7204 - val_accuracy: 0.8118\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8397 - accuracy: 0.7791 - val_loss: 0.7169 - val_accuracy: 0.8111\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8349 - accuracy: 0.7806 - val_loss: 0.7174 - val_accuracy: 0.8127\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7130 - accuracy: 0.8128\n",
            "Test accuarcy: 81.28%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84      2000\n",
            "           1       0.92      0.93      0.92      2000\n",
            "           2       0.73      0.84      0.78      2000\n",
            "           3       0.85      0.80      0.83      2000\n",
            "           4       0.89      0.82      0.85      2000\n",
            "           5       0.88      0.85      0.86      2000\n",
            "           6       0.79      0.83      0.81      2000\n",
            "           7       0.68      0.73      0.70      2000\n",
            "           8       0.58      0.48      0.52      2000\n",
            "           9       0.78      0.85      0.81      2000\n",
            "          10       0.87      0.87      0.87      2000\n",
            "          11       0.83      0.88      0.85      2000\n",
            "          12       0.94      0.93      0.93      2000\n",
            "          13       0.87      0.86      0.87      2000\n",
            "          14       0.85      0.90      0.87      2000\n",
            "          15       0.79      0.77      0.78      2000\n",
            "          16       0.87      0.82      0.84      2000\n",
            "          17       0.88      0.90      0.89      2000\n",
            "          18       0.87      0.86      0.87      2000\n",
            "          19       0.76      0.76      0.76      2000\n",
            "          20       0.92      0.91      0.91      2000\n",
            "          21       0.82      0.73      0.77      2000\n",
            "          22       0.77      0.87      0.81      2000\n",
            "          23       0.91      0.89      0.90      2000\n",
            "          24       0.87      0.91      0.89      2000\n",
            "          25       0.83      0.66      0.73      2000\n",
            "          26       0.76      0.83      0.79      2000\n",
            "          27       0.64      0.77      0.70      2000\n",
            "          28       0.76      0.77      0.77      2000\n",
            "          29       0.66      0.58      0.62      2000\n",
            "          30       0.70      0.49      0.58      2000\n",
            "          31       0.73      0.67      0.70      2000\n",
            "          32       0.91      0.92      0.92      2000\n",
            "          33       0.91      0.89      0.90      2000\n",
            "          34       0.50      0.62      0.55      2000\n",
            "          35       0.90      0.90      0.90      2000\n",
            "          36       0.78      0.79      0.78      2000\n",
            "          37       0.75      0.68      0.71      2000\n",
            "          38       0.90      0.83      0.86      2000\n",
            "          39       0.80      0.76      0.78      2000\n",
            "          40       0.98      0.95      0.97      2000\n",
            "          41       0.70      0.66      0.68      2000\n",
            "          42       0.90      0.91      0.91      2000\n",
            "          43       0.86      0.85      0.85      2000\n",
            "          44       0.78      0.70      0.74      2000\n",
            "          45       0.83      0.77      0.80      2000\n",
            "          46       0.87      0.87      0.87      2000\n",
            "          47       0.80      0.90      0.85      2000\n",
            "          48       0.61      0.53      0.57      2000\n",
            "          49       0.57      0.74      0.64      2000\n",
            "          50       0.67      0.89      0.76      2000\n",
            "          51       0.83      0.83      0.83      2000\n",
            "          52       0.90      0.92      0.91      2000\n",
            "          53       0.85      0.83      0.84      2000\n",
            "          54       0.89      0.89      0.89      2000\n",
            "          55       0.86      0.83      0.84      2000\n",
            "          56       0.83      0.74      0.79      2000\n",
            "          57       0.80      0.79      0.80      2000\n",
            "          58       0.92      0.91      0.91      2000\n",
            "          59       0.78      0.82      0.80      2000\n",
            "          60       0.87      0.90      0.89      2000\n",
            "          61       0.81      0.81      0.81      2000\n",
            "          62       0.89      0.76      0.82      2000\n",
            "          63       0.71      0.90      0.79      2000\n",
            "          64       0.81      0.86      0.84      2000\n",
            "          65       0.68      0.83      0.75      2000\n",
            "          66       0.94      0.93      0.94      2000\n",
            "          67       0.79      0.82      0.81      2000\n",
            "          68       0.89      0.90      0.89      2000\n",
            "          69       0.64      0.72      0.68      2000\n",
            "          70       0.77      0.62      0.68      2000\n",
            "          71       0.87      0.90      0.88      2000\n",
            "          72       0.63      0.57      0.60      2000\n",
            "          73       0.69      0.82      0.75      2000\n",
            "          74       0.87      0.64      0.74      2000\n",
            "          75       0.87      0.79      0.83      2000\n",
            "          76       0.85      0.81      0.83      2000\n",
            "          77       0.75      0.81      0.78      2000\n",
            "          78       0.76      0.78      0.77      2000\n",
            "          79       0.88      0.86      0.87      2000\n",
            "          80       0.72      0.66      0.69      2000\n",
            "          81       0.74      0.71      0.72      2000\n",
            "          82       0.82      0.83      0.83      2000\n",
            "          83       0.66      0.74      0.70      2000\n",
            "          84       0.80      0.69      0.74      2000\n",
            "          85       0.73      0.74      0.73      2000\n",
            "          86       0.74      0.89      0.81      2000\n",
            "          87       0.71      0.67      0.69      2000\n",
            "          88       0.81      0.79      0.80      2000\n",
            "          89       0.89      0.85      0.87      2000\n",
            "          90       0.87      0.92      0.89      2000\n",
            "          91       0.93      0.96      0.95      2000\n",
            "          92       0.79      0.83      0.81      2000\n",
            "          93       0.78      0.80      0.79      2000\n",
            "          94       0.87      0.86      0.86      2000\n",
            "          95       0.92      0.89      0.90      2000\n",
            "          96       0.76      0.75      0.75      2000\n",
            "          97       0.87      0.82      0.85      2000\n",
            "          98       0.92      0.91      0.91      2000\n",
            "          99       0.91      0.92      0.91      2000\n",
            "         100       0.86      0.90      0.88      2000\n",
            "         101       0.83      0.81      0.82      2000\n",
            "         102       0.90      0.91      0.91      2000\n",
            "         103       0.97      0.91      0.94      2000\n",
            "         104       0.93      0.91      0.92      2000\n",
            "         105       0.80      0.80      0.80      2000\n",
            "         106       0.88      0.91      0.90      2000\n",
            "         107       0.89      0.89      0.89      2000\n",
            "         108       0.86      0.87      0.87      2000\n",
            "         109       0.90      0.89      0.89      2000\n",
            "         110       0.93      0.82      0.87      2000\n",
            "         111       0.89      0.86      0.87      2000\n",
            "         112       0.81      0.85      0.83      2000\n",
            "         113       0.69      0.65      0.67      2000\n",
            "         114       0.69      0.82      0.75      2000\n",
            "         115       0.90      0.92      0.91      2000\n",
            "         116       0.82      0.77      0.79      2000\n",
            "         117       0.94      0.94      0.94      2000\n",
            "         118       0.79      0.75      0.77      2000\n",
            "         119       0.85      0.81      0.83      2000\n",
            "\n",
            "    accuracy                           0.81    240000\n",
            "   macro avg       0.82      0.81      0.81    240000\n",
            "weighted avg       0.82      0.81      0.81    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.18716666666666668\n",
            "processing batch: 11\n",
            "processing file: data_processed/11/airplane.npy - 0\n",
            "processing file: data_processed/11/apple.npy - 1\n",
            "processing file: data_processed/11/ant.npy - 2\n",
            "processing file: data_processed/11/axe.npy - 3\n",
            "processing file: data_processed/11/banana.npy - 4\n",
            "processing file: data_processed/11/baseball.npy - 5\n",
            "processing file: data_processed/11/bee.npy - 6\n",
            "processing file: data_processed/11/beach.npy - 7\n",
            "processing file: data_processed/11/bird.npy - 8\n",
            "processing file: data_processed/11/book.npy - 9\n",
            "processing file: data_processed/11/bucket.npy - 10\n",
            "processing file: data_processed/11/bus.npy - 11\n",
            "processing file: data_processed/11/butterfly.npy - 12\n",
            "processing file: data_processed/11/cactus.npy - 13\n",
            "processing file: data_processed/11/cake.npy - 14\n",
            "processing file: data_processed/11/cannon.npy - 15\n",
            "processing file: data_processed/11/car.npy - 16\n",
            "processing file: data_processed/11/carrot.npy - 17\n",
            "processing file: data_processed/11/castle.npy - 18\n",
            "processing file: data_processed/11/cat.npy - 19\n",
            "processing file: data_processed/11/chair.npy - 20\n",
            "processing file: data_processed/11/church.npy - 21\n",
            "processing file: data_processed/11/circle.npy - 22\n",
            "processing file: data_processed/11/clock.npy - 23\n",
            "processing file: data_processed/11/cloud.npy - 24\n",
            "processing file: data_processed/11/computer.npy - 25\n",
            "processing file: data_processed/11/cookie.npy - 26\n",
            "processing file: data_processed/11/cow.npy - 27\n",
            "processing file: data_processed/11/crab.npy - 28\n",
            "processing file: data_processed/11/crayon.npy - 29\n",
            "processing file: data_processed/11/cup.npy - 30\n",
            "processing file: data_processed/11/dolphin.npy - 31\n",
            "processing file: data_processed/11/donut.npy - 32\n",
            "processing file: data_processed/11/door.npy - 33\n",
            "processing file: data_processed/11/dragon.npy - 34\n",
            "processing file: data_processed/11/diamond.npy - 35\n",
            "processing file: data_processed/11/drums.npy - 36\n",
            "processing file: data_processed/11/duck.npy - 37\n",
            "processing file: data_processed/11/ear.npy - 38\n",
            "processing file: data_processed/11/elephant.npy - 39\n",
            "processing file: data_processed/11/envelope.npy - 40\n",
            "processing file: data_processed/11/eraser.npy - 41\n",
            "processing file: data_processed/11/eye.npy - 42\n",
            "processing file: data_processed/11/face.npy - 43\n",
            "processing file: data_processed/11/fan.npy - 44\n",
            "processing file: data_processed/11/finger.npy - 45\n",
            "processing file: data_processed/11/fish.npy - 46\n",
            "processing file: data_processed/11/flower.npy - 47\n",
            "processing file: data_processed/11/frog.npy - 48\n",
            "processing file: data_processed/11/garden.npy - 49\n",
            "processing file: data_processed/11/grapes.npy - 50\n",
            "processing file: data_processed/11/grass.npy - 51\n",
            "processing file: data_processed/11/guitar.npy - 52\n",
            "processing file: data_processed/11/hammer.npy - 53\n",
            "processing file: data_processed/11/hand.npy - 54\n",
            "processing file: data_processed/11/hat.npy - 55\n",
            "processing file: data_processed/11/helmet.npy - 56\n",
            "processing file: data_processed/11/horse.npy - 57\n",
            "processing file: data_processed/11/house.npy - 58\n",
            "processing file: data_processed/11/hospital.npy - 59\n",
            "processing file: data_processed/11/jail.npy - 60\n",
            "processing file: data_processed/11/kangaroo.npy - 61\n",
            "processing file: data_processed/11/knife.npy - 62\n",
            "processing file: data_processed/11/laptop.npy - 63\n",
            "processing file: data_processed/11/leg.npy - 64\n",
            "processing file: data_processed/11/lion.npy - 65\n",
            "processing file: data_processed/11/lollipop.npy - 66\n",
            "processing file: data_processed/11/map.npy - 67\n",
            "processing file: data_processed/11/microwave.npy - 68\n",
            "processing file: data_processed/11/monkey.npy - 69\n",
            "processing file: data_processed/11/moon.npy - 70\n",
            "processing file: data_processed/11/mountain.npy - 71\n",
            "processing file: data_processed/11/mouse.npy - 72\n",
            "processing file: data_processed/11/mug.npy - 73\n",
            "processing file: data_processed/11/nail.npy - 74\n",
            "processing file: data_processed/11/nose.npy - 75\n",
            "processing file: data_processed/11/onion.npy - 76\n",
            "processing file: data_processed/11/owl.npy - 77\n",
            "processing file: data_processed/11/panda.npy - 78\n",
            "processing file: data_processed/11/parachute.npy - 79\n",
            "processing file: data_processed/11/parrot.npy - 80\n",
            "processing file: data_processed/11/passport.npy - 81\n",
            "processing file: data_processed/11/peanut.npy - 82\n",
            "processing file: data_processed/11/pencil.npy - 83\n",
            "processing file: data_processed/11/piano.npy - 84\n",
            "processing file: data_processed/11/pig.npy - 85\n",
            "processing file: data_processed/11/pizza.npy - 86\n",
            "processing file: data_processed/11/potato.npy - 87\n",
            "processing file: data_processed/11/rabbit.npy - 88\n",
            "processing file: data_processed/11/radio.npy - 89\n",
            "processing file: data_processed/11/rain.npy - 90\n",
            "processing file: data_processed/11/rainbow.npy - 91\n",
            "processing file: data_processed/11/river.npy - 92\n",
            "processing file: data_processed/11/sandwich.npy - 93\n",
            "processing file: data_processed/11/saw.npy - 94\n",
            "processing file: data_processed/11/sun.npy - 95\n",
            "processing file: data_processed/11/shark.npy - 96\n",
            "processing file: data_processed/11/shoe.npy - 97\n",
            "processing file: data_processed/11/skull.npy - 98\n",
            "processing file: data_processed/11/snail.npy - 99\n",
            "processing file: data_processed/11/snowflake.npy - 100\n",
            "processing file: data_processed/11/spider.npy - 101\n",
            "processing file: data_processed/11/square.npy - 102\n",
            "processing file: data_processed/11/stairs.npy - 103\n",
            "processing file: data_processed/11/star.npy - 104\n",
            "processing file: data_processed/11/stove.npy - 105\n",
            "processing file: data_processed/11/strawberry.npy - 106\n",
            "processing file: data_processed/11/table.npy - 107\n",
            "processing file: data_processed/11/teapot.npy - 108\n",
            "processing file: data_processed/11/television.npy - 109\n",
            "processing file: data_processed/11/tent.npy - 110\n",
            "processing file: data_processed/11/toilet.npy - 111\n",
            "processing file: data_processed/11/toothbrush.npy - 112\n",
            "processing file: data_processed/11/toothpaste.npy - 113\n",
            "processing file: data_processed/11/train.npy - 114\n",
            "processing file: data_processed/11/triangle.npy - 115\n",
            "processing file: data_processed/11/truck.npy - 116\n",
            "processing file: data_processed/11/umbrella.npy - 117\n",
            "processing file: data_processed/11/whale.npy - 118\n",
            "processing file: data_processed/11/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8927 - accuracy: 0.7686 - val_loss: 0.7133 - val_accuracy: 0.8142\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8769 - accuracy: 0.7711 - val_loss: 0.7186 - val_accuracy: 0.8149\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8672 - accuracy: 0.7733 - val_loss: 0.7157 - val_accuracy: 0.8149\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8610 - accuracy: 0.7750 - val_loss: 0.7140 - val_accuracy: 0.8162\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8513 - accuracy: 0.7765 - val_loss: 0.7139 - val_accuracy: 0.8148\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8473 - accuracy: 0.7784 - val_loss: 0.7156 - val_accuracy: 0.8145\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8436 - accuracy: 0.7788 - val_loss: 0.7228 - val_accuracy: 0.8133\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8395 - accuracy: 0.7807 - val_loss: 0.7148 - val_accuracy: 0.8148\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8323 - accuracy: 0.7816 - val_loss: 0.7162 - val_accuracy: 0.8140\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8288 - accuracy: 0.7814 - val_loss: 0.7143 - val_accuracy: 0.8152\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7061 - accuracy: 0.8145\n",
            "Test accuarcy: 81.45%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84      2000\n",
            "           1       0.90      0.94      0.92      2000\n",
            "           2       0.76      0.84      0.80      2000\n",
            "           3       0.83      0.81      0.82      2000\n",
            "           4       0.86      0.84      0.85      2000\n",
            "           5       0.86      0.85      0.86      2000\n",
            "           6       0.80      0.82      0.81      2000\n",
            "           7       0.74      0.70      0.72      2000\n",
            "           8       0.58      0.47      0.52      2000\n",
            "           9       0.79      0.84      0.82      2000\n",
            "          10       0.86      0.88      0.87      2000\n",
            "          11       0.80      0.89      0.84      2000\n",
            "          12       0.93      0.93      0.93      2000\n",
            "          13       0.89      0.86      0.87      2000\n",
            "          14       0.84      0.90      0.87      2000\n",
            "          15       0.79      0.77      0.78      2000\n",
            "          16       0.84      0.84      0.84      2000\n",
            "          17       0.88      0.90      0.89      2000\n",
            "          18       0.87      0.86      0.86      2000\n",
            "          19       0.80      0.76      0.78      2000\n",
            "          20       0.92      0.91      0.91      2000\n",
            "          21       0.81      0.74      0.77      2000\n",
            "          22       0.74      0.89      0.81      2000\n",
            "          23       0.90      0.90      0.90      2000\n",
            "          24       0.86      0.91      0.89      2000\n",
            "          25       0.83      0.67      0.74      2000\n",
            "          26       0.73      0.85      0.79      2000\n",
            "          27       0.74      0.71      0.72      2000\n",
            "          28       0.76      0.77      0.77      2000\n",
            "          29       0.68      0.55      0.61      2000\n",
            "          30       0.66      0.53      0.59      2000\n",
            "          31       0.74      0.66      0.70      2000\n",
            "          32       0.90      0.93      0.91      2000\n",
            "          33       0.92      0.86      0.89      2000\n",
            "          34       0.52      0.60      0.56      2000\n",
            "          35       0.88      0.91      0.89      2000\n",
            "          36       0.75      0.82      0.78      2000\n",
            "          37       0.73      0.70      0.71      2000\n",
            "          38       0.91      0.82      0.86      2000\n",
            "          39       0.80      0.77      0.78      2000\n",
            "          40       0.98      0.95      0.97      2000\n",
            "          41       0.70      0.69      0.69      2000\n",
            "          42       0.90      0.91      0.91      2000\n",
            "          43       0.85      0.86      0.85      2000\n",
            "          44       0.79      0.73      0.76      2000\n",
            "          45       0.84      0.76      0.80      2000\n",
            "          46       0.85      0.88      0.86      2000\n",
            "          47       0.80      0.90      0.85      2000\n",
            "          48       0.60      0.54      0.56      2000\n",
            "          49       0.60      0.73      0.66      2000\n",
            "          50       0.64      0.89      0.75      2000\n",
            "          51       0.76      0.88      0.81      2000\n",
            "          52       0.91      0.92      0.92      2000\n",
            "          53       0.85      0.82      0.84      2000\n",
            "          54       0.88      0.89      0.88      2000\n",
            "          55       0.85      0.83      0.84      2000\n",
            "          56       0.82      0.75      0.79      2000\n",
            "          57       0.79      0.79      0.79      2000\n",
            "          58       0.91      0.92      0.91      2000\n",
            "          59       0.77      0.83      0.80      2000\n",
            "          60       0.88      0.90      0.89      2000\n",
            "          61       0.79      0.82      0.81      2000\n",
            "          62       0.84      0.79      0.82      2000\n",
            "          63       0.73      0.88      0.80      2000\n",
            "          64       0.82      0.86      0.84      2000\n",
            "          65       0.73      0.82      0.77      2000\n",
            "          66       0.93      0.94      0.93      2000\n",
            "          67       0.80      0.83      0.81      2000\n",
            "          68       0.88      0.91      0.89      2000\n",
            "          69       0.66      0.73      0.69      2000\n",
            "          70       0.79      0.60      0.68      2000\n",
            "          71       0.88      0.89      0.89      2000\n",
            "          72       0.65      0.56      0.60      2000\n",
            "          73       0.71      0.77      0.74      2000\n",
            "          74       0.83      0.66      0.74      2000\n",
            "          75       0.87      0.78      0.82      2000\n",
            "          76       0.87      0.80      0.83      2000\n",
            "          77       0.76      0.82      0.79      2000\n",
            "          78       0.81      0.76      0.78      2000\n",
            "          79       0.88      0.86      0.87      2000\n",
            "          80       0.69      0.68      0.69      2000\n",
            "          81       0.81      0.67      0.73      2000\n",
            "          82       0.84      0.83      0.83      2000\n",
            "          83       0.64      0.78      0.70      2000\n",
            "          84       0.79      0.70      0.74      2000\n",
            "          85       0.70      0.75      0.72      2000\n",
            "          86       0.78      0.88      0.82      2000\n",
            "          87       0.71      0.64      0.67      2000\n",
            "          88       0.83      0.79      0.81      2000\n",
            "          89       0.92      0.83      0.87      2000\n",
            "          90       0.87      0.91      0.89      2000\n",
            "          91       0.93      0.96      0.94      2000\n",
            "          92       0.83      0.82      0.83      2000\n",
            "          93       0.79      0.80      0.79      2000\n",
            "          94       0.90      0.84      0.87      2000\n",
            "          95       0.92      0.90      0.91      2000\n",
            "          96       0.74      0.79      0.76      2000\n",
            "          97       0.87      0.83      0.85      2000\n",
            "          98       0.93      0.90      0.91      2000\n",
            "          99       0.92      0.91      0.92      2000\n",
            "         100       0.87      0.89      0.88      2000\n",
            "         101       0.82      0.82      0.82      2000\n",
            "         102       0.88      0.93      0.91      2000\n",
            "         103       0.97      0.92      0.94      2000\n",
            "         104       0.89      0.92      0.91      2000\n",
            "         105       0.82      0.80      0.81      2000\n",
            "         106       0.91      0.90      0.90      2000\n",
            "         107       0.88      0.90      0.89      2000\n",
            "         108       0.86      0.87      0.87      2000\n",
            "         109       0.89      0.90      0.90      2000\n",
            "         110       0.92      0.82      0.87      2000\n",
            "         111       0.88      0.87      0.87      2000\n",
            "         112       0.82      0.85      0.84      2000\n",
            "         113       0.72      0.64      0.68      2000\n",
            "         114       0.72      0.80      0.76      2000\n",
            "         115       0.87      0.94      0.91      2000\n",
            "         116       0.83      0.75      0.79      2000\n",
            "         117       0.94      0.94      0.94      2000\n",
            "         118       0.82      0.73      0.77      2000\n",
            "         119       0.82      0.84      0.83      2000\n",
            "\n",
            "    accuracy                           0.81    240000\n",
            "   macro avg       0.82      0.81      0.81    240000\n",
            "weighted avg       0.82      0.81      0.81    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.18554166666666666\n",
            "processing batch: 12\n",
            "processing file: data_processed/12/airplane.npy - 0\n",
            "processing file: data_processed/12/apple.npy - 1\n",
            "processing file: data_processed/12/ant.npy - 2\n",
            "processing file: data_processed/12/axe.npy - 3\n",
            "processing file: data_processed/12/banana.npy - 4\n",
            "processing file: data_processed/12/baseball.npy - 5\n",
            "processing file: data_processed/12/bee.npy - 6\n",
            "processing file: data_processed/12/beach.npy - 7\n",
            "processing file: data_processed/12/bird.npy - 8\n",
            "processing file: data_processed/12/book.npy - 9\n",
            "processing file: data_processed/12/bucket.npy - 10\n",
            "processing file: data_processed/12/bus.npy - 11\n",
            "processing file: data_processed/12/butterfly.npy - 12\n",
            "processing file: data_processed/12/cactus.npy - 13\n",
            "processing file: data_processed/12/cake.npy - 14\n",
            "processing file: data_processed/12/cannon.npy - 15\n",
            "processing file: data_processed/12/car.npy - 16\n",
            "processing file: data_processed/12/carrot.npy - 17\n",
            "processing file: data_processed/12/castle.npy - 18\n",
            "processing file: data_processed/12/cat.npy - 19\n",
            "processing file: data_processed/12/chair.npy - 20\n",
            "processing file: data_processed/12/church.npy - 21\n",
            "processing file: data_processed/12/circle.npy - 22\n",
            "processing file: data_processed/12/clock.npy - 23\n",
            "processing file: data_processed/12/cloud.npy - 24\n",
            "processing file: data_processed/12/computer.npy - 25\n",
            "processing file: data_processed/12/cookie.npy - 26\n",
            "processing file: data_processed/12/cow.npy - 27\n",
            "processing file: data_processed/12/crab.npy - 28\n",
            "processing file: data_processed/12/crayon.npy - 29\n",
            "processing file: data_processed/12/cup.npy - 30\n",
            "processing file: data_processed/12/dolphin.npy - 31\n",
            "processing file: data_processed/12/donut.npy - 32\n",
            "processing file: data_processed/12/door.npy - 33\n",
            "processing file: data_processed/12/dragon.npy - 34\n",
            "processing file: data_processed/12/diamond.npy - 35\n",
            "processing file: data_processed/12/drums.npy - 36\n",
            "processing file: data_processed/12/duck.npy - 37\n",
            "processing file: data_processed/12/ear.npy - 38\n",
            "processing file: data_processed/12/elephant.npy - 39\n",
            "processing file: data_processed/12/envelope.npy - 40\n",
            "processing file: data_processed/12/eraser.npy - 41\n",
            "processing file: data_processed/12/eye.npy - 42\n",
            "processing file: data_processed/12/face.npy - 43\n",
            "processing file: data_processed/12/fan.npy - 44\n",
            "processing file: data_processed/12/finger.npy - 45\n",
            "processing file: data_processed/12/fish.npy - 46\n",
            "processing file: data_processed/12/flower.npy - 47\n",
            "processing file: data_processed/12/frog.npy - 48\n",
            "processing file: data_processed/12/garden.npy - 49\n",
            "processing file: data_processed/12/grapes.npy - 50\n",
            "processing file: data_processed/12/grass.npy - 51\n",
            "processing file: data_processed/12/guitar.npy - 52\n",
            "processing file: data_processed/12/hammer.npy - 53\n",
            "processing file: data_processed/12/hand.npy - 54\n",
            "processing file: data_processed/12/hat.npy - 55\n",
            "processing file: data_processed/12/helmet.npy - 56\n",
            "processing file: data_processed/12/horse.npy - 57\n",
            "processing file: data_processed/12/house.npy - 58\n",
            "processing file: data_processed/12/hospital.npy - 59\n",
            "processing file: data_processed/12/jail.npy - 60\n",
            "processing file: data_processed/12/kangaroo.npy - 61\n",
            "processing file: data_processed/12/knife.npy - 62\n",
            "processing file: data_processed/12/laptop.npy - 63\n",
            "processing file: data_processed/12/leg.npy - 64\n",
            "processing file: data_processed/12/lion.npy - 65\n",
            "processing file: data_processed/12/lollipop.npy - 66\n",
            "processing file: data_processed/12/map.npy - 67\n",
            "processing file: data_processed/12/microwave.npy - 68\n",
            "processing file: data_processed/12/monkey.npy - 69\n",
            "processing file: data_processed/12/moon.npy - 70\n",
            "processing file: data_processed/12/mountain.npy - 71\n",
            "processing file: data_processed/12/mouse.npy - 72\n",
            "processing file: data_processed/12/mug.npy - 73\n",
            "processing file: data_processed/12/nail.npy - 74\n",
            "processing file: data_processed/12/nose.npy - 75\n",
            "processing file: data_processed/12/onion.npy - 76\n",
            "processing file: data_processed/12/owl.npy - 77\n",
            "processing file: data_processed/12/panda.npy - 78\n",
            "processing file: data_processed/12/parachute.npy - 79\n",
            "processing file: data_processed/12/parrot.npy - 80\n",
            "processing file: data_processed/12/passport.npy - 81\n",
            "processing file: data_processed/12/peanut.npy - 82\n",
            "processing file: data_processed/12/pencil.npy - 83\n",
            "processing file: data_processed/12/piano.npy - 84\n",
            "processing file: data_processed/12/pig.npy - 85\n",
            "processing file: data_processed/12/pizza.npy - 86\n",
            "processing file: data_processed/12/potato.npy - 87\n",
            "processing file: data_processed/12/rabbit.npy - 88\n",
            "processing file: data_processed/12/radio.npy - 89\n",
            "processing file: data_processed/12/rain.npy - 90\n",
            "processing file: data_processed/12/rainbow.npy - 91\n",
            "processing file: data_processed/12/river.npy - 92\n",
            "processing file: data_processed/12/sandwich.npy - 93\n",
            "processing file: data_processed/12/saw.npy - 94\n",
            "processing file: data_processed/12/sun.npy - 95\n",
            "processing file: data_processed/12/shark.npy - 96\n",
            "processing file: data_processed/12/shoe.npy - 97\n",
            "processing file: data_processed/12/skull.npy - 98\n",
            "processing file: data_processed/12/snail.npy - 99\n",
            "processing file: data_processed/12/snowflake.npy - 100\n",
            "processing file: data_processed/12/spider.npy - 101\n",
            "processing file: data_processed/12/square.npy - 102\n",
            "processing file: data_processed/12/stairs.npy - 103\n",
            "processing file: data_processed/12/star.npy - 104\n",
            "processing file: data_processed/12/stove.npy - 105\n",
            "processing file: data_processed/12/strawberry.npy - 106\n",
            "processing file: data_processed/12/table.npy - 107\n",
            "processing file: data_processed/12/teapot.npy - 108\n",
            "processing file: data_processed/12/television.npy - 109\n",
            "processing file: data_processed/12/tent.npy - 110\n",
            "processing file: data_processed/12/toilet.npy - 111\n",
            "processing file: data_processed/12/toothbrush.npy - 112\n",
            "processing file: data_processed/12/toothpaste.npy - 113\n",
            "processing file: data_processed/12/train.npy - 114\n",
            "processing file: data_processed/12/triangle.npy - 115\n",
            "processing file: data_processed/12/truck.npy - 116\n",
            "processing file: data_processed/12/umbrella.npy - 117\n",
            "processing file: data_processed/12/whale.npy - 118\n",
            "processing file: data_processed/12/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8872 - accuracy: 0.7698 - val_loss: 0.6942 - val_accuracy: 0.8190\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8708 - accuracy: 0.7731 - val_loss: 0.6921 - val_accuracy: 0.8197\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8605 - accuracy: 0.7760 - val_loss: 0.6931 - val_accuracy: 0.8190\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8536 - accuracy: 0.7769 - val_loss: 0.6919 - val_accuracy: 0.8211\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8464 - accuracy: 0.7791 - val_loss: 0.6897 - val_accuracy: 0.8225\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8443 - accuracy: 0.7795 - val_loss: 0.6897 - val_accuracy: 0.8209\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8364 - accuracy: 0.7809 - val_loss: 0.6889 - val_accuracy: 0.8212\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8339 - accuracy: 0.7806 - val_loss: 0.6994 - val_accuracy: 0.8182\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8295 - accuracy: 0.7823 - val_loss: 0.6905 - val_accuracy: 0.8206\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8255 - accuracy: 0.7829 - val_loss: 0.6961 - val_accuracy: 0.8188\n",
            "7500/7500 [==============================] - 23s 3ms/step - loss: 0.7045 - accuracy: 0.8144\n",
            "Test accuarcy: 81.44%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84      2000\n",
            "           1       0.92      0.93      0.93      2000\n",
            "           2       0.72      0.85      0.78      2000\n",
            "           3       0.85      0.80      0.83      2000\n",
            "           4       0.87      0.83      0.85      2000\n",
            "           5       0.88      0.85      0.86      2000\n",
            "           6       0.84      0.81      0.82      2000\n",
            "           7       0.62      0.79      0.69      2000\n",
            "           8       0.61      0.44      0.51      2000\n",
            "           9       0.79      0.85      0.82      2000\n",
            "          10       0.89      0.86      0.87      2000\n",
            "          11       0.81      0.89      0.85      2000\n",
            "          12       0.92      0.94      0.93      2000\n",
            "          13       0.84      0.88      0.86      2000\n",
            "          14       0.84      0.90      0.87      2000\n",
            "          15       0.80      0.76      0.78      2000\n",
            "          16       0.84      0.85      0.85      2000\n",
            "          17       0.91      0.89      0.90      2000\n",
            "          18       0.87      0.87      0.87      2000\n",
            "          19       0.84      0.73      0.78      2000\n",
            "          20       0.92      0.91      0.91      2000\n",
            "          21       0.80      0.73      0.77      2000\n",
            "          22       0.79      0.83      0.81      2000\n",
            "          23       0.91      0.90      0.90      2000\n",
            "          24       0.82      0.92      0.87      2000\n",
            "          25       0.82      0.71      0.76      2000\n",
            "          26       0.74      0.85      0.79      2000\n",
            "          27       0.69      0.75      0.72      2000\n",
            "          28       0.76      0.78      0.77      2000\n",
            "          29       0.66      0.57      0.61      2000\n",
            "          30       0.69      0.50      0.58      2000\n",
            "          31       0.75      0.64      0.69      2000\n",
            "          32       0.86      0.94      0.90      2000\n",
            "          33       0.92      0.87      0.90      2000\n",
            "          34       0.52      0.60      0.56      2000\n",
            "          35       0.91      0.89      0.90      2000\n",
            "          36       0.77      0.80      0.78      2000\n",
            "          37       0.71      0.73      0.72      2000\n",
            "          38       0.89      0.83      0.86      2000\n",
            "          39       0.76      0.79      0.78      2000\n",
            "          40       0.99      0.95      0.97      2000\n",
            "          41       0.75      0.65      0.70      2000\n",
            "          42       0.92      0.89      0.91      2000\n",
            "          43       0.89      0.83      0.86      2000\n",
            "          44       0.76      0.74      0.75      2000\n",
            "          45       0.84      0.77      0.80      2000\n",
            "          46       0.88      0.86      0.87      2000\n",
            "          47       0.82      0.90      0.85      2000\n",
            "          48       0.61      0.53      0.57      2000\n",
            "          49       0.56      0.74      0.64      2000\n",
            "          50       0.69      0.88      0.77      2000\n",
            "          51       0.81      0.86      0.84      2000\n",
            "          52       0.88      0.93      0.91      2000\n",
            "          53       0.84      0.84      0.84      2000\n",
            "          54       0.89      0.88      0.89      2000\n",
            "          55       0.89      0.81      0.85      2000\n",
            "          56       0.82      0.76      0.79      2000\n",
            "          57       0.80      0.78      0.79      2000\n",
            "          58       0.92      0.90      0.91      2000\n",
            "          59       0.78      0.83      0.80      2000\n",
            "          60       0.86      0.91      0.88      2000\n",
            "          61       0.81      0.82      0.82      2000\n",
            "          62       0.88      0.78      0.82      2000\n",
            "          63       0.78      0.83      0.80      2000\n",
            "          64       0.82      0.87      0.84      2000\n",
            "          65       0.68      0.84      0.75      2000\n",
            "          66       0.94      0.93      0.94      2000\n",
            "          67       0.77      0.85      0.81      2000\n",
            "          68       0.88      0.91      0.89      2000\n",
            "          69       0.60      0.76      0.67      2000\n",
            "          70       0.80      0.59      0.68      2000\n",
            "          71       0.90      0.89      0.89      2000\n",
            "          72       0.64      0.56      0.60      2000\n",
            "          73       0.69      0.82      0.75      2000\n",
            "          74       0.85      0.66      0.74      2000\n",
            "          75       0.85      0.78      0.81      2000\n",
            "          76       0.82      0.83      0.82      2000\n",
            "          77       0.80      0.79      0.79      2000\n",
            "          78       0.79      0.77      0.78      2000\n",
            "          79       0.89      0.87      0.88      2000\n",
            "          80       0.74      0.65      0.69      2000\n",
            "          81       0.78      0.69      0.73      2000\n",
            "          82       0.86      0.82      0.84      2000\n",
            "          83       0.69      0.73      0.71      2000\n",
            "          84       0.74      0.73      0.74      2000\n",
            "          85       0.69      0.75      0.72      2000\n",
            "          86       0.81      0.86      0.83      2000\n",
            "          87       0.68      0.69      0.68      2000\n",
            "          88       0.83      0.79      0.81      2000\n",
            "          89       0.89      0.84      0.87      2000\n",
            "          90       0.85      0.92      0.89      2000\n",
            "          91       0.94      0.95      0.94      2000\n",
            "          92       0.79      0.83      0.81      2000\n",
            "          93       0.77      0.83      0.80      2000\n",
            "          94       0.90      0.84      0.87      2000\n",
            "          95       0.89      0.92      0.91      2000\n",
            "          96       0.76      0.77      0.76      2000\n",
            "          97       0.85      0.84      0.84      2000\n",
            "          98       0.92      0.91      0.91      2000\n",
            "          99       0.90      0.92      0.91      2000\n",
            "         100       0.83      0.90      0.87      2000\n",
            "         101       0.83      0.79      0.81      2000\n",
            "         102       0.92      0.92      0.92      2000\n",
            "         103       0.95      0.93      0.94      2000\n",
            "         104       0.93      0.91      0.92      2000\n",
            "         105       0.86      0.78      0.82      2000\n",
            "         106       0.88      0.91      0.90      2000\n",
            "         107       0.88      0.89      0.89      2000\n",
            "         108       0.89      0.86      0.88      2000\n",
            "         109       0.89      0.89      0.89      2000\n",
            "         110       0.92      0.83      0.87      2000\n",
            "         111       0.88      0.87      0.88      2000\n",
            "         112       0.82      0.85      0.84      2000\n",
            "         113       0.70      0.64      0.67      2000\n",
            "         114       0.72      0.81      0.76      2000\n",
            "         115       0.90      0.92      0.91      2000\n",
            "         116       0.83      0.75      0.79      2000\n",
            "         117       0.95      0.93      0.94      2000\n",
            "         118       0.75      0.78      0.77      2000\n",
            "         119       0.83      0.83      0.83      2000\n",
            "\n",
            "    accuracy                           0.81    240000\n",
            "   macro avg       0.82      0.81      0.81    240000\n",
            "weighted avg       0.82      0.81      0.81    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.1856125\n",
            "processing batch: 13\n",
            "processing file: data_processed/13/airplane.npy - 0\n",
            "processing file: data_processed/13/apple.npy - 1\n",
            "processing file: data_processed/13/ant.npy - 2\n",
            "processing file: data_processed/13/axe.npy - 3\n",
            "processing file: data_processed/13/banana.npy - 4\n",
            "processing file: data_processed/13/baseball.npy - 5\n",
            "processing file: data_processed/13/bee.npy - 6\n",
            "processing file: data_processed/13/beach.npy - 7\n",
            "processing file: data_processed/13/bird.npy - 8\n",
            "processing file: data_processed/13/book.npy - 9\n",
            "processing file: data_processed/13/bucket.npy - 10\n",
            "processing file: data_processed/13/bus.npy - 11\n",
            "processing file: data_processed/13/butterfly.npy - 12\n",
            "processing file: data_processed/13/cactus.npy - 13\n",
            "processing file: data_processed/13/cake.npy - 14\n",
            "processing file: data_processed/13/cannon.npy - 15\n",
            "processing file: data_processed/13/car.npy - 16\n",
            "processing file: data_processed/13/carrot.npy - 17\n",
            "processing file: data_processed/13/castle.npy - 18\n",
            "processing file: data_processed/13/cat.npy - 19\n",
            "processing file: data_processed/13/chair.npy - 20\n",
            "processing file: data_processed/13/church.npy - 21\n",
            "processing file: data_processed/13/circle.npy - 22\n",
            "processing file: data_processed/13/clock.npy - 23\n",
            "processing file: data_processed/13/cloud.npy - 24\n",
            "processing file: data_processed/13/computer.npy - 25\n",
            "processing file: data_processed/13/cookie.npy - 26\n",
            "processing file: data_processed/13/cow.npy - 27\n",
            "processing file: data_processed/13/crab.npy - 28\n",
            "processing file: data_processed/13/crayon.npy - 29\n",
            "processing file: data_processed/13/cup.npy - 30\n",
            "processing file: data_processed/13/dolphin.npy - 31\n",
            "processing file: data_processed/13/donut.npy - 32\n",
            "processing file: data_processed/13/door.npy - 33\n",
            "processing file: data_processed/13/dragon.npy - 34\n",
            "processing file: data_processed/13/diamond.npy - 35\n",
            "processing file: data_processed/13/drums.npy - 36\n",
            "processing file: data_processed/13/duck.npy - 37\n",
            "processing file: data_processed/13/ear.npy - 38\n",
            "processing file: data_processed/13/elephant.npy - 39\n",
            "processing file: data_processed/13/envelope.npy - 40\n",
            "processing file: data_processed/13/eraser.npy - 41\n",
            "processing file: data_processed/13/eye.npy - 42\n",
            "processing file: data_processed/13/face.npy - 43\n",
            "processing file: data_processed/13/fan.npy - 44\n",
            "processing file: data_processed/13/finger.npy - 45\n",
            "processing file: data_processed/13/fish.npy - 46\n",
            "processing file: data_processed/13/flower.npy - 47\n",
            "processing file: data_processed/13/frog.npy - 48\n",
            "processing file: data_processed/13/garden.npy - 49\n",
            "processing file: data_processed/13/grapes.npy - 50\n",
            "processing file: data_processed/13/grass.npy - 51\n",
            "processing file: data_processed/13/guitar.npy - 52\n",
            "processing file: data_processed/13/hammer.npy - 53\n",
            "processing file: data_processed/13/hand.npy - 54\n",
            "processing file: data_processed/13/hat.npy - 55\n",
            "processing file: data_processed/13/helmet.npy - 56\n",
            "processing file: data_processed/13/horse.npy - 57\n",
            "processing file: data_processed/13/house.npy - 58\n",
            "processing file: data_processed/13/hospital.npy - 59\n",
            "processing file: data_processed/13/jail.npy - 60\n",
            "processing file: data_processed/13/kangaroo.npy - 61\n",
            "processing file: data_processed/13/knife.npy - 62\n",
            "processing file: data_processed/13/laptop.npy - 63\n",
            "processing file: data_processed/13/leg.npy - 64\n",
            "processing file: data_processed/13/lion.npy - 65\n",
            "processing file: data_processed/13/lollipop.npy - 66\n",
            "processing file: data_processed/13/map.npy - 67\n",
            "processing file: data_processed/13/microwave.npy - 68\n",
            "processing file: data_processed/13/monkey.npy - 69\n",
            "processing file: data_processed/13/moon.npy - 70\n",
            "processing file: data_processed/13/mountain.npy - 71\n",
            "processing file: data_processed/13/mouse.npy - 72\n",
            "processing file: data_processed/13/mug.npy - 73\n",
            "processing file: data_processed/13/nail.npy - 74\n",
            "processing file: data_processed/13/nose.npy - 75\n",
            "processing file: data_processed/13/onion.npy - 76\n",
            "processing file: data_processed/13/owl.npy - 77\n",
            "processing file: data_processed/13/panda.npy - 78\n",
            "processing file: data_processed/13/parachute.npy - 79\n",
            "processing file: data_processed/13/parrot.npy - 80\n",
            "processing file: data_processed/13/passport.npy - 81\n",
            "processing file: data_processed/13/peanut.npy - 82\n",
            "processing file: data_processed/13/pencil.npy - 83\n",
            "processing file: data_processed/13/piano.npy - 84\n",
            "processing file: data_processed/13/pig.npy - 85\n",
            "processing file: data_processed/13/pizza.npy - 86\n",
            "processing file: data_processed/13/potato.npy - 87\n",
            "processing file: data_processed/13/rabbit.npy - 88\n",
            "processing file: data_processed/13/radio.npy - 89\n",
            "processing file: data_processed/13/rain.npy - 90\n",
            "processing file: data_processed/13/rainbow.npy - 91\n",
            "processing file: data_processed/13/river.npy - 92\n",
            "processing file: data_processed/13/sandwich.npy - 93\n",
            "processing file: data_processed/13/saw.npy - 94\n",
            "processing file: data_processed/13/sun.npy - 95\n",
            "processing file: data_processed/13/shark.npy - 96\n",
            "processing file: data_processed/13/shoe.npy - 97\n",
            "processing file: data_processed/13/skull.npy - 98\n",
            "processing file: data_processed/13/snail.npy - 99\n",
            "processing file: data_processed/13/snowflake.npy - 100\n",
            "processing file: data_processed/13/spider.npy - 101\n",
            "processing file: data_processed/13/square.npy - 102\n",
            "processing file: data_processed/13/stairs.npy - 103\n",
            "processing file: data_processed/13/star.npy - 104\n",
            "processing file: data_processed/13/stove.npy - 105\n",
            "processing file: data_processed/13/strawberry.npy - 106\n",
            "processing file: data_processed/13/table.npy - 107\n",
            "processing file: data_processed/13/teapot.npy - 108\n",
            "processing file: data_processed/13/television.npy - 109\n",
            "processing file: data_processed/13/tent.npy - 110\n",
            "processing file: data_processed/13/toilet.npy - 111\n",
            "processing file: data_processed/13/toothbrush.npy - 112\n",
            "processing file: data_processed/13/toothpaste.npy - 113\n",
            "processing file: data_processed/13/train.npy - 114\n",
            "processing file: data_processed/13/triangle.npy - 115\n",
            "processing file: data_processed/13/truck.npy - 116\n",
            "processing file: data_processed/13/umbrella.npy - 117\n",
            "processing file: data_processed/13/whale.npy - 118\n",
            "processing file: data_processed/13/zebra.npy - 119\n",
            "---------- Loading complete\n",
            "0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "844/844 [==============================] - 15s 18ms/step - loss: 0.8716 - accuracy: 0.7735 - val_loss: 0.6850 - val_accuracy: 0.8189\n",
            "1\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8538 - accuracy: 0.7773 - val_loss: 0.6909 - val_accuracy: 0.8161\n",
            "2\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8478 - accuracy: 0.7786 - val_loss: 0.6900 - val_accuracy: 0.8167\n",
            "3\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8377 - accuracy: 0.7806 - val_loss: 0.6859 - val_accuracy: 0.8177\n",
            "4\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8321 - accuracy: 0.7814 - val_loss: 0.6907 - val_accuracy: 0.8181\n",
            "5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8261 - accuracy: 0.7831 - val_loss: 0.6914 - val_accuracy: 0.8188\n",
            "6\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8222 - accuracy: 0.7837 - val_loss: 0.6899 - val_accuracy: 0.8184\n",
            "7\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8157 - accuracy: 0.7861 - val_loss: 0.6804 - val_accuracy: 0.8201\n",
            "8\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8123 - accuracy: 0.7866 - val_loss: 0.6841 - val_accuracy: 0.8200\n",
            "9\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.8091 - accuracy: 0.7866 - val_loss: 0.6872 - val_accuracy: 0.8182\n",
            "7500/7500 [==============================] - 22s 3ms/step - loss: 0.6951 - accuracy: 0.8174\n",
            "Test accuarcy: 81.74%\n",
            "----------\n",
            "7500/7500 - 13s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84      2000\n",
            "           1       0.92      0.93      0.93      2000\n",
            "           2       0.77      0.84      0.80      2000\n",
            "           3       0.85      0.81      0.83      2000\n",
            "           4       0.88      0.81      0.85      2000\n",
            "           5       0.87      0.86      0.86      2000\n",
            "           6       0.76      0.85      0.80      2000\n",
            "           7       0.72      0.70      0.71      2000\n",
            "           8       0.60      0.42      0.49      2000\n",
            "           9       0.79      0.85      0.82      2000\n",
            "          10       0.87      0.88      0.87      2000\n",
            "          11       0.88      0.83      0.85      2000\n",
            "          12       0.90      0.94      0.92      2000\n",
            "          13       0.89      0.86      0.87      2000\n",
            "          14       0.84      0.90      0.87      2000\n",
            "          15       0.83      0.76      0.79      2000\n",
            "          16       0.83      0.86      0.85      2000\n",
            "          17       0.90      0.89      0.90      2000\n",
            "          18       0.84      0.88      0.86      2000\n",
            "          19       0.82      0.76      0.79      2000\n",
            "          20       0.91      0.92      0.91      2000\n",
            "          21       0.80      0.73      0.77      2000\n",
            "          22       0.75      0.87      0.81      2000\n",
            "          23       0.92      0.90      0.91      2000\n",
            "          24       0.84      0.93      0.88      2000\n",
            "          25       0.81      0.70      0.75      2000\n",
            "          26       0.74      0.85      0.79      2000\n",
            "          27       0.76      0.69      0.72      2000\n",
            "          28       0.80      0.77      0.78      2000\n",
            "          29       0.64      0.61      0.63      2000\n",
            "          30       0.69      0.51      0.59      2000\n",
            "          31       0.74      0.67      0.70      2000\n",
            "          32       0.90      0.93      0.92      2000\n",
            "          33       0.93      0.88      0.90      2000\n",
            "          34       0.53      0.59      0.56      2000\n",
            "          35       0.88      0.91      0.90      2000\n",
            "          36       0.76      0.81      0.78      2000\n",
            "          37       0.70      0.74      0.72      2000\n",
            "          38       0.91      0.83      0.87      2000\n",
            "          39       0.74      0.81      0.77      2000\n",
            "          40       0.98      0.96      0.97      2000\n",
            "          41       0.70      0.66      0.68      2000\n",
            "          42       0.92      0.90      0.91      2000\n",
            "          43       0.87      0.84      0.86      2000\n",
            "          44       0.81      0.73      0.77      2000\n",
            "          45       0.81      0.78      0.80      2000\n",
            "          46       0.87      0.87      0.87      2000\n",
            "          47       0.84      0.89      0.87      2000\n",
            "          48       0.60      0.56      0.58      2000\n",
            "          49       0.64      0.71      0.67      2000\n",
            "          50       0.74      0.87      0.80      2000\n",
            "          51       0.76      0.89      0.82      2000\n",
            "          52       0.91      0.93      0.92      2000\n",
            "          53       0.87      0.82      0.84      2000\n",
            "          54       0.88      0.89      0.89      2000\n",
            "          55       0.88      0.81      0.85      2000\n",
            "          56       0.80      0.77      0.78      2000\n",
            "          57       0.78      0.81      0.79      2000\n",
            "          58       0.92      0.91      0.91      2000\n",
            "          59       0.79      0.81      0.80      2000\n",
            "          60       0.86      0.91      0.88      2000\n",
            "          61       0.81      0.83      0.82      2000\n",
            "          62       0.83      0.80      0.82      2000\n",
            "          63       0.76      0.85      0.80      2000\n",
            "          64       0.82      0.87      0.84      2000\n",
            "          65       0.76      0.80      0.78      2000\n",
            "          66       0.93      0.94      0.94      2000\n",
            "          67       0.78      0.85      0.81      2000\n",
            "          68       0.90      0.90      0.90      2000\n",
            "          69       0.62      0.76      0.68      2000\n",
            "          70       0.75      0.64      0.69      2000\n",
            "          71       0.86      0.91      0.88      2000\n",
            "          72       0.59      0.61      0.60      2000\n",
            "          73       0.69      0.81      0.75      2000\n",
            "          74       0.83      0.66      0.74      2000\n",
            "          75       0.83      0.80      0.81      2000\n",
            "          76       0.79      0.84      0.82      2000\n",
            "          77       0.80      0.79      0.80      2000\n",
            "          78       0.82      0.76      0.79      2000\n",
            "          79       0.89      0.86      0.87      2000\n",
            "          80       0.75      0.65      0.70      2000\n",
            "          81       0.77      0.69      0.73      2000\n",
            "          82       0.84      0.83      0.84      2000\n",
            "          83       0.69      0.71      0.70      2000\n",
            "          84       0.79      0.72      0.75      2000\n",
            "          85       0.67      0.78      0.72      2000\n",
            "          86       0.81      0.87      0.84      2000\n",
            "          87       0.71      0.65      0.68      2000\n",
            "          88       0.78      0.82      0.80      2000\n",
            "          89       0.88      0.86      0.87      2000\n",
            "          90       0.86      0.92      0.89      2000\n",
            "          91       0.94      0.95      0.94      2000\n",
            "          92       0.82      0.82      0.82      2000\n",
            "          93       0.78      0.82      0.80      2000\n",
            "          94       0.93      0.82      0.87      2000\n",
            "          95       0.90      0.92      0.91      2000\n",
            "          96       0.75      0.78      0.76      2000\n",
            "          97       0.87      0.83      0.85      2000\n",
            "          98       0.92      0.91      0.91      2000\n",
            "          99       0.88      0.93      0.91      2000\n",
            "         100       0.86      0.89      0.87      2000\n",
            "         101       0.86      0.79      0.82      2000\n",
            "         102       0.90      0.93      0.91      2000\n",
            "         103       0.97      0.92      0.94      2000\n",
            "         104       0.91      0.92      0.92      2000\n",
            "         105       0.88      0.77      0.82      2000\n",
            "         106       0.91      0.90      0.91      2000\n",
            "         107       0.89      0.89      0.89      2000\n",
            "         108       0.87      0.88      0.88      2000\n",
            "         109       0.92      0.87      0.90      2000\n",
            "         110       0.91      0.84      0.87      2000\n",
            "         111       0.86      0.89      0.87      2000\n",
            "         112       0.81      0.86      0.84      2000\n",
            "         113       0.70      0.65      0.67      2000\n",
            "         114       0.76      0.79      0.78      2000\n",
            "         115       0.89      0.94      0.91      2000\n",
            "         116       0.78      0.80      0.79      2000\n",
            "         117       0.94      0.93      0.94      2000\n",
            "         118       0.81      0.75      0.78      2000\n",
            "         119       0.83      0.83      0.83      2000\n",
            "\n",
            "    accuracy                           0.82    240000\n",
            "   macro avg       0.82      0.82      0.82    240000\n",
            "weighted avg       0.82      0.82      0.82    240000\n",
            "\n",
            "----------\n",
            "HammingLoss:  0.18262916666666668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbWbLMLNxhnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf205a09-f608-4c91-b1ff-d2d427b88a30"
      },
      "source": [
        "def get_vgg_bn_reg_seq(num_classes):\n",
        "\n",
        "  activation = 'elu'\n",
        "\n",
        "  model = keras.Sequential(name=\"vgg_bn_reg_seq\")\n",
        "  model.add(layers.Convolution2D(16, (3, 3), name='c2d_11', input_shape=(28, 28, 1), padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(16, (3, 3), name='c2d_12', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(16, (3, 3), name='c2d_13', padding='same', activation=activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Convolution2D(32, (3, 3), name='c2d_21', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(32, (3, 3), name='c2d_22', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(32, (3, 3), name='c2d_23', padding='same', activation=activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Convolution2D(64, (3, 3), name='c2d_31', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(64, (3, 3), name='c2d_32', padding='same', activation=activation))\n",
        "  model.add(layers.Convolution2D(64, (3, 3), name='c2d_33', padding='same', activation=activation))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(256, name='d1', activation=activation))\n",
        "  model.add(layers.Dense(256, name='d2', activation=activation))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Dense(num_classes, name='softmax', activation='softmax'))\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "model = run_exp(get_vgg_bn_reg_seq(120))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg_bn_reg_seq\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "c2d_11 (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "c2d_12 (Conv2D)              (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "c2d_13 (Conv2D)              (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "c2d_21 (Conv2D)              (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "c2d_22 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "_________________________________________________________________\n",
            "c2d_23 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "c2d_31 (Conv2D)              (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "c2d_32 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "c2d_33 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 576)               2304      \n",
            "_________________________________________________________________\n",
            "d1 (Dense)                   (None, 256)               147712    \n",
            "_________________________________________________________________\n",
            "d2 (Dense)                   (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 337,316\n",
            "Trainable params: 336,068\n",
            "Non-trainable params: 1,248\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-f977a13b53ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vgg_bn_reg_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: run_exp() missing 2 required positional arguments: 'x_train' and 'y_train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWJg5cZACIY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflowjs as tfjs\n",
        "# name = 'qd_vgg_81.h5'\n",
        "# model.save(name)\n",
        "# tfjs.converters.save_keras_model(model, 'qd_vgg_81_tfjs')\n",
        "\n",
        "# !gsutil cp -r qd_vgg_81_tfjs gs://ml_workspace/quick_draw/model\n",
        "# !gsutil cp qd_vgg_81.h5 gs://ml_workspace/quick_draw/model/qd_vgg_81.h5\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "608afjYXr_dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classes = ','.join(class_names)\n",
        "# text_file = open(\"classes.txt\", \"w\")\n",
        "# n = text_file.write(classes)\n",
        "# text_file.close()\n",
        "# !more classes.txt\n",
        "\n",
        "# !gsutil cp classes.txt gs://ml_workspace/quick_draw/model/qd_vgg_81_tfjs/classes.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}